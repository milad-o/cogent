#!/usr/bin/env python3
"""
Framework Comparison Benchmark
==============================

Empirical comparison of AgenticFlow vs CrewAI, OpenAI Agents SDK, and Agno
on realistic single-agent and multi-agent tasks.

Tasks:
1. Single Agent - Research & Summarize: Web search + synthesis
2. Single Agent - Multi-Step Calculation: Tool chaining with dependencies
3. Multi-Agent - Content Pipeline: Researcher ‚Üí Writer ‚Üí Editor
4. Multi-Agent - Parallel Analysis: Multiple specialists working together

Metrics:
- Task completion (pass/fail)
- Total time
- Token usage (estimated via tool calls)
- Number of LLM calls
- Number of tool calls

Requirements:
    pip install agenticflow crewai openai-agents agno langchain-openai

Usage:
    uv run python scripts/benchmark_frameworks.py
    uv run python scripts/benchmark_frameworks.py --framework agenticflow
    uv run python scripts/benchmark_frameworks.py --task single-calc
"""

import argparse
import asyncio
import json
import os
import sys
import time
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any

from dotenv import load_dotenv

load_dotenv(Path(__file__).parent.parent / ".env")

# =============================================================================
# BENCHMARK INFRASTRUCTURE
# =============================================================================


@dataclass
class BenchmarkResult:
    """Result from a single benchmark run."""

    framework: str
    task: str
    passed: bool
    duration: float
    llm_calls: int = 0
    tool_calls: int = 0
    error: str | None = None
    output: str = ""
    details: dict = field(default_factory=dict)


@dataclass
class TaskConfig:
    """Configuration for a benchmark task."""

    name: str
    description: str
    task_type: str  # "single" or "multi"
    prompt: str
    expected_output_contains: list[str]
    tools_required: list[str]
    timeout: float = 120.0


# Shared test data for tools
COMPANY_DATABASE = {
    "ACME Corp": {
        "revenue_2024": 15_200_000,
        "employees": 450,
        "founded": 2015,
        "sector": "Technology",
        "products": ["CloudSync", "DataFlow", "SecureVault"],
    },
    "GlobalTech": {
        "revenue_2024": 8_700_000,
        "employees": 230,
        "founded": 2018,
        "sector": "Technology",
        "products": ["AIAssist", "SmartAnalytics"],
    },
    "EcoSolutions": {
        "revenue_2024": 5_400_000,
        "employees": 120,
        "founded": 2020,
        "sector": "Green Energy",
        "products": ["SolarMax", "WindPower", "EcoGrid"],
    },
}

MARKET_DATA = {
    "Technology": {"growth_rate": 0.12, "market_size": 500_000_000_000, "outlook": "Strong"},
    "Green Energy": {"growth_rate": 0.18, "market_size": 150_000_000_000, "outlook": "Very Strong"},
    "Healthcare": {"growth_rate": 0.08, "market_size": 400_000_000_000, "outlook": "Stable"},
}

# Track tool calls globally for metrics
_tool_call_log: list[str] = []


def reset_tool_log():
    global _tool_call_log
    _tool_call_log = []


def log_tool_call(name: str, args: dict):
    _tool_call_log.append(f"{name}({json.dumps(args)})")


# =============================================================================
# BENCHMARK TASKS
# =============================================================================

TASKS = [
    TaskConfig(
        name="single-calc",
        description="Single Agent: Multi-step financial calculation",
        task_type="single",
        prompt="""Calculate the total combined revenue of all Technology sector companies 
in our database, then calculate what percentage this represents of the total 
Technology market size. Show your work step by step.

Use the available tools to:
1. Get company data for each company
2. Filter to Technology sector companies  
3. Sum their revenues
4. Get the Technology market data
5. Calculate the percentage""",
        # ACME (15.2M) + GlobalTech (8.7M) = 23.9M, market = 500B, pct = 0.00478%
        expected_output_contains=["23", "technology", "market"],  # More lenient
        tools_required=["get_company_data", "get_market_data"],
    ),
    TaskConfig(
        name="single-research",
        description="Single Agent: Research and synthesize company analysis",
        task_type="single",
        prompt="""Provide a competitive analysis of ACME Corp. Include:
1. Company overview (revenue, employees, products)
2. Market position (what sector, market outlook)
3. Comparison to at least one competitor in the same sector
4. A brief recommendation (1-2 sentences)

Use the tools to gather all necessary data.""",
        expected_output_contains=["ACME", "Technology"],  # More lenient
        tools_required=["get_company_data", "get_market_data"],
    ),
    TaskConfig(
        name="multi-handoff",
        description="Multi-Agent: Simple handoff between 2 agents",
        task_type="multi",
        prompt="""Get EcoSolutions company data, then calculate their revenue per employee.
First agent gathers data, second agent does the calculation.""",
        expected_output_contains=["EcoSolutions", "revenue"],
        tools_required=["get_company_data", "calculate_metrics"],
    ),
]


# =============================================================================
# AGENTICFLOW IMPLEMENTATION
# =============================================================================


class AgenticFlowBenchmark:
    """Benchmark runner for AgenticFlow."""

    def __init__(self):
        from langchain_core.tools import tool
        from langchain_openai import ChatOpenAI

        self.model = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        self.tools = self._create_tools(tool)

    def _create_tools(self, tool_decorator):
        @tool_decorator
        def get_company_data(company_name: str) -> str:
            """Get company information from the database.

            Args:
                company_name: Name of the company (e.g., "ACME Corp", "GlobalTech", "EcoSolutions")

            Returns:
                JSON string with company data including revenue, employees, sector, products
            """
            log_tool_call("get_company_data", {"company_name": company_name})
            if company_name in COMPANY_DATABASE:
                return json.dumps(COMPANY_DATABASE[company_name])
            return f"ERROR: Company '{company_name}' not found. Available: {list(COMPANY_DATABASE.keys())}"

        @tool_decorator
        def get_market_data(sector: str) -> str:
            """Get market data for a specific sector.

            Args:
                sector: The sector name (e.g., "Technology", "Green Energy", "Healthcare")

            Returns:
                JSON string with growth_rate, market_size, and outlook
            """
            log_tool_call("get_market_data", {"sector": sector})
            if sector in MARKET_DATA:
                return json.dumps(MARKET_DATA[sector])
            return f"ERROR: Sector '{sector}' not found. Available: {list(MARKET_DATA.keys())}"

        @tool_decorator
        def calculate_metrics(revenue: float, employees: int, market_size: float) -> str:
            """Calculate business metrics.

            Args:
                revenue: Company annual revenue
                employees: Number of employees
                market_size: Total market size

            Returns:
                JSON with revenue_per_employee and market_share_percent
            """
            log_tool_call(
                "calculate_metrics",
                {"revenue": revenue, "employees": employees, "market_size": market_size},
            )
            rev_per_emp = revenue / employees if employees > 0 else 0
            market_share = (revenue / market_size * 100) if market_size > 0 else 0
            return json.dumps(
                {
                    "revenue_per_employee": round(rev_per_emp, 2),
                    "market_share_percent": round(market_share, 6),
                }
            )

        return [get_company_data, get_market_data, calculate_metrics]

    async def run_single_agent(self, task: TaskConfig) -> BenchmarkResult:
        """Run a single-agent task."""
        from agenticflow import Agent, AgentConfig
        from agenticflow.executors import NativeExecutor

        reset_tool_log()
        start = time.time()

        try:
            # Use simple, efficient system prompt like Agno
            agent = Agent(
                config=AgentConfig(
                    name="analyst",
                    model=self.model,
                    system_prompt="You are a financial analyst. Use tools to gather data and perform calculations efficiently. Call multiple tools at once when possible.",
                ),
                tools=self.tools,
            )
            
            # Enable turbo mode to skip events for speed
            agent.enable_turbo_mode(True)

            # Use NativeExecutor for maximum speed
            executor = NativeExecutor(agent)
            executor.max_iterations = 10

            result = await executor.execute(task.prompt)
            duration = time.time() - start

            # Check if output contains expected content
            passed = any(exp.lower() in result.lower() for exp in task.expected_output_contains)

            return BenchmarkResult(
                framework="AgenticFlow",
                task=task.name,
                passed=passed,
                duration=duration,
                tool_calls=len(_tool_call_log),
                llm_calls=executor.iterations if hasattr(executor, "iterations") else len(_tool_call_log) + 1,
                output=result[:500],
            )

        except Exception as e:
            return BenchmarkResult(
                framework="AgenticFlow",
                task=task.name,
                passed=False,
                duration=time.time() - start,
                error=str(e),
            )

    async def run_multi_agent(self, task: TaskConfig) -> BenchmarkResult:
        """Run a multi-agent task with simple 2-agent pipeline."""
        from agenticflow import Agent, AgentConfig, TopologyFactory, TopologyType

        reset_tool_log()
        start = time.time()

        try:
            # Simple 2-agent pipeline: researcher -> analyst
            researcher = Agent(
                config=AgentConfig(
                    name="researcher",
                    description="Gathers company data",
                    system_prompt="You gather company data using tools. Pass the data to the analyst.",
                    model=self.model,
                ),
                tools=[self.tools[0]],  # get_company_data
            )

            analyst = Agent(
                config=AgentConfig(
                    name="analyst",
                    description="Calculates metrics from data",
                    system_prompt="You calculate metrics from company data.",
                    model=self.model,
                ),
                tools=[self.tools[2]],  # calculate_metrics
            )

            # Simple pipeline topology with fast execution
            topology = TopologyFactory.create(
                TopologyType.PIPELINE,
                name="analysis_pipeline",
                agents=[researcher, analyst],
            )

            # Use run_fast() for direct sequential execution (like CrewAI)
            result = await topology.run_fast(task.prompt)
            duration = time.time() - start

            # Get output from final result
            output = result.results[-1]["thought"] if result.results else str(result)
            passed = any(exp.lower() in output.lower() for exp in task.expected_output_contains)

            return BenchmarkResult(
                framework="AgenticFlow",
                task=task.name,
                passed=passed,
                duration=duration,
                tool_calls=len(_tool_call_log),
                llm_calls=len(_tool_call_log) + 2,  # 2 agents
                output=output[:500],
            )

        except Exception as e:
            import traceback

            return BenchmarkResult(
                framework="AgenticFlow",
                task=task.name,
                passed=False,
                duration=time.time() - start,
                error=f"{e}\n{traceback.format_exc()}",
            )


# =============================================================================
# CREWAI IMPLEMENTATION
# =============================================================================


class CrewAIBenchmark:
    """Benchmark runner for CrewAI."""

    def __init__(self):
        try:
            from crewai import Agent, Crew, Process, Task
            from crewai.tools import tool

            self.available = True
            self.tool_decorator = tool
            self.Agent = Agent
            self.Task = Task
            self.Crew = Crew
            self.Process = Process
            self.tools = self._create_tools()
        except ImportError:
            self.available = False
            print("‚ö†Ô∏è  CrewAI not installed. Run: pip install crewai")

    def _create_tools(self):
        @self.tool_decorator
        def get_company_data(company_name: str) -> str:
            """Get company information from the database.

            Args:
                company_name: Name of the company (e.g., "ACME Corp", "GlobalTech", "EcoSolutions")

            Returns:
                JSON string with company data including revenue, employees, sector, products
            """
            log_tool_call("get_company_data", {"company_name": company_name})
            if company_name in COMPANY_DATABASE:
                return json.dumps(COMPANY_DATABASE[company_name])
            return f"ERROR: Company '{company_name}' not found"

        @self.tool_decorator
        def get_market_data(sector: str) -> str:
            """Get market data for a specific sector.

            Args:
                sector: The sector name (e.g., "Technology", "Green Energy", "Healthcare")

            Returns:
                JSON string with growth_rate, market_size, and outlook
            """
            log_tool_call("get_market_data", {"sector": sector})
            if sector in MARKET_DATA:
                return json.dumps(MARKET_DATA[sector])
            return f"ERROR: Sector '{sector}' not found"

        @self.tool_decorator
        def calculate_metrics(revenue: float, employees: int, market_size: float) -> str:
            """Calculate business metrics.

            Args:
                revenue: Company annual revenue
                employees: Number of employees
                market_size: Total market size

            Returns:
                JSON with revenue_per_employee and market_share_percent
            """
            log_tool_call(
                "calculate_metrics",
                {"revenue": revenue, "employees": employees, "market_size": market_size},
            )
            rev_per_emp = revenue / employees if employees > 0 else 0
            market_share = (revenue / market_size * 100) if market_size > 0 else 0
            return json.dumps(
                {
                    "revenue_per_employee": round(rev_per_emp, 2),
                    "market_share_percent": round(market_share, 6),
                }
            )

        return [get_company_data, get_market_data, calculate_metrics]

    async def run_single_agent(self, task: TaskConfig) -> BenchmarkResult:
        if not self.available:
            return BenchmarkResult(
                framework="CrewAI", task=task.name, passed=False, duration=0, error="Not installed"
            )

        reset_tool_log()
        start = time.time()

        try:
            agent = self.Agent(
                role="Financial Analyst",
                goal="Analyze company and market data accurately",
                backstory="Expert analyst with access to company databases",
                tools=self.tools,
                verbose=False,
            )

            crew_task = self.Task(
                description=task.prompt,
                expected_output="Detailed analysis with specific numbers",
                agent=agent,
            )

            crew = self.Crew(agents=[agent], tasks=[crew_task], process=self.Process.sequential, verbose=False)

            # Run synchronously (CrewAI's kickoff is sync)
            result = await asyncio.to_thread(crew.kickoff)
            duration = time.time() - start

            output = str(result)
            passed = any(exp.lower() in output.lower() for exp in task.expected_output_contains)

            return BenchmarkResult(
                framework="CrewAI",
                task=task.name,
                passed=passed,
                duration=duration,
                tool_calls=len(_tool_call_log),
                llm_calls=len(_tool_call_log) + 1,
                output=output[:500],
            )

        except Exception as e:
            return BenchmarkResult(
                framework="CrewAI",
                task=task.name,
                passed=False,
                duration=time.time() - start,
                error=str(e),
            )

    async def run_multi_agent(self, task: TaskConfig) -> BenchmarkResult:
        if not self.available:
            return BenchmarkResult(
                framework="CrewAI", task=task.name, passed=False, duration=0, error="Not installed"
            )

        reset_tool_log()
        start = time.time()

        try:
            # Simple 2-agent pipeline
            researcher = self.Agent(
                role="Data Gatherer",
                goal="Get company data",
                backstory="Data specialist",
                tools=[self.tools[0]],  # get_company_data
                verbose=False,
            )

            analyst = self.Agent(
                role="Analyst",
                goal="Calculate metrics",
                backstory="Numbers specialist",
                tools=[self.tools[2]],  # calculate_metrics
                verbose=False,
            )

            research_task = self.Task(
                description="Get the company data requested",
                expected_output="Company data",
                agent=researcher,
            )

            analysis_task = self.Task(
                description=task.prompt,
                expected_output="Calculated metrics",
                agent=analyst,
                context=[research_task],
            )

            crew = self.Crew(
                agents=[researcher, analyst],
                tasks=[research_task, analysis_task],
                process=self.Process.sequential,
                verbose=False,
            )

            result = await asyncio.to_thread(crew.kickoff)
            duration = time.time() - start

            output = str(result)
            passed = any(exp.lower() in output.lower() for exp in task.expected_output_contains)

            return BenchmarkResult(
                framework="CrewAI",
                task=task.name,
                passed=passed,
                duration=duration,
                tool_calls=len(_tool_call_log),
                llm_calls=len(_tool_call_log) + 2,
                output=output[:500],
            )

        except Exception as e:
            return BenchmarkResult(
                framework="CrewAI",
                task=task.name,
                passed=False,
                duration=time.time() - start,
                error=str(e),
            )


# =============================================================================
# OPENAI AGENTS SDK IMPLEMENTATION
# =============================================================================


class OpenAIAgentsBenchmark:
    """Benchmark runner for OpenAI Agents SDK."""

    def __init__(self):
        try:
            from agents import Agent, Runner, function_tool

            self.available = True
            self.Agent = Agent
            self.Runner = Runner
            self.function_tool = function_tool
            self.tools = self._create_tools()
        except ImportError:
            self.available = False
            print("‚ö†Ô∏è  OpenAI Agents SDK not installed. Run: pip install openai-agents")

    def _create_tools(self):
        @self.function_tool
        def get_company_data(company_name: str) -> str:
            """Get company information from the database."""
            log_tool_call("get_company_data", {"company_name": company_name})
            if company_name in COMPANY_DATABASE:
                return json.dumps(COMPANY_DATABASE[company_name])
            return f"ERROR: Company '{company_name}' not found"

        @self.function_tool
        def get_market_data(sector: str) -> str:
            """Get market data for a specific sector."""
            log_tool_call("get_market_data", {"sector": sector})
            if sector in MARKET_DATA:
                return json.dumps(MARKET_DATA[sector])
            return f"ERROR: Sector '{sector}' not found"

        @self.function_tool
        def calculate_metrics(revenue: float, employees: int, market_size: float) -> str:
            """Calculate business metrics."""
            log_tool_call(
                "calculate_metrics",
                {"revenue": revenue, "employees": employees, "market_size": market_size},
            )
            rev_per_emp = revenue / employees if employees > 0 else 0
            market_share = (revenue / market_size * 100) if market_size > 0 else 0
            return json.dumps(
                {
                    "revenue_per_employee": round(rev_per_emp, 2),
                    "market_share_percent": round(market_share, 6),
                }
            )

        return [get_company_data, get_market_data, calculate_metrics]

    async def run_single_agent(self, task: TaskConfig) -> BenchmarkResult:
        if not self.available:
            return BenchmarkResult(
                framework="OpenAI SDK", task=task.name, passed=False, duration=0, error="Not installed"
            )

        reset_tool_log()
        start = time.time()

        try:
            agent = self.Agent(
                name="analyst",
                instructions="You are a financial analyst. Use tools to gather data and perform calculations.",
                tools=self.tools,
                model="gpt-4o-mini",
            )

            result = await self.Runner.run(agent, task.prompt, max_turns=20)
            duration = time.time() - start

            output = result.final_output or ""
            passed = any(exp.lower() in output.lower() for exp in task.expected_output_contains)

            return BenchmarkResult(
                framework="OpenAI SDK",
                task=task.name,
                passed=passed,
                duration=duration,
                tool_calls=len(_tool_call_log),
                llm_calls=len(_tool_call_log) + 1,
                output=output[:500],
            )

        except Exception as e:
            return BenchmarkResult(
                framework="OpenAI SDK",
                task=task.name,
                passed=False,
                duration=time.time() - start,
                error=str(e),
            )

    async def run_multi_agent(self, task: TaskConfig) -> BenchmarkResult:
        if not self.available:
            return BenchmarkResult(
                framework="OpenAI SDK", task=task.name, passed=False, duration=0, error="Not installed"
            )

        reset_tool_log()
        start = time.time()

        try:
            # Simple 2-agent handoff
            researcher = self.Agent(
                name="researcher",
                instructions="Get company data. After getting data, hand off to analyst.",
                tools=[self.tools[0]],  # get_company_data
                model="gpt-4o-mini",
            )

            analyst = self.Agent(
                name="analyst",
                instructions="Calculate metrics from the company data provided.",
                tools=[self.tools[2]],  # calculate_metrics
                model="gpt-4o-mini",
            )

            researcher.handoffs = [analyst]

            result = await self.Runner.run(researcher, task.prompt, max_turns=15)
            duration = time.time() - start

            output = result.final_output or ""
            passed = any(exp.lower() in output.lower() for exp in task.expected_output_contains)

            return BenchmarkResult(
                framework="OpenAI SDK",
                task=task.name,
                passed=passed,
                duration=duration,
                tool_calls=len(_tool_call_log),
                llm_calls=len(_tool_call_log) + 2,
                output=output[:500],
            )

        except Exception as e:
            return BenchmarkResult(
                framework="OpenAI SDK",
                task=task.name,
                passed=False,
                duration=time.time() - start,
                error=str(e),
            )


# =============================================================================
# AGNO IMPLEMENTATION
# =============================================================================


class AgnoBenchmark:
    """Benchmark runner for Agno."""

    def __init__(self):
        try:
            from agno.agent import Agent
            from agno.models.openai import OpenAIChat
            from agno.tools import tool

            self.available = True
            self.Agent = Agent
            self.OpenAIChat = OpenAIChat
            self.tool_decorator = tool
            self.tools = self._create_tools()
        except ImportError:
            self.available = False
            print("‚ö†Ô∏è  Agno not installed. Run: pip install agno")

    def _create_tools(self):
        @self.tool_decorator
        def get_company_data(company_name: str) -> str:
            """Get company information from the database.

            Args:
                company_name: Name of the company
            """
            log_tool_call("get_company_data", {"company_name": company_name})
            if company_name in COMPANY_DATABASE:
                return json.dumps(COMPANY_DATABASE[company_name])
            return f"ERROR: Company '{company_name}' not found"

        @self.tool_decorator
        def get_market_data(sector: str) -> str:
            """Get market data for a specific sector.

            Args:
                sector: The sector name
            """
            log_tool_call("get_market_data", {"sector": sector})
            if sector in MARKET_DATA:
                return json.dumps(MARKET_DATA[sector])
            return f"ERROR: Sector '{sector}' not found"

        @self.tool_decorator
        def calculate_metrics(revenue: float, employees: int, market_size: float) -> str:
            """Calculate business metrics.

            Args:
                revenue: Company annual revenue
                employees: Number of employees
                market_size: Total market size
            """
            log_tool_call(
                "calculate_metrics",
                {"revenue": revenue, "employees": employees, "market_size": market_size},
            )
            rev_per_emp = revenue / employees if employees > 0 else 0
            market_share = (revenue / market_size * 100) if market_size > 0 else 0
            return json.dumps(
                {
                    "revenue_per_employee": round(rev_per_emp, 2),
                    "market_share_percent": round(market_share, 6),
                }
            )

        return [get_company_data, get_market_data, calculate_metrics]

    async def run_single_agent(self, task: TaskConfig) -> BenchmarkResult:
        if not self.available:
            return BenchmarkResult(
                framework="Agno", task=task.name, passed=False, duration=0, error="Not installed"
            )

        reset_tool_log()
        start = time.time()

        try:
            agent = self.Agent(
                name="analyst",
                model=self.OpenAIChat(id="gpt-4o-mini"),
                tools=self.tools,
                instructions="You are a financial analyst. Use tools to gather data and perform calculations.",
                markdown=False,
            )

            # Agno uses sync run by default
            response = await asyncio.to_thread(agent.run, task.prompt)
            duration = time.time() - start

            output = response.content if hasattr(response, "content") else str(response)
            passed = any(exp.lower() in output.lower() for exp in task.expected_output_contains)

            return BenchmarkResult(
                framework="Agno",
                task=task.name,
                passed=passed,
                duration=duration,
                tool_calls=len(_tool_call_log),
                llm_calls=len(_tool_call_log) + 1,
                output=output[:500],
            )

        except Exception as e:
            return BenchmarkResult(
                framework="Agno",
                task=task.name,
                passed=False,
                duration=time.time() - start,
                error=str(e),
            )

    async def run_multi_agent(self, task: TaskConfig) -> BenchmarkResult:
        if not self.available:
            return BenchmarkResult(
                framework="Agno", task=task.name, passed=False, duration=0, error="Not installed"
            )

        reset_tool_log()
        start = time.time()

        try:
            from agno.team import Team

            # Simple 2-agent team
            researcher = self.Agent(
                name="researcher",
                model=self.OpenAIChat(id="gpt-4o-mini"),
                tools=[self.tools[0]],  # get_company_data
                instructions="Get company data.",
            )

            analyst = self.Agent(
                name="analyst",
                model=self.OpenAIChat(id="gpt-4o-mini"),
                tools=[self.tools[2]],  # calculate_metrics
                instructions="Calculate metrics.",
            )

            team = Team(
                members=[researcher, analyst],  # Agno uses 'members' not 'agents'
                name="team",
                model=self.OpenAIChat(id="gpt-4o-mini"),
            )

            response = await asyncio.to_thread(team.run, task.prompt)
            duration = time.time() - start

            output = response.content if hasattr(response, "content") else str(response)
            passed = any(exp.lower() in output.lower() for exp in task.expected_output_contains)

            return BenchmarkResult(
                framework="Agno",
                task=task.name,
                passed=passed,
                duration=duration,
                tool_calls=len(_tool_call_log),
                llm_calls=len(_tool_call_log) + 2,
                output=output[:500],
            )

        except Exception as e:
            return BenchmarkResult(
                framework="Agno",
                task=task.name,
                passed=False,
                duration=time.time() - start,
                error=str(e),
            )


# =============================================================================
# MAIN BENCHMARK RUNNER
# =============================================================================


async def run_benchmarks(
    frameworks: list[str] | None = None,
    tasks: list[str] | None = None,
    runs_per_task: int = 1,
) -> list[BenchmarkResult]:
    """Run benchmarks for specified frameworks and tasks."""

    # Initialize benchmark runners
    runners = {
        "agenticflow": AgenticFlowBenchmark(),
        "crewai": CrewAIBenchmark(),
        "openai": OpenAIAgentsBenchmark(),
        "agno": AgnoBenchmark(),
    }

    if frameworks:
        runners = {k: v for k, v in runners.items() if k in frameworks}

    task_configs = TASKS
    if tasks:
        task_configs = [t for t in TASKS if t.name in tasks]

    results: list[BenchmarkResult] = []

    print("=" * 80)
    print("FRAMEWORK COMPARISON BENCHMARK")
    print("=" * 80)
    print(f"Frameworks: {list(runners.keys())}")
    print(f"Tasks: {[t.name for t in task_configs]}")
    print(f"Runs per task: {runs_per_task}")
    print("=" * 80)

    for task in task_configs:
        print(f"\nüìã Task: {task.name} ({task.description})")
        print("-" * 60)

        for framework_name, runner in runners.items():
            for run_num in range(runs_per_task):
                run_label = f" (run {run_num + 1})" if runs_per_task > 1 else ""
                print(f"  üîÑ {framework_name}{run_label}...", end=" ", flush=True)

                try:
                    if task.task_type == "single":
                        result = await asyncio.wait_for(
                            runner.run_single_agent(task),
                            timeout=task.timeout,
                        )
                    else:
                        result = await asyncio.wait_for(
                            runner.run_multi_agent(task),
                            timeout=task.timeout,
                        )
                except asyncio.TimeoutError:
                    result = BenchmarkResult(
                        framework=framework_name.title(),
                        task=task.name,
                        passed=False,
                        duration=task.timeout,
                        error=f"Timeout after {task.timeout}s",
                    )

                results.append(result)

                if result.passed:
                    print(f"‚úÖ {result.duration:.2f}s ({result.tool_calls} tools)")
                else:
                    print(f"‚ùå {result.duration:.2f}s")
                    if result.error:
                        print(f"      Error: {result.error[:80]}")

    return results


def print_summary(results: list[BenchmarkResult]):
    """Print benchmark summary table."""
    print("\n" + "=" * 80)
    print("BENCHMARK SUMMARY")
    print("=" * 80)

    # Group by framework
    frameworks = {}
    for r in results:
        if r.framework not in frameworks:
            frameworks[r.framework] = {"passed": 0, "total": 0, "time": 0, "tools": 0}
        frameworks[r.framework]["total"] += 1
        frameworks[r.framework]["time"] += r.duration
        frameworks[r.framework]["tools"] += r.tool_calls
        if r.passed:
            frameworks[r.framework]["passed"] += 1

    # Print table header
    print(f"\n{'Framework':<15} {'Pass Rate':<12} {'Avg Time':<12} {'Avg Tools':<12}")
    print("-" * 55)

    for fw, stats in sorted(frameworks.items(), key=lambda x: -x[1]["passed"] / max(x[1]["total"], 1)):
        pass_rate = stats["passed"] / stats["total"] * 100 if stats["total"] > 0 else 0
        avg_time = stats["time"] / stats["total"] if stats["total"] > 0 else 0
        avg_tools = stats["tools"] / stats["total"] if stats["total"] > 0 else 0

        print(f"{fw:<15} {pass_rate:>5.1f}%      {avg_time:>6.2f}s      {avg_tools:>6.1f}")

    # Print per-task breakdown
    print("\n" + "-" * 80)
    print("Per-Task Results:")
    print("-" * 80)

    tasks_seen = set()
    for r in results:
        if r.task not in tasks_seen:
            tasks_seen.add(r.task)
            print(f"\nüìã {r.task}:")
            task_results = [x for x in results if x.task == r.task]
            for tr in task_results:
                status = "‚úÖ" if tr.passed else "‚ùå"
                print(f"   {status} {tr.framework:<15} {tr.duration:>6.2f}s  {tr.tool_calls} tools")


async def main():
    parser = argparse.ArgumentParser(description="Framework Comparison Benchmark")
    parser.add_argument(
        "--framework",
        "-f",
        action="append",
        choices=["agenticflow", "crewai", "openai", "agno"],
        help="Framework(s) to benchmark (can specify multiple)",
    )
    parser.add_argument(
        "--task",
        "-t",
        action="append",
        choices=[t.name for t in TASKS],
        help="Task(s) to run (can specify multiple)",
    )
    parser.add_argument("--runs", "-r", type=int, default=1, help="Number of runs per task")

    args = parser.parse_args()

    results = await run_benchmarks(
        frameworks=args.framework,
        tasks=args.task,
        runs_per_task=args.runs,
    )

    print_summary(results)

    # Return exit code based on AgenticFlow results
    af_results = [r for r in results if r.framework == "AgenticFlow"]
    if af_results:
        af_pass_rate = sum(1 for r in af_results if r.passed) / len(af_results)
        return 0 if af_pass_rate >= 0.5 else 1
    return 0


if __name__ == "__main__":
    sys.exit(asyncio.run(main()))
