{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"AgenticFlow","text":"<p> Build AI agents that actually work. </p> <p>AgenticFlow is a production-grade multi-agent framework designed for performance, simplicity, and real-world deployment. Unlike frameworks that wrap LangChain or add unnecessary abstractions, AgenticFlow uses native SDK integrations and a zero-overhead executor to deliver the fastest possible agent execution.</p>"},{"location":"#why-agenticflow","title":"Why AgenticFlow?","text":"<ul> <li>\ud83d\ude80 Fast \u2014 Parallel tool execution, cached model binding, direct SDK calls</li> <li>\ud83d\udd27 Simple \u2014 Define tools with <code>@tool</code>, create agents in 3 lines, no boilerplate</li> <li>\ud83c\udfed Production-ready \u2014 Built-in resilience, observability, and security interceptors</li> <li>\ud83e\udd1d Multi-agent \u2014 Supervisor, Pipeline, Mesh, and Hierarchical coordination patterns</li> <li>\ud83d\udce6 Batteries included \u2014 File system, web search, code sandbox, browser, PDF, knowledge graphs, and more</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from cogent import Agent, tool\n\n@tool\ndef search(query: str) -&gt; str:\n    \"\"\"Search the web.\"\"\"\n    return web_search(query)\n\n# v1.14.1: Simple string models!\nagent = Agent(name=\"Assistant\", model=\"gpt4\", tools=[search])\nresult = await agent.run(\"Find the latest news on AI agents\")\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<pre><code># Install from GitHub\npip install git+https://github.com/milad-o/cogent.git\n\n# Or with uv (recommended)\nuv add git+https://github.com/milad-o/cogent.git\n</code></pre> <p>Get Started \u2192</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Native Executor \u2014 High-performance parallel tool execution with zero framework overhead</li> <li>Native Model Support \u2014 OpenAI, Azure, Anthropic, Gemini, Groq, Ollama, Custom endpoints</li> <li>Multi-Agent Patterns \u2014 Supervisor, Pipeline, Mesh, Hierarchical</li> <li>Capabilities \u2014 Filesystem, Web Search, Code Sandbox, Browser, PDF, Shell, MCP, Spreadsheet, and more</li> <li>RAG Pipeline \u2014 Document loading, per-file-type splitting, embeddings, vector stores, retrievers</li> <li>Memory &amp; Persistence \u2014 Conversation history, long-term memory with semantic search</li> <li>Graph Visualization \u2014 Mermaid, Graphviz, ASCII diagrams for agents, patterns, and flows</li> <li>Observability \u2014 Tracing, metrics, progress tracking, structured logging</li> <li>Interceptors \u2014 Budget guards, rate limiting, PII protection, tool gates</li> <li>Resilience \u2014 Retry policies, circuit breakers, fallbacks</li> <li>Human-in-the-Loop \u2014 Tool approval, guidance, interruption handling</li> <li>Streaming \u2014 Real-time token streaming with callbacks</li> <li>Structured Output \u2014 Type-safe responses with Pydantic schemas</li> <li>Reasoning \u2014 Extended thinking mode with chain-of-thought</li> </ul>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Getting Started \u2014 Get started in 5 minutes</li> <li>Agent Documentation \u2014 Learn about the core Agent class</li> <li>Multi-Agent Flow \u2014 Build coordinated multi-agent systems</li> <li>Capabilities \u2014 Explore built-in capabilities</li> <li>Examples \u2014 See working examples</li> </ul>"},{"location":"#latest-release-v1141","title":"Latest Release (v1.14.1)","text":"<p>3-Tier Model API - String Models</p> <ul> <li>\ud83c\udfaf Simple String Models \u2014 <code>Agent(model=\"gpt4\")</code> auto-resolves to gpt-4o</li> <li>\ud83c\udff7\ufe0f 50+ Model Aliases \u2014 <code>gpt5</code>, <code>gpt4</code>, <code>claude</code>, <code>gemini3</code>, <code>mistral</code>, <code>command-a</code>, etc.</li> <li>\ud83d\udd17 Provider Prefix \u2014 <code>\"anthropic:claude\"</code>, <code>\"groq:llama-70b\"</code></li> <li>\u2699\ufe0f Auto-Configuration \u2014 Loads API keys and model overrides from <code>.env</code>, TOML/YAML, or env vars</li> <li>\ud83d\udd04 Backward Compatible \u2014 Existing code works unchanged</li> <li>\ud83e\udde0 3 API Tiers \u2014 String (simple), Factory (4 patterns), Direct (full control)</li> <li>\ud83d\udd0d Auto Provider Detection \u2014 Supports GPT-5, Gemini 3, Mistral Large 3, Command A, and all mainstream models</li> <li>\u2705 74 New Tests \u2014 Comprehensive test coverage for all new features</li> </ul> <p>See CHANGELOG for full version history.</p>"},{"location":"agent/","title":"Agent Module","text":"<p>The <code>cogent.agent</code> module defines the core agent abstraction - autonomous entities that can think, act, and communicate.</p>"},{"location":"agent/#overview","title":"Overview","text":"<p>Agents are the primary actors in the system. Each agent has: - A unique identity and role - Configuration defining its capabilities - Runtime state tracking its activity - Access to tools and the event bus</p> <pre><code>from cogent import Agent\n\n# Simple string model (recommended for v1.14.1+)\nagent = Agent(\n    name=\"Researcher\",\n    model=\"gpt4\",  # Auto-resolves to gpt-4o\n    tools=[search_tool],\n    instructions=\"You are a research assistant.\",\n)\n\n# With provider prefix\nagent = Agent(\n    name=\"Researcher\",\n    model=\"anthropic:claude\",  # Explicit provider\n    tools=[search_tool],\n)\n\n# Medium-level: Factory function\nfrom cogent.models import create_chat\nagent = Agent(\n    name=\"Researcher\",\n    model=create_chat(\"gpt4\"),\n    tools=[search_tool],\n)\n\n# Low-level: Full control\nfrom cogent.models import OpenAIChat\nmodel = OpenAIChat(model=\"gpt-4o\", temperature=0.7)\nagent = Agent(\n    name=\"Researcher\",\n    model=model,\n    tools=[search_tool],\n)\n\nresult = await agent.run(\"Find information about quantum computing\")\n</code></pre>"},{"location":"agent/#core-classes","title":"Core Classes","text":""},{"location":"agent/#agent","title":"Agent","text":"<p>The main agent class with multiple construction patterns:</p> <pre><code>from cogent import Agent\n\n# Simplified API (recommended)\nagent = Agent(\n    name=\"Writer\",\n    model=\"gpt4\",  # String model - auto-resolves to gpt-4o\n    role=\"worker\",  # String: \"worker\", \"supervisor\", \"autonomous\", \"reviewer\"\n    tools=[write_tool],\n    instructions=\"You write compelling content.\",\n)\n\n# With provider prefix for other providers\nagent = Agent(\n    name=\"Writer\",\n    model=\"anthropic:claude-sonnet-4\",\n    role=\"worker\",\n)\n\n# Advanced API with AgentConfig\nfrom cogent.agent import AgentConfig\nfrom cogent.core.enums import AgentRole\nfrom cogent.models import create_chat\n\nconfig = AgentConfig(\n    name=\"Writer\",\n    role=AgentRole.WORKER,\n    model=create_chat(\"gpt4\"),\n    tools=[\"write_poem\", \"write_story\"],\n    resilience_config=ResilienceConfig.aggressive(),\n)\nagent = Agent(config=config)\n</code></pre>"},{"location":"agent/#roleconfig-objects-recommended","title":"RoleConfig Objects (Recommended)","text":"<p>Use role configuration objects for type-safe, immutable role definitions:</p> <pre><code>from cogent import (\n    SupervisorRole,\n    WorkerRole,\n    ReviewerRole,\n    AutonomousRole,\n    CustomRole,\n)\n\n# Supervisor - coordinates workers\nsupervisor = Agent(\n    name=\"Manager\",\n    model=\"gpt4\",  # String model\n    role=SupervisorRole(workers=[\"Analyst\", \"Writer\"]),\n)\n\n# Worker - executes tasks with tools\nworker = Agent(\n    name=\"Analyst\",\n    model=\"claude\",  # Alias for claude-sonnet-4\n    role=WorkerRole(specialty=\"data analysis and visualization\"),\n    tools=[search, analyze],\n)\n\n# Reviewer - evaluates and approves work\nreviewer = Agent(\n    name=\"QA\",\n    model=\"gemini-pro\",  # Alias for gemini-2.5-pro\n    role=ReviewerRole(criteria=[\"accuracy\", \"clarity\", \"completeness\"]),\n)\n\n# Autonomous - independent agent with full capabilities\nautonomous = Agent(\n    name=\"Assistant\",\n    model=\"anthropic:claude-opus-4\",  # Provider prefix\n    role=AutonomousRole(),\n)    tools=[search, write],\n)\n\n# Custom - hybrid role with explicit capability overrides\ncustom = Agent(\n    name=\"TechnicalReviewer\",\n    model=model,\n    role=CustomRole(\n        base_role=AgentRole.REVIEWER,\n        can_use_tools=True,  # Reviewer that can use tools!\n    ),\n    tools=[code_analyzer, linter],\n)\n</code></pre> <p>Benefits of RoleConfig objects: - Type-safe configuration - Immutable (frozen dataclasses) - Built-in prompt enhancement - Clear, explicit role definitions - IDE autocomplete and type checking</p>"},{"location":"agent/#role-specific-parameters-backward-compatible","title":"Role-Specific Parameters (Backward Compatible)","text":"<p>You can also use string/enum roles with parameters:</p> <pre><code># Supervisor - with team members\nsupervisor = Agent(\n    name=\"Manager\",\n    model=model,\n    role=AgentRole.SUPERVISOR,  # or \"supervisor\"\n    workers=[\"analyst\", \"writer\"],  # Adds team members to prompt\n)\n\n# Worker - with specialty description\nworker = Agent(\n    name=\"Analyst\", \n    model=model,\n    role=\"worker\",\n    specialty=\"data analysis and visualization\",  # Adds specialty to prompt\n    tools=[search, analyze],\n)\n\n# Reviewer - with evaluation criteria\nreviewer = Agent(\n    name=\"QA\",\n    model=model,\n    role=\"reviewer\",\n    criteria=[\"accuracy\", \"clarity\", \"completeness\"],  # Adds criteria to prompt\n)\n\n# Autonomous - works independently, can finish\nautonomous = Agent(\n    name=\"Assistant\",\n    model=model,\n    role=\"autonomous\",\n    tools=[search, write],\n)\n</code></pre> <p>Note: While backward compatible, RoleConfig objects are recommended for new code.</p>"},{"location":"agent/#custom-roles-capability-overrides","title":"Custom Roles (Capability Overrides)","text":"<p>Recommended: Use <code>CustomRole</code> for hybrid capabilities:</p> <pre><code>from cogent import CustomRole\nfrom cogent.core import AgentRole\n\n# Reviewer that can use tools\nhybrid_reviewer = Agent(\n    name=\"TechnicalReviewer\",\n    model=model,\n    role=CustomRole(\n        base_role=AgentRole.REVIEWER,\n        can_use_tools=True,  # Override!\n    ),\n    tools=[code_analyzer, linter],\n)\n\n# Worker that can finish and delegate\norchestrator = Agent(\n    name=\"Orchestrator\",\n    model=model,\n    role=CustomRole(\n        base_role=AgentRole.WORKER,\n        can_finish=True,      # Override!\n        can_delegate=True,    # Override!\n    ),\n    tools=[deployment_tool],\n)\n</code></pre> <p>Backward compatible: You can also use capability overrides with string/enum roles:</p> <pre><code># Hybrid: Reviewer that can use tools\nhybrid_reviewer = Agent(\n    name=\"TechnicalReviewer\",\n    model=model,\n    role=\"reviewer\",\n    can_use_tools=True,  # Override! Reviewer normally can't use tools\n    tools=[code_analyzer, linter],\n)\n\n# Custom orchestrator: Worker that can finish and delegate\norchestrator = Agent(\n    name=\"Orchestrator\",\n    model=model,\n    role=\"worker\",\n    can_finish=True,      # Override! Worker normally can't finish\n    can_delegate=True,   # Override! Worker normally can't delegate\n    tools=[deployment_tool],\n)\n</code></pre>"},{"location":"agent/#role-system","title":"Role System","text":"<p>Roles define capabilities (what an agent CAN do) and inject system prompts that guide LLM behavior. They don't define personalities - that comes from your <code>instructions</code>.</p>"},{"location":"agent/#role-capabilities","title":"Role Capabilities","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Role        \u2502 can_finish \u2502 can_delegate \u2502 can_use_tools \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 WORKER      \u2502     \u274c     \u2502      \u274c      \u2502      \u2705       \u2502\n\u2502 SUPERVISOR  \u2502     \u2705     \u2502      \u2705      \u2502      \u274c       \u2502\n\u2502 AUTONOMOUS  \u2502     \u2705     \u2502      \u274c      \u2502      \u2705       \u2502\n\u2502 REVIEWER    \u2502     \u2705     \u2502      \u274c      \u2502      \u274c       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>When to Use: - WORKER: Executes tasks with tools, reports back - SUPERVISOR: Coordinates workers, makes final decisions - AUTONOMOUS: Independent operation, full lifecycle - REVIEWER: Evaluates work, approves/rejects</p>"},{"location":"agent/#how-roles-work","title":"How Roles Work","text":"<p>Roles affect agent behavior in two ways:</p> <p>1. Capability Controls - What the agent is allowed to do: <pre><code># WORKER can use tools but cannot finish\nworker = Agent(name=\"Analyst\", model=model, role=\"worker\", tools=[analyze_tool])\nassert worker.can_use_tools == True\nassert worker.can_finish == False  # Must report to supervisor\n\n# AUTONOMOUS can use tools AND finish\nautonomous = Agent(name=\"Assistant\", model=model, role=\"autonomous\", tools=[search_tool])\nassert autonomous.can_use_tools == True\nassert autonomous.can_finish == True  # Can conclude independently\n</code></pre></p> <p>2. System Prompt Injection - How the LLM thinks:</p> <p>Each role gets a specialized system prompt that guides its behavior:</p> <ul> <li>WORKER: \"Execute tasks using tools... You cannot finish the workflow yourself\"</li> <li>SUPERVISOR: \"Delegate tasks to workers... Provide FINAL ANSWER when complete\"</li> <li>AUTONOMOUS: \"Work independently... Finish when the task is complete\"</li> <li>REVIEWER: \"Evaluate work quality... Approve or request revisions\"</li> </ul> <p>Example - See the difference: <pre><code># WORKER won't conclude\nworker = Agent(name=\"Worker\", model=model, role=\"worker\")\nresult = await worker.run(\"What is Python?\")\n# Response: \"Python is a programming language...\" (no conclusion)\n\n# AUTONOMOUS will conclude\nautonomous = Agent(name=\"Assistant\", model=model, role=\"autonomous\")\nresult = await autonomous.run(\"What is Python?\")\n# Response: \"FINAL ANSWER: Python is a high-level programming language...\"\n</code></pre></p>"},{"location":"agent/#when-to-use-each-role","title":"When to Use Each Role","text":"<p>WORKER - Task execution: <pre><code># \u2705 Good: Has tools, reports results\ndata_analyst = Agent(\n    name=\"DataAnalyst\",\n    model=model,\n    role=\"worker\",\n    tools=[load_data, analyze, plot],\n    instructions=\"Analyze datasets and create visualizations\",\n)\n\n# In a Flow, supervisor coordinates workers\n</code></pre></p> <p>SUPERVISOR - Team coordination: <pre><code># \u2705 Good: Delegates to workers, makes final decisions\nmanager = Agent(\n    name=\"Manager\",\n    model=model,\n    role=\"supervisor\",\n    instructions=\"Coordinate the research team to deliver comprehensive reports\",\n)\n\n# LLM will try to delegate: \"DELEGATE TO researcher: Find information about...\"\n</code></pre></p> <p>AUTONOMOUS - Independent agents: <pre><code># \u2705 Good: Standalone assistant, full capability\nassistant = Agent(\n    name=\"Assistant\",\n    model=model,\n    role=\"autonomous\",\n    tools=[search, calculator, send_email],\n    instructions=\"Help users with their requests\",\n)\n\n# Can use tools AND provide final answers independently\n</code></pre></p> <p>REVIEWER - Quality control: <pre><code># \u2705 Good: Evaluates quality, no tool execution\nqa = Agent(\n    name=\"QualityAssurance\",\n    model=model,\n    role=\"reviewer\",\n    instructions=\"Review code for quality, security, and best practices\",\n)\n\n# LLM focuses on judgment: \"FINAL ANSWER: Approved\" or \"REVISION NEEDED: ...\"\n</code></pre></p>"},{"location":"agent/#capability-overrides","title":"Capability Overrides","text":"<p>Override role capabilities when needed: <pre><code># Hybrid: Reviewer that can use tools\ntech_reviewer = Agent(\n    name=\"TechnicalReviewer\",\n    model=model,\n    role=\"reviewer\",\n    can_use_tools=True,  # Override! Run automated checks\n    tools=[lint_code, run_tests],\n)\n</code></pre></p> <p>See <code>examples/basics/role_behavior.py</code> for real LLM behavior examples.</p>"},{"location":"agent/#memory","title":"Memory","text":"<p>Enable conversation memory for multi-turn interactions:</p> <pre><code>from cogent.agent import InMemorySaver\n\nagent = Agent(\n    name=\"Assistant\",\n    model=model,\n    memory=InMemorySaver(),\n)\n\n# Run with thread-based memory\nresponse = await agent.run(\"Hi, I'm Alice\", thread_id=\"conv-1\")\nresponse = await agent.run(\"What's my name?\", thread_id=\"conv-1\")  # Remembers!\n</code></pre>"},{"location":"agent/#memory-backends","title":"Memory Backends","text":"<ul> <li>InMemorySaver: In-memory storage (for testing)</li> <li>SqliteSaver: SQLite-based persistence (coming soon)</li> <li>Custom: Implement <code>AgentMemory</code> protocol</li> </ul>"},{"location":"agent/#resilience","title":"Resilience","text":"<p>Built-in fault tolerance with retries, circuit breakers, and fallbacks:</p> <pre><code>from cogent.agent import ResilienceConfig, RetryPolicy\n\nagent = Agent(\n    name=\"Worker\",\n    model=model,\n    resilience=ResilienceConfig(\n        retry_policy=RetryPolicy(\n            max_retries=3,\n            base_delay=1.0,\n            strategy=\"exponential\",\n        ),\n    ),\n)\n</code></pre>"},{"location":"agent/#resilience-components","title":"Resilience Components","text":"<ul> <li>RetryPolicy: Configure retry behavior with exponential/linear backoff</li> <li>CircuitBreaker: Prevent cascading failures</li> <li>FallbackRegistry: Define fallback behaviors for failures</li> </ul>"},{"location":"agent/#human-in-the-loop-hitl","title":"Human-in-the-Loop (HITL)","text":"<p>Enable human oversight for sensitive operations:</p> <pre><code>agent = Agent(\n    name=\"Executor\",\n    model=model,\n    tools=[delete_file, send_email],\n    interrupt_on={\n        \"tools\": [\"delete_file\", \"send_email\"],  # Require approval\n    },\n)\n\ntry:\n    result = await agent.run(\"Delete temp files\")\nexcept InterruptedException as e:\n    # Human reviews pending action\n    decision = HumanDecision(approved=True)\n    result = await agent.resume(e.state, decision)\n</code></pre>"},{"location":"agent/#reasoning","title":"Reasoning","text":"<p>Enable extended thinking for complex problems with AI-controlled reasoning rounds.</p>"},{"location":"agent/#basic-usage","title":"Basic Usage","text":"<pre><code>from cogent import Agent\nfrom cogent.agent.reasoning import ReasoningConfig\n\n# Simple: Enable with defaults\nagent = Agent(\n    name=\"Analyst\",\n    model=model,\n    reasoning=True,  # Default config\n)\n\nresult = await agent.run(\"Analyze this complex problem...\")\n</code></pre>"},{"location":"agent/#custom-configuration","title":"Custom Configuration","text":"<pre><code># Full control with ReasoningConfig\nagent = Agent(\n    name=\"DeepThinker\",\n    model=model,\n    reasoning=ReasoningConfig(\n        max_thinking_rounds=15,         # AI decides when ready (up to 15)\n        style=ReasoningStyle.CRITICAL,  # Critical reasoning style\n        show_thinking=True,             # Include thoughts in output\n    ),\n)\n</code></pre>"},{"location":"agent/#per-call-overrides","title":"Per-Call Overrides","text":"<p>Enable or customize reasoning for specific calls:</p> <pre><code># Agent without reasoning by default\nagent = Agent(name=\"Helper\", model=model, reasoning=False)\n\n# Simple task - no reasoning\nresult = await agent.run(\"What time is it?\")\n\n# Complex task - enable reasoning\nresult = await agent.run(\n    \"Analyze this codebase architecture\",\n    reasoning=True,  # Enable for this call\n)\n\n# Very complex - custom config\nresult = await agent.run(\n    \"Debug this complex issue\",\n    reasoning=ReasoningConfig(\n        max_thinking_rounds=10,\n        style=ReasoningStyle.ANALYTICAL,\n    ),\n)\n</code></pre>"},{"location":"agent/#reasoning-styles","title":"Reasoning Styles","text":"<ul> <li><code>ANALYTICAL</code>: Step-by-step logical breakdown (default)</li> <li><code>EXPLORATORY</code>: Consider multiple approaches</li> <li><code>CRITICAL</code>: Question assumptions, find flaws</li> <li><code>CREATIVE</code>: Generate novel solutions</li> </ul>"},{"location":"agent/#ai-controlled-rounds","title":"AI-Controlled Rounds","text":"<p>The AI signals when reasoning is complete via <code>&lt;ready&gt;true&lt;/ready&gt;</code> tags. The <code>max_thinking_rounds</code> is a safety limit, not a fixed count:</p> <pre><code>ReasoningConfig.standard()  # max 10 rounds (safety net)\nReasoningConfig.deep()      # max 15 rounds (complex problems)\n</code></pre>"},{"location":"agent/#structured-output","title":"Structured Output","text":"<p>Enforce response schemas with validation:</p> <pre><code>from pydantic import BaseModel, Field\n\nclass ContactInfo(BaseModel):\n    name: str = Field(description=\"Full name\")\n    email: str = Field(description=\"Email address\")\n    phone: str | None = Field(None, description=\"Phone number\")\n\nagent = Agent(\n    name=\"Extractor\",\n    model=model,\n    output=ContactInfo,  # Enforce schema\n)\n\nresult = await agent.run(\"Extract: John Doe, john@acme.com\")\nprint(result.data)  # ContactInfo(name=\"John Doe\", ...)\n</code></pre>"},{"location":"agent/#taskboard","title":"TaskBoard","text":"<p>Human-like task tracking for complex workflows:</p> <pre><code>agent = Agent(\n    name=\"Researcher\",\n    model=model,\n    tools=[search, summarize],\n    taskboard=True,  # Adds task management tools\n)\n\nresult = await agent.run(\"Research Python async patterns\")\nprint(agent.taskboard.summary())\n</code></pre>"},{"location":"agent/#streaming","title":"Streaming","text":"<p>Enable token-by-token streaming:</p> <pre><code>agent = Agent(\n    name=\"Writer\",\n    model=model,\n    stream=True,\n)\n\nasync for chunk in agent.run(\"Write a story\", stream=True):\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"agent/#spawning","title":"Spawning","text":"<p>Dynamic agent creation at runtime:</p> <pre><code>from cogent.agent import SpawningConfig, AgentSpec\n\nagent = Agent(\n    name=\"Coordinator\",\n    model=model,\n    spawning=SpawningConfig(\n        allowed_specs=[\n            AgentSpec(name=\"researcher\", tools=[\"search\"]),\n            AgentSpec(name=\"writer\", tools=[\"write\"]),\n        ],\n    ),\n)\n\n# Agent can spawn sub-agents during execution\nresult = await agent.run(\"Research and write about AI\")\n</code></pre>"},{"location":"agent/#observability","title":"Observability","text":"<p>Built-in observability for standalone usage:</p> <pre><code>from cogent import Agent\nfrom cogent.observability import ObservabilityLevel\n\n# Boolean shorthand\nagent = Agent(name=\"Worker\", model=model, verbosity=True)  # Progress level\n\n# String levels\nagent = Agent(name=\"Worker\", model=model, verbosity=\"debug\")\n\n# Enum (explicit)\nagent = Agent(name=\"Worker\", model=model, verbosity=ObservabilityLevel.DEBUG)\n\n# Integer (0-5)\nagent = Agent(name=\"Worker\", model=model, verbosity=4)  # DEBUG\n\n# Advanced: Full control with observer\nfrom cogent.observability import Observer\n\nobserver = Observer.debug()\nagent = Agent(name=\"Worker\", model=model, observer=observer)\n</code></pre> <p>Verbosity levels:</p> Level Int String Description <code>OFF</code> 0 <code>\"off\"</code> No output <code>RESULT</code> 1 <code>\"result\"</code>, <code>\"minimal\"</code> Only final results <code>PROGRESS</code> 2 <code>\"progress\"</code>, <code>\"normal\"</code> Key milestones (default for <code>True</code>) <code>DETAILED</code> 3 <code>\"detailed\"</code>, <code>\"verbose\"</code> Tool calls, timing <code>DEBUG</code> 4 <code>\"debug\"</code> Everything including internal events <code>TRACE</code> 5 <code>\"trace\"</code> Maximum detail + execution graph <p>Priority: <code>observer</code> parameter takes precedence over <code>verbosity</code>.</p>"},{"location":"agent/#api-reference","title":"API Reference","text":""},{"location":"agent/#agent-methods","title":"Agent Methods","text":"Method Description <code>run(task)</code> Execute a task and return result <code>chat(message, thread_id)</code> Chat with memory support <code>think(prompt)</code> Single reasoning step <code>stream_chat(message)</code> Streaming chat response <code>resume(state, decision)</code> Resume after HITL interrupt"},{"location":"agent/#agentconfig-fields","title":"AgentConfig Fields","text":"Field Type Description <code>name</code> <code>str</code> Agent name <code>role</code> <code>AgentRole</code> Agent role <code>model</code> <code>BaseChatModel</code> Chat model <code>tools</code> <code>list[str]</code> Tool names <code>system_prompt</code> <code>str</code> System instructions <code>resilience_config</code> <code>ResilienceConfig</code> Fault tolerance <code>interrupt_on</code> <code>dict</code> HITL triggers <code>stream</code> <code>bool</code> Enable streaming"},{"location":"agent/#exports","title":"Exports","text":"<pre><code>from cogent.agent import (\n    # Core\n    Agent,\n    AgentConfig,\n    AgentState,\n    # Memory\n    AgentMemory,\n    MemorySnapshot,\n    InMemorySaver,\n    ThreadConfig,\n    # Roles\n    RoleBehavior,\n    get_role_prompt,\n    get_role_behavior,\n    # Resilience\n    RetryStrategy,\n    RetryPolicy,\n    CircuitBreaker,\n    ResilienceConfig,\n    ToolResilience,\n    # HITL\n    InterruptReason,\n    HumanDecision,\n    InterruptedException,\n    # TaskBoard\n    TaskBoard,\n    TaskBoardConfig,\n    Task,\n    TaskStatus,\n    # Reasoning\n    ReasoningConfig,\n    ReasoningStyle,\n    ThinkingStep,\n    # Output\n    ResponseSchema,\n    StructuredResult,\n    # Spawning\n    AgentSpec,\n    SpawningConfig,\n    SpawnManager,\n)\n</code></pre>"},{"location":"capabilities/","title":"Capabilities Module","text":"<p>The <code>cogent.capabilities</code> module provides composable tools that plug into any agent. Capabilities are reusable building blocks that add domain-specific functionality.</p>"},{"location":"capabilities/#overview","title":"Overview","text":"<p>Capabilities encapsulate related functionality and expose it through native tools. Each capability: - Provides tools for the agent to use - May maintain internal state (graphs, caches, connections) - Can be initialized/shutdown with the agent lifecycle</p> <pre><code>from cogent import Agent\nfrom cogent.capabilities import (\n    KnowledgeGraph, FileSystem, WebSearch, CodeSandbox,\n    MCP, Spreadsheet, PDF, Browser, Shell, Summarizer,\n)\n\nagent = Agent(\n    name=\"Assistant\",\n    model=model,\n    capabilities=[\n        KnowledgeGraph(),                     # Entity/relationship memory\n        FileSystem(allowed_paths=[\"./data\"]), # Sandboxed file operations\n        WebSearch(),                          # Web search and fetching\n        CodeSandbox(),                        # Safe Python execution\n    ],\n)\n</code></pre>"},{"location":"capabilities/#available-capabilities","title":"Available Capabilities","text":""},{"location":"capabilities/#knowledgegraph","title":"KnowledgeGraph","text":"<p>Entity/relationship memory with multi-hop reasoning and multiple storage backends.</p> <pre><code>from cogent.capabilities import KnowledgeGraph\n\n# In-memory (default, no persistence)\nkg = KnowledgeGraph()\n\n# In-memory with auto-save to file\nkg = KnowledgeGraph(backend=\"memory\", path=\"memory.json\", auto_save=True)\n\n# SQLite for persistence\nkg = KnowledgeGraph(backend=\"sqlite\", path=\"knowledge.db\")\n\n# JSON file with auto-save\nkg = KnowledgeGraph(backend=\"json\", path=\"knowledge.json\")\n\n# Custom backend instance\nfrom cogent.capabilities.knowledge_graph.backends import GraphBackend\ncustom_backend = MyCustomBackend()  # Your implementation\nkg = KnowledgeGraph(backend=custom_backend)\n\nagent = Agent(\n    name=\"Researcher\",\n    model=model,\n    capabilities=[kg],\n)\n</code></pre> <p>Backend Switching:</p> <p>Switch backends dynamically with optional data migration:</p> <pre><code># Start with in-memory\nkg = KnowledgeGraph()\nkg.remember(\"Alice\", \"Person\", {\"role\": \"Engineer\"})\n\n# Switch to SQLite with migration\nkg.set_backend(\"sqlite\", path=\"knowledge.db\", migrate=True)\n\n# All data is now persisted, continue using same instance\nkg.remember(\"Bob\", \"Person\", {\"role\": \"Manager\"})\n\n# Switch to JSON\nkg.set_backend(\"json\", path=\"knowledge.json\", migrate=True)\n</code></pre> <p>Tools provided: | Tool | Description | |------|-------------| | <code>remember</code> | Store entities with attributes (dict or JSON string) | | <code>recall</code> | Retrieve information about entities | | <code>connect</code> | Create relationships between entities | | <code>query_knowledge</code> | Query relationships (source/relation/target params) | | <code>forget</code> | Remove entities and their relationships | | <code>list_knowledge</code> | List all entities, optionally filtered by type |</p> <p>Backends: - <code>memory</code>: Fast in-memory (uses networkx if available) - optionally auto-save to file - <code>sqlite</code>: Persistent SQLite for large graphs - always saves - <code>json</code>: Simple JSON file with auto-save - always saves - <code>neo4j</code>: Production graph database (requires neo4j package) - always saves - Custom: Extend <code>GraphBackend</code> for your own implementation</p> <p>Visualization:</p> <p>KnowledgeGraph provides a three-level API for visualization:</p> <pre><code>kg = KnowledgeGraph()\n# ... add entities and relationships ...\n\n# 1. LOW-LEVEL: kg.mermaid() - raw Mermaid code\ncode = kg.mermaid(direction=\"LR\")\nprint(code)  # Raw Mermaid diagram code\n\n# 2. MEDIUM-LEVEL: kg.render(format) - multiple formats\nascii_art = kg.render(\"ascii\")     # Terminal-friendly\nhtml = kg.render(\"html\")           # Interactive HTML\npng_bytes = kg.render(\"png\")       # PNG image bytes\nsvg_bytes = kg.render(\"svg\")       # SVG vector bytes\n\n# 3. HIGH-LEVEL: kg.display() - Jupyter inline rendering\nkg.display()  # Renders inline in Jupyter notebook\nkg.display(direction=\"TB\", show_attributes=True)\n\n# GRAPHVIEW: kg.visualize() - full control\nview = kg.visualize(direction=\"LR\", group_by_type=True)\nview.mermaid()   # Mermaid code\nview.ascii()     # ASCII art  \nview.url()       # Shareable mermaid.ink URL\nview.save(\"graph.mmd\")   # Mermaid source\nview.save(\"graph.html\")  # Interactive HTML\nview.save(\"graph.png\")   # PNG image\nview.save(\"graph.svg\")   # SVG vector\nview.save(\"graph.dot\")   # Graphviz DOT\n</code></pre> <p>Visualization options: - <code>direction</code>: Layout direction - <code>\"LR\"</code> (left-right), <code>\"TB\"</code> (top-bottom), <code>\"BT\"</code>, <code>\"RL\"</code> - <code>group_by_type</code>: Group entities by type in subgraphs (default: True) - <code>show_attributes</code>: Display entity attributes in labels (default: False)</p> <p>Entity colors: - Person: Blue (<code>#60a5fa</code>) - Company/Organization: Green (<code>#7eb36a</code>) - Location: Orange (<code>#f59e0b</code>) - Event: Purple (<code>#9b59b6</code>) - Generic: Gray (<code>#94a3b8</code>)</p> <p>Tool API (improved in v1.8.3): <pre><code># Query with structured parameters (NEW)\nquery_knowledge(source=None, relation=\"works_at\", target=\"TechCorp\")\n# Returns: Who works at TechCorp?\n\n# Remember with dict attributes (NEW - preferred)\nremember(entity=\"Alice\", entity_type=\"Person\", attributes={\"role\": \"CEO\", \"age\": 35})\n\n# Also accepts JSON string for backward compatibility\nremember(entity=\"Alice\", entity_type=\"Person\", attributes='{\"role\": \"CEO\"}')\n</code></pre></p>"},{"location":"capabilities/#filesystem","title":"FileSystem","text":"<p>Sandboxed file operations with security controls.</p> <pre><code>from cogent.capabilities import FileSystem\n\n# Read-only access to docs\nfs = FileSystem(\n    allowed_paths=[\"./docs\"],\n    allow_write=False,\n)\n\n# Full access with security\nfs = FileSystem(\n    allowed_paths=[\"./workspace\"],\n    deny_patterns=[\"*.env\", \"*.key\", \".git/*\"],\n    allow_delete=True,\n    max_file_size=10 * 1024 * 1024,  # 10MB\n)\n\nagent = Agent(name=\"Worker\", model=model, capabilities=[fs])\n</code></pre> <p>Tools provided: | Tool | Description | |------|-------------| | <code>read_file</code> | Read file contents | | <code>write_file</code> | Write/update files | | <code>list_directory</code> | List directory contents | | <code>search_files</code> | Search by pattern/content | | <code>copy_file</code> | Copy files | | <code>move_file</code> | Move/rename files | | <code>delete_file</code> | Delete files (if enabled) |</p> <p>Parameters: - <code>allowed_paths</code>: Directories the agent can access - <code>deny_patterns</code>: Glob patterns to block (e.g., <code>[\"*.env\", \".git/*\"]</code>) - <code>max_file_size</code>: Maximum file size in bytes (default: 10MB) - <code>allow_write</code>: Enable write operations (default: True) - <code>allow_delete</code>: Enable delete operations (default: False)</p>"},{"location":"capabilities/#websearch","title":"WebSearch","text":"<p>Web search and page fetching using DuckDuckGo (free, no API key).</p> <pre><code>from cogent.capabilities import WebSearch\n\n# Default configuration\nws = WebSearch()\n\n# Custom settings\nws = WebSearch(\n    max_results=10,\n    timeout=30,\n    include_raw_html=False,\n)\n\nagent = Agent(name=\"Researcher\", model=model, capabilities=[ws])\n</code></pre> <p>Tools provided: | Tool | Description | |------|-------------| | <code>web_search</code> | Search the web | | <code>news_search</code> | Search news articles | | <code>fetch_page</code> | Fetch and extract page content | | <code>fetch_multiple</code> | Fetch multiple URLs concurrently |</p> <p>Requires: <code>uv add ddgs</code></p>"},{"location":"capabilities/#codesandbox","title":"CodeSandbox","text":"<p>Safe Python code execution with security controls.</p> <pre><code>from cogent.capabilities import CodeSandbox\n\nsandbox = CodeSandbox(\n    timeout=30,           # Execution timeout in seconds\n    max_output_size=10000, # Max output characters\n    allow_imports=[\"math\", \"json\", \"re\"],  # Allowed imports\n)\n\nagent = Agent(name=\"Coder\", model=model, capabilities=[sandbox])\n</code></pre> <p>Tools provided: | Tool | Description | |------|-------------| | <code>execute_python</code> | Run Python code | | <code>run_function</code> | Execute a specific function | | <code>eval_expression</code> | Evaluate an expression |</p> <p>Security features: - Blocked dangerous imports (os, subprocess, socket, etc.) - Resource limits (time, memory) - AST-based code analysis - Sandboxed execution environment</p>"},{"location":"capabilities/#mcp-model-context-protocol","title":"MCP (Model Context Protocol)","text":"<p>Connect to local and remote MCP servers.</p> <pre><code>from cogent.capabilities import MCP\n\n# Local server (stdio)\nmcp_local = MCP.stdio(\n    command=\"uv\",\n    args=[\"run\", \"my-mcp-server\"],\n)\n\n# Remote server (HTTP/SSE)\nmcp_http = MCP.http(\"https://api.example.com/mcp\")\n\n# WebSocket\nmcp_ws = MCP.websocket(\"ws://localhost:8766\")\n\n# Multiple servers\nagent = Agent(\n    name=\"Assistant\",\n    model=model,\n    capabilities=[\n        MCP.stdio(command=\"npx\", args=[\"-y\", \"@modelcontextprotocol/server-filesystem\", \".\"]),\n        MCP.http(\"https://weather-api.example.com/mcp\"),\n    ],\n)\n</code></pre> <p>Transport types: - <code>STDIO</code>: Local process communication - <code>HTTP</code>: HTTP/Streamable HTTP - <code>SSE</code>: Server-Sent Events - <code>WEBSOCKET</code>: WebSocket connection</p>"},{"location":"capabilities/#spreadsheet","title":"Spreadsheet","text":"<p>Excel and CSV file operations.</p> <pre><code>from cogent.capabilities import Spreadsheet\n\nss = Spreadsheet(\n    default_format=\"xlsx\",\n    max_rows=100000,\n)\n\nagent = Agent(name=\"Analyst\", model=model, capabilities=[ss])\n</code></pre> <p>Tools provided: | Tool | Description | |------|-------------| | <code>read_spreadsheet</code> | Read Excel/CSV files | | <code>write_spreadsheet</code> | Create/update spreadsheets | | <code>query_spreadsheet</code> | Query with filters | | <code>spreadsheet_stats</code> | Get statistics |</p> <p>Requires: <code>uv add openpyxl pandas</code></p>"},{"location":"capabilities/#browser","title":"Browser","text":"<p>Headless browser automation with Playwright.</p> <pre><code>from cogent.capabilities import Browser\n\nbrowser = Browser(\n    headless=True,\n    timeout=30000,\n)\n\nagent = Agent(name=\"WebAgent\", model=model, capabilities=[browser])\n</code></pre> <p>Tools provided: | Tool | Description | |------|-------------| | <code>browse_url</code> | Navigate to URL | | <code>click_element</code> | Click on elements | | <code>fill_form</code> | Fill form fields | | <code>screenshot</code> | Take screenshots | | <code>extract_content</code> | Extract page content |</p> <p>Requires: <code>uv add playwright &amp;&amp; playwright install</code></p>"},{"location":"capabilities/#shell","title":"Shell","text":"<p>Sandboxed terminal command execution.</p> <pre><code>from cogent.capabilities import Shell\n\nshell = Shell(\n    allowed_commands=[\"ls\", \"cat\", \"grep\", \"find\"],\n    working_dir=\"./workspace\",\n    timeout=30,\n)\n\nagent = Agent(name=\"SysAdmin\", model=model, capabilities=[shell])\n</code></pre> <p>Tools provided: | Tool | Description | |------|-------------| | <code>run_command</code> | Execute shell command | | <code>run_script</code> | Execute shell script |</p> <p>Security features: - Command allowlist/blocklist - Working directory sandboxing - Timeout enforcement - Output size limits</p>"},{"location":"capabilities/#summarizer","title":"Summarizer","text":"<p>Document summarization with multiple strategies.</p> <pre><code>from cogent.capabilities import Summarizer, SummarizerConfig\n\nsummarizer = Summarizer(\n    config=SummarizerConfig(\n        strategy=\"map_reduce\",  # or \"refine\", \"hierarchical\"\n        chunk_size=4000,\n        max_concurrency=5,\n    ),\n)\n\nagent = Agent(name=\"Summarizer\", model=model, capabilities=[summarizer])\n</code></pre> <p>Tools provided: | Tool | Description | |------|-------------| | <code>summarize_text</code> | Summarize text content | | <code>summarize_file</code> | Summarize file contents | | <code>summarize_url</code> | Fetch and summarize URL |</p> <p>Strategies: - <code>map_reduce</code>: Parallel chunk summarization + combine - <code>refine</code>: Iterative refinement through chunks - <code>hierarchical</code>: Multi-level summarization tree</p>"},{"location":"capabilities/#codebaseanalyzer","title":"CodebaseAnalyzer","text":"<p>Python codebase analysis with AST parsing.</p> <pre><code>from cogent.capabilities import CodebaseAnalyzer\n\nanalyzer = CodebaseAnalyzer(\n    root_path=\"./src\",\n    include_patterns=[\"*.py\"],\n)\n\nagent = Agent(name=\"CodeReviewer\", model=model, capabilities=[analyzer])\n</code></pre> <p>Tools provided: | Tool | Description | |------|-------------| | <code>analyze_file</code> | Analyze Python file structure | | <code>find_definitions</code> | Find classes/functions | | <code>find_usages</code> | Find symbol usages | | <code>get_dependencies</code> | Analyze imports |</p>"},{"location":"capabilities/#creating-custom-capabilities","title":"Creating Custom Capabilities","text":"<p>Extend <code>BaseCapability</code> to create your own:</p> <pre><code>from cogent.capabilities.base import BaseCapability\nfrom cogent.tools import tool, BaseTool\n\nclass MyCapability(BaseCapability):\n    @property\n    def name(self) -&gt; str:\n        return \"my_capability\"\n\n    @property\n    def description(self) -&gt; str:\n        return \"Does something useful\"\n\n    @property\n    def tools(self) -&gt; list[BaseTool]:\n        return [self._my_tool()]\n\n    def _my_tool(self):\n        @tool\n        def do_something(x: str) -&gt; str:\n            '''Do something useful.'''\n            return f\"Did: {x}\"\n        return do_something\n\n    async def initialize(self, agent) -&gt; None:\n        \"\"\"Called when attached to agent.\"\"\"\n        pass\n\n    async def shutdown(self) -&gt; None:\n        \"\"\"Called when agent shuts down.\"\"\"\n        pass\n</code></pre>"},{"location":"capabilities/#exports","title":"Exports","text":"<pre><code>from cogent.capabilities import (\n    # Base class\n    BaseCapability,\n    # Capabilities\n    Browser,\n    CodebaseAnalyzer,\n    CodeSandbox,\n    FileSystem,\n    KnowledgeGraph,\n    MCP,\n    MCPServerConfig,\n    MCPTransport,\n    PDF,\n    Shell,\n    Spreadsheet,\n    Summarizer,\n    SummarizerConfig,\n    WebSearch,\n)\n</code></pre>"},{"location":"context/","title":"RunContext Module","text":"<p>The <code>cogent.context</code> module provides invocation-scoped context for dependency injection into tools and interceptors.</p>"},{"location":"context/#overview","title":"Overview","text":"<p>RunContext enables passing typed data to tools and interceptors at invocation time without global state:</p> <pre><code>from dataclasses import dataclass\nfrom cogent import Agent, RunContext, tool\n\n@dataclass\nclass AppContext(RunContext):\n    user_id: str\n    db: Database\n    api_key: str\n\n@tool\ndef get_user_data(ctx: RunContext) -&gt; str:\n    \"\"\"Get data for the current user.\"\"\"\n    user = ctx.db.get_user(ctx.user_id)\n    return f\"User: {user.name}\"\n\nagent = Agent(name=\"assistant\", model=model, tools=[get_user_data])\n\nresult = await agent.run(\n    \"Get my profile data\",\n    context=AppContext(user_id=\"123\", db=db, api_key=key),\n)\n</code></pre>"},{"location":"context/#creating-custom-contexts","title":"Creating Custom Contexts","text":""},{"location":"context/#basic-context","title":"Basic Context","text":"<pre><code>from dataclasses import dataclass\nfrom cogent import RunContext\n\n@dataclass\nclass MyContext(RunContext):\n    user_id: str\n    session_id: str\n    permissions: list[str] = field(default_factory=list)\n\n    def has_permission(self, perm: str) -&gt; bool:\n        return perm in self.permissions\n</code></pre>"},{"location":"context/#with-services","title":"With Services","text":"<pre><code>@dataclass\nclass ServiceContext(RunContext):\n    db: Database\n    cache: Redis\n    api_client: APIClient\n    logger: Logger\n\n    async def get_user(self, user_id: str) -&gt; User:\n        \"\"\"Get user with caching.\"\"\"\n        cached = await self.cache.get(f\"user:{user_id}\")\n        if cached:\n            return User.from_json(cached)\n        user = await self.db.get_user(user_id)\n        await self.cache.set(f\"user:{user_id}\", user.to_json())\n        return user\n</code></pre>"},{"location":"context/#using-context-in-tools","title":"Using Context in Tools","text":""},{"location":"context/#accessing-context","title":"Accessing Context","text":"<pre><code>from cogent import tool, RunContext\n\n@tool\ndef get_user_orders(ctx: RunContext) -&gt; str:\n    \"\"\"Get orders for the current user.\"\"\"\n    # Access context properties\n    user_id = ctx.user_id\n    orders = ctx.db.get_orders(user_id)\n    return f\"Found {len(orders)} orders\"\n\n@tool\ndef admin_action(action: str, ctx: RunContext) -&gt; str:\n    \"\"\"Perform admin action (requires admin permission).\"\"\"\n    if not ctx.has_permission(\"admin\"):\n        return \"Permission denied\"\n    return f\"Performed: {action}\"\n</code></pre>"},{"location":"context/#context-with-other-parameters","title":"Context with Other Parameters","text":"<pre><code>@tool\ndef search_with_context(\n    query: str,\n    limit: int = 10,\n    ctx: RunContext = None,  # Context is optional\n) -&gt; str:\n    \"\"\"Search with user context.\"\"\"\n    if ctx and ctx.user_id:\n        # Personalized search\n        return personalized_search(query, ctx.user_id, limit)\n    return generic_search(query, limit)\n</code></pre>"},{"location":"context/#using-context-in-interceptors","title":"Using Context in Interceptors","text":"<pre><code>from cogent.interceptors import Interceptor, InterceptContext, InterceptResult\n\nclass PermissionInterceptor(Interceptor):\n    async def intercept(\n        self,\n        phase: Phase,\n        context: InterceptContext,\n    ) -&gt; InterceptResult:\n        run_ctx = context.run_context\n\n        # Check permissions\n        if not run_ctx.has_permission(\"use_agent\"):\n            return InterceptResult.stop(\"Permission denied\")\n\n        return InterceptResult.continue_()\n</code></pre>"},{"location":"context/#context-metadata","title":"Context Metadata","text":"<p>The base <code>RunContext</code> includes a metadata dict for extension:</p> <pre><code>from cogent import RunContext\n\nctx = RunContext(metadata={\n    \"request_id\": \"req-123\",\n    \"correlation_id\": \"corr-456\",\n    \"trace_id\": \"trace-789\",\n})\n\n# Access metadata\nrequest_id = ctx.get(\"request_id\")\nrequest_id = ctx.metadata.get(\"request_id\")\n\n# Create new context with additional metadata\nnew_ctx = ctx.with_metadata(\n    timestamp=datetime.now(),\n    version=\"1.0\",\n)\n</code></pre>"},{"location":"context/#passing-context-to-agents","title":"Passing Context to Agents","text":"<pre><code>from cogent import Agent\n\nagent = Agent(name=\"assistant\", model=model, tools=[...])\n\n# Pass context at run time\nresult = await agent.run(\n    \"Perform action\",\n    context=MyContext(\n        user_id=\"user-123\",\n        session_id=\"sess-456\",\n        permissions=[\"read\", \"write\"],\n    ),\n)\n</code></pre>"},{"location":"context/#context-immutability","title":"Context Immutability","text":"<p>Context should be treated as immutable during execution:</p> <pre><code>@dataclass(frozen=True)  # Enforce immutability\nclass ImmutableContext(RunContext):\n    user_id: str\n    tenant_id: str\n\n# To \"modify\", create a new instance\nnew_ctx = ctx.with_metadata(extra=\"value\")\n</code></pre>"},{"location":"context/#request-scoped-context","title":"Request-Scoped Context","text":"<p>Common pattern for web applications:</p> <pre><code>from fastapi import FastAPI, Depends\nfrom cogent import Agent, RunContext\n\napp = FastAPI()\n\n@dataclass\nclass RequestContext(RunContext):\n    user_id: str\n    tenant_id: str\n    trace_id: str\n    db: AsyncSession\n\nasync def get_context(\n    request: Request,\n    db: AsyncSession = Depends(get_db),\n) -&gt; RequestContext:\n    return RequestContext(\n        user_id=request.state.user_id,\n        tenant_id=request.state.tenant_id,\n        trace_id=request.headers.get(\"X-Trace-ID\"),\n        db=db,\n    )\n\n@app.post(\"/chat\")\nasync def chat(\n    message: str,\n    context: RequestContext = Depends(get_context),\n):\n    agent = get_agent()\n    result = await agent.run(message, context=context)\n    return {\"response\": result.output}\n</code></pre>"},{"location":"context/#multi-tenant-context","title":"Multi-Tenant Context","text":"<pre><code>@dataclass\nclass TenantContext(RunContext):\n    tenant_id: str\n    tenant_config: dict\n    allowed_tools: list[str]\n    rate_limit: int\n\n    def can_use_tool(self, tool_name: str) -&gt; bool:\n        return tool_name in self.allowed_tools\n\n# Use in agent\nresult = await agent.run(\n    \"Query\",\n    context=TenantContext(\n        tenant_id=\"acme-corp\",\n        tenant_config={\"max_tokens\": 4000},\n        allowed_tools=[\"search\", \"calculate\"],\n        rate_limit=100,\n    ),\n)\n</code></pre>"},{"location":"context/#testing-with-context","title":"Testing with Context","text":"<pre><code>import pytest\nfrom unittest.mock import Mock\n\n@pytest.fixture\ndef test_context():\n    return MyContext(\n        user_id=\"test-user\",\n        db=Mock(),\n        cache=Mock(),\n    )\n\nasync def test_tool_with_context(test_context):\n    result = await get_user_orders.__wrapped__(ctx=test_context)\n    assert \"orders\" in result\n</code></pre>"},{"location":"context/#api-reference","title":"API Reference","text":""},{"location":"context/#runcontext","title":"RunContext","text":"<pre><code>@dataclass\nclass RunContext:\n    metadata: dict[str, Any] = field(default_factory=dict)\n\n    def get(self, key: str, default: Any = None) -&gt; Any:\n        \"\"\"Get metadata value by key.\"\"\"\n\n    def with_metadata(self, **kwargs) -&gt; RunContext:\n        \"\"\"Create new context with additional metadata.\"\"\"\n</code></pre>"},{"location":"context/#usage-patterns","title":"Usage Patterns","text":"Pattern Description Tool injection <code>def my_tool(ctx: RunContext)</code> Optional context <code>def my_tool(arg: str, ctx: RunContext = None)</code> Interceptor access <code>context.run_context</code> Metadata access <code>ctx.get(\"key\")</code> or <code>ctx.metadata[\"key\"]</code> Extend context <code>ctx.with_metadata(key=\"value\")</code>"},{"location":"core/","title":"Core Module","text":"<p>The <code>cogent.core</code> module provides foundational types, enums, utilities, and dependency injection used throughout the framework.</p>"},{"location":"core/#overview","title":"Overview","text":"<p>The core module defines: - Enums for status types, event types, and roles - Native message types compatible with all LLM providers - Utility functions for IDs, timestamps, etc. - RunContext for dependency injection (tools and interceptors) - Reactive utilities for event-driven systems (idempotency, retries, delays)</p> <pre><code>from cogent.core import (\n    # Enums\n    TaskStatus,\n    AgentStatus,\n    EventType,\n    Priority,\n    AgentRole,\n    # Context\n    RunContext,\n    EMPTY_CONTEXT,\n    # Utilities\n    generate_id,\n    now_utc,\n    # Reactive utilities\n    IdempotencyGuard,\n    RetryBudget,\n    emit_later,\n    jittered_delay,\n    Stopwatch,\n)\n</code></pre>"},{"location":"core/#enums","title":"Enums","text":""},{"location":"core/#taskstatus","title":"TaskStatus","text":"<p>Task lifecycle states:</p> <pre><code>from cogent.core import TaskStatus\n\nstatus = TaskStatus.RUNNING\n\n# Check state categories\nstatus.is_terminal()  # COMPLETED, FAILED, CANCELLED\nstatus.is_active()    # RUNNING, SPAWNING\n</code></pre> Status Description <code>PENDING</code> Task created, not yet scheduled <code>SCHEDULED</code> Task scheduled for execution <code>BLOCKED</code> Task waiting on dependencies <code>RUNNING</code> Task actively executing <code>SPAWNING</code> Task creating subtasks <code>COMPLETED</code> Task finished successfully <code>FAILED</code> Task failed with error <code>CANCELLED</code> Task was cancelled"},{"location":"core/#agentstatus","title":"AgentStatus","text":"<p>Agent lifecycle states:</p> <pre><code>from cogent.core import AgentStatus\n\nstatus = AgentStatus.THINKING\n\n# Check state categories\nstatus.is_available()  # Can accept new work (IDLE)\nstatus.is_working()    # Currently working (THINKING, ACTING)\n</code></pre> Status Description <code>IDLE</code> Agent ready for new work <code>THINKING</code> Agent reasoning/planning <code>ACTING</code> Agent executing tools <code>WAITING</code> Agent waiting for response <code>ERROR</code> Agent in error state <code>OFFLINE</code> Agent unavailable"},{"location":"core/#eventtype","title":"EventType","text":"<p>System events for observability and coordination:</p> <pre><code>from cogent.core import EventType\n\nevent = EventType.TASK_COMPLETED\nevent.category  # \"task\"\n</code></pre> <p>Categories:</p> Category Examples <code>system</code> SYSTEM_STARTED, SYSTEM_STOPPED, SYSTEM_ERROR <code>task</code> TASK_CREATED, TASK_STARTED, TASK_COMPLETED, TASK_FAILED <code>subtask</code> SUBTASK_SPAWNED, SUBTASK_COMPLETED <code>agent</code> AGENT_THINKING, AGENT_ACTING, AGENT_RESPONDED <code>tool</code> TOOL_CALLED, TOOL_RESULT, TOOL_ERROR <code>llm</code> LLM_REQUEST, LLM_RESPONSE, LLM_TOOL_DECISION <code>stream</code> STREAM_START, TOKEN_STREAMED, STREAM_END <code>plan</code> PLAN_CREATED, PLAN_STEP_STARTED, PLAN_STEP_COMPLETED <code>message</code> MESSAGE_SENT, MESSAGE_RECEIVED"},{"location":"core/#priority","title":"Priority","text":"<p>Task priority levels (comparable):</p> <pre><code>from cogent.core import Priority\n\n# Priorities are comparable\nPriority.HIGH &gt; Priority.NORMAL  # True\nPriority.LOW &lt; Priority.CRITICAL  # True\n</code></pre> Priority Value Description <code>LOW</code> 1 Background tasks <code>NORMAL</code> 2 Standard priority <code>HIGH</code> 3 Important tasks <code>CRITICAL</code> 4 Urgent tasks"},{"location":"core/#agentrole","title":"AgentRole","text":"<p>Agent roles define capabilities:</p> <pre><code>from cogent.core import AgentRole\n\nrole = AgentRole.SUPERVISOR\nrole.can_finish     # True\nrole.can_delegate   # True\nrole.can_use_tools  # False\n</code></pre> Role can_finish can_delegate can_use_tools <code>WORKER</code> \u274c \u274c \u2705 <code>SUPERVISOR</code> \u2705 \u2705 \u274c <code>AUTONOMOUS</code> \u2705 \u274c \u2705 <code>REVIEWER</code> \u2705 \u274c \u274c"},{"location":"core/#messages","title":"Messages","text":"<p>Native message types compatible with OpenAI/Anthropic/etc. APIs:</p>"},{"location":"core/#systemmessage","title":"SystemMessage","text":"<pre><code>from cogent.core.messages import SystemMessage\n\nmsg = SystemMessage(\"You are a helpful assistant.\")\nmsg.to_dict()  # {\"role\": \"system\", \"content\": \"...\"}\n</code></pre>"},{"location":"core/#humanmessage","title":"HumanMessage","text":"<pre><code>from cogent.core.messages import HumanMessage\n\nmsg = HumanMessage(\"Hello, how are you?\")\nmsg.to_dict()  # {\"role\": \"user\", \"content\": \"...\"}\n</code></pre>"},{"location":"core/#aimessage","title":"AIMessage","text":"<pre><code>from cogent.core.messages import AIMessage\n\n# Text response\nmsg = AIMessage(content=\"I'm doing well!\")\n\n# Response with tool calls\nmsg = AIMessage(\n    content=\"\",\n    tool_calls=[\n        {\"id\": \"call_1\", \"name\": \"search\", \"args\": {\"query\": \"weather\"}}\n    ],\n)\nmsg.to_dict()  # Includes properly formatted tool_calls\n</code></pre>"},{"location":"core/#toolmessage","title":"ToolMessage","text":"<pre><code>from cogent.core.messages import ToolMessage\n\nmsg = ToolMessage(\n    content='{\"temperature\": 72}',\n    tool_call_id=\"call_1\",\n)\n</code></pre>"},{"location":"core/#helper-functions","title":"Helper Functions","text":"<pre><code>from cogent.core.messages import (\n    messages_to_dict,\n    parse_openai_response,\n)\n\n# Convert message list for API calls\nmessages = [SystemMessage(\"...\"), HumanMessage(\"...\")]\napi_messages = messages_to_dict(messages)\n\n# Parse OpenAI response into AIMessage\nai_msg = parse_openai_response(openai_response)\n</code></pre>"},{"location":"core/#context-dependency-injection","title":"Context (Dependency Injection)","text":""},{"location":"core/#runcontext","title":"RunContext","text":"<p>Base class for invocation-scoped context that provides dependency injection for tools and interceptors.</p> <pre><code>from dataclasses import dataclass\nfrom cogent import Agent, tool\nfrom cogent.core import RunContext\n\n@dataclass\nclass AppContext(RunContext):\n    user_id: str\n    db: Any  # Your database connection\n    api_key: str\n\n@tool\ndef get_user_data(ctx: RunContext) -&gt; str:\n    \"\"\"Get data for the current user.\"\"\"\n    user = ctx.db.get_user(ctx.user_id)\n    return f\"User: {user.name}\"\n\nagent = Agent(name=\"assistant\", model=model, tools=[get_user_data])\n\n# Pass context at invocation time\nresult = await agent.run(\n    \"Get my profile data\",\n    context=AppContext(user_id=\"123\", db=db, api_key=key),\n)\n</code></pre> <p>Key Features: - Type-safe context data passed to tools and interceptors - No global state \u2014 context scoped to single invocation - Access via <code>ctx: RunContext</code> parameter in tools - Available in interceptors via <code>InterceptContext.run_context</code></p> <p>Methods: - <code>get(key, default)</code> \u2014 Get metadata value by key - <code>with_metadata(**kwargs)</code> \u2014 Create new context with additional metadata</p>"},{"location":"core/#reactive-utilities","title":"Reactive Utilities","text":"<p>Event-driven utilities for building robust reactive systems.</p>"},{"location":"core/#idempotencyguard","title":"IdempotencyGuard","text":"<p>In-memory idempotency guard to ensure side-effects execute only once per key:</p> <pre><code>from cogent.core import IdempotencyGuard\n\nguard = IdempotencyGuard()\n\nasync def process_event(event_id: str, data: dict):\n    if not await guard.claim(event_id):\n        return  # Already processed\n\n    # Process event exactly once\n    await do_work(data)\n</code></pre> <p>Methods: - <code>claim(key: str) -&gt; bool</code> \u2014 Atomically claim a key (returns True if first time) - <code>run_once(key: str, fn: Callable) -&gt; tuple[bool, Any]</code> \u2014 Run coroutine exactly once per key</p> <p>Note: Process-local memory. For distributed systems, back with Redis/database.</p>"},{"location":"core/#retrybudget","title":"RetryBudget","text":"<p>Bounded retry tracker for exponential backoff and retry policies:</p> <pre><code>from cogent.core import RetryBudget\n\nbudget = RetryBudget.in_memory(max_attempts=3)\n\nasync def handle_with_retries(task_id: str):\n    attempt = budget.next_attempt(task_id)\n\n    if attempt &gt;= 3:\n        # Escalate to error handler\n        await escalate_error(task_id)\n        return\n\n    # Try again\n    await retry_task(task_id)\n</code></pre> <p>Methods: - <code>in_memory(max_attempts: int) -&gt; RetryBudget</code> \u2014 Create in-memory tracker - <code>next_attempt(key: str) -&gt; int</code> \u2014 Increment and return attempt count (0-based) - <code>can_retry(key: str) -&gt; bool</code> \u2014 Check if more retries available</p>"},{"location":"core/#emit_later","title":"emit_later","text":"<p>Schedule delayed event emission:</p> <pre><code>from cogent.core import emit_later\n\n# In a Flow\nasync def handle_timeout(event, ctx):\n    # Schedule a timeout event\n    ctx.flow.spawn(\n        emit_later(\n            flow=ctx.flow,\n            delay_seconds=30.0,\n            event_name=\"task.timeout\",\n            data={\"task_id\": event.data[\"task_id\"]},\n        )\n    )\n</code></pre>"},{"location":"core/#jittered_delay","title":"jittered_delay","text":"<p>Calculate exponential backoff with jitter:</p> <pre><code>from cogent.core import jittered_delay\nimport random\n\nattempt = 2\nbase_delay = 2 ** attempt  # 4 seconds\njitter = random.uniform(-1, 1)\n\ndelay = jittered_delay(\n    base_seconds=base_delay,\n    jitter_seconds=jitter,\n    min_seconds=1.0,\n    max_seconds=60.0,\n)\n\nawait asyncio.sleep(delay)\n</code></pre>"},{"location":"core/#stopwatch","title":"Stopwatch","text":"<p>Performance timing helper:</p> <pre><code>from cogent.core import Stopwatch\n\nstopwatch = Stopwatch()\n\nawait do_work()\n\nelapsed = stopwatch.elapsed_s\nprint(f\"Work completed in {elapsed:.2f}s\")\n</code></pre>"},{"location":"core/#utilities","title":"Utilities","text":""},{"location":"core/#generate_id","title":"generate_id","text":"<p>Generate unique identifiers:</p> <pre><code>from cogent.core import generate_id\n\ntask_id = generate_id()  # \"task_a1b2c3d4\"\nagent_id = generate_id(prefix=\"agent\")  # \"agent_e5f6g7h8\"\n</code></pre>"},{"location":"core/#timestamps","title":"Timestamps","text":"<pre><code>from cogent.core import now_utc, now_local, to_local, format_timestamp\n\n# Get current time\nutc_now = now_utc()      # datetime in UTC\nlocal_now = now_local()  # datetime in local timezone\n\n# Convert UTC to local\nlocal_time = to_local(utc_now)\n\n# Format for display\nformatted = format_timestamp(utc_now)  # \"2024-12-04 10:30:45 UTC\"\n</code></pre>"},{"location":"core/#response-protocol","title":"Response Protocol","text":"<p>New in v1.13.0 \u2014 Unified response protocol for all agent operations with consistent metadata, observability, and error handling.</p>"},{"location":"core/#responset","title":"Response[T]","text":"<p>Generic container for agent responses with full metadata:</p> <pre><code>from cogent import Agent\nfrom cogent.core.response import Response\n\nagent = Agent(name=\"analyst\", model=model)\n\n# Agent.run() and Agent.think() return Response[T]\nresponse = await agent.think(\"Analyze sales data\")\n\n# Access response data\nresult: str = response.content\nsuccess: bool = response.success\n\n# Access metadata\ntokens = response.metadata.tokens.total_tokens\nduration = response.metadata.duration\nmodel = response.metadata.model\ncorrelation_id = response.metadata.correlation_id\n\n# Access tool calls with timing\nfor tool_call in response.tool_calls:\n    print(f\"{tool_call.tool_name}: {tool_call.duration}s\")\n\n# Access conversation history\nfor message in response.messages:\n    print(f\"{message.role}: {message.content}\")\n\n# Unwrap or handle errors\ntry:\n    result = response.unwrap()  # Returns content or raises\nexcept ResponseError as e:\n    print(f\"Error: {e.response.error.message}\")\n</code></pre>"},{"location":"core/#response-types","title":"Response Types","text":""},{"location":"core/#tokenusage","title":"TokenUsage","text":"<p>Token consumption tracking:</p> <pre><code>from cogent.core.response import TokenUsage\n\ntokens = response.metadata.tokens\nprint(f\"Prompt: {tokens.prompt_tokens}\")\nprint(f\"Completion: {tokens.completion_tokens}\")\nprint(f\"Total: {tokens.total_tokens}\")\n</code></pre>"},{"location":"core/#toolcall","title":"ToolCall","text":"<p>Tool invocation tracking with timing:</p> <pre><code>from cogent.core.response import ToolCall\n\nfor call in response.tool_calls:\n    print(f\"Tool: {call.tool_name}\")\n    print(f\"Duration: {call.duration}s\")\n    print(f\"Success: {call.success}\")\n    if call.error:\n        print(f\"Error: {call.error}\")\n</code></pre>"},{"location":"core/#responsemetadata","title":"ResponseMetadata","text":"<p>Consistent metadata across all responses:</p> <pre><code>from cogent.core.response import ResponseMetadata\n\nmetadata = response.metadata\n# agent: Agent that generated response\n# model: Model used (e.g., \"gpt-4\")\n# tokens: Token usage (if available)\n# duration: Execution time in seconds\n# timestamp: Unix timestamp\n# correlation_id: For distributed tracing\n# trace_id: Trace identifier\n</code></pre>"},{"location":"core/#errorinfo","title":"ErrorInfo","text":"<p>Structured error information:</p> <pre><code>from cogent.core.response import ErrorInfo\n\nif not response.success:\n    error = response.error\n    print(f\"Type: {error.type}\")\n    print(f\"Message: {error.message}\")\n    if error.traceback:\n        print(f\"Traceback: {error.traceback}\")\n</code></pre>"},{"location":"core/#response-features","title":"Response Features","text":""},{"location":"core/#serialization","title":"Serialization","text":"<pre><code># Convert to dictionary\ndata = response.to_dict()\n# {\n#     \"content\": \"...\",\n#     \"success\": true,\n#     \"metadata\": {...},\n#     \"tool_calls\": [...],\n#     \"messages\": [...],\n#     ...\n# }\n</code></pre>"},{"location":"core/#event-conversion","title":"Event Conversion","text":"<pre><code>from cogent.events import Event\n\n# Convert response to event for flow orchestration\nevent = Event.from_response(\n    response,\n    name=\"analysis.done\",\n    source=\"analyst\",\n)\n\n# Event includes response metadata\nassert event.data[\"content\"] == response.content\nassert event.metadata[\"tokens\"][\"total\"] == response.metadata.tokens.total_tokens\n</code></pre>"},{"location":"core/#a2a-integration","title":"A2A Integration","text":"<pre><code>from cogent.flow.a2a import AgentResponse\n\n# Create A2A response from agent Response\na2a_response = AgentResponse.from_response(\n    response,\n    from_agent=\"analyst\",\n    to_agent=\"coordinator\",\n)\n\n# Access underlying Response\ncore_response = a2a_response.unwrap()\n</code></pre>"},{"location":"core/#benefits","title":"Benefits","text":"<p>Observability: - Full conversation history with <code>response.messages</code> - Token usage tracking across all operations - Tool call timing and success/failure tracking - Correlation IDs for distributed tracing</p> <p>Debugging: - Inspect exact prompts sent to LLM - See all tool invocations with results - Track execution timing per operation - Access full error context with tracebacks</p> <p>Consistency: - Same response format for <code>Agent.run()</code>, <code>Agent.think()</code>, and A2A - Predictable error handling across all agent operations - Unified metadata structure</p> <p>Integration: - Seamless conversion to Events for flow orchestration - Compatible with A2A communication protocol - Works with all executor types (Native, ReAct, ChainOfThought)</p>"},{"location":"core/#exports","title":"Exports","text":"<pre><code>from cogent.core import (\n    # Enums\n    TaskStatus,\n    AgentStatus,\n    EventType,\n    Priority,\n    AgentRole,\n    # Context\n    RunContext,\n    EMPTY_CONTEXT,\n    # Utilities\n    generate_id,\n    now_utc,\n    now_local,\n    to_local,\n    format_timestamp,\n    # Reactive utilities\n    IdempotencyGuard,\n    RetryBudget,\n    emit_later,\n    jittered_delay,\n    Stopwatch,\n)\n\nfrom cogent.core.messages import (\n    BaseMessage,\n    SystemMessage,\n    HumanMessage,\n    AIMessage,\n    ToolMessage,\n    messages_to_dict,\n    parse_openai_response,\n)\n\nfrom cogent.core.response import (\n    Response,\n    ResponseMetadata,\n    TokenUsage,\n    ToolCall,\n    ErrorInfo,\n    ResponseError,\n)\n</code></pre>"},{"location":"document/","title":"Document Module","text":"<p>The <code>cogent.documents</code> module provides comprehensive document loading, text splitting, and summarization capabilities for RAG (Retrieval Augmented Generation) systems.</p>"},{"location":"document/#overview","title":"Overview","text":"<p>The document module includes: - Document &amp; DocumentMetadata: Type-safe document structure with rich metadata - Loaders: Load documents from various file formats with automatic metadata population - Splitters: Chunk text for embedding and retrieval while preserving metadata - Summarizers: Handle documents exceeding LLM context limits</p> <pre><code>from cogent.documents import (\n    Document,\n    DocumentMetadata,\n    DocumentLoader,\n    RecursiveCharacterSplitter,\n)\n\n# Load documents - metadata automatically populated\nloader = DocumentLoader()\ndocs = await loader.load_directory(\"./documents\")\n\n# Access structured metadata\nfor doc in docs:\n    print(f\"Source: {doc.source}\")\n    print(f\"Type: {doc.metadata.source_type}\")\n    print(f\"Page: {doc.metadata.page}\")\n\n# Split into chunks - metadata preserved\nsplitter = RecursiveCharacterSplitter(chunk_size=1000, chunk_overlap=200)\nchunks = splitter.split_documents(docs)\n</code></pre>"},{"location":"document/#document-metadata","title":"Document &amp; Metadata","text":""},{"location":"document/#document","title":"Document","text":"<p>Core document type used throughout cogent:</p> <pre><code>from cogent.documents import Document, DocumentMetadata\n\n# Create with structured metadata\ndoc = Document(\n    text=\"Document content...\",\n    metadata=DocumentMetadata(\n        source=\"report.pdf\",\n        source_type=\"pdf\",\n        page=5,\n        loader=\"PDFMarkdownLoader\"\n    )\n)\n\n# Convenience properties\nprint(doc.id)        # doc_a1b2c3d4e5f6g7h8\nprint(doc.source)    # report.pdf\nprint(doc.page)      # 5\n\n# Access full metadata\nprint(doc.metadata.char_count)  # Auto-populated\nprint(doc.metadata.timestamp)   # Creation time\n</code></pre>"},{"location":"document/#documentmetadata","title":"DocumentMetadata","text":"<p>Structured metadata with type safety and IDE autocomplete:</p> <pre><code>@dataclass\nclass DocumentMetadata:\n    \"\"\"Structured metadata for documents with provenance and tracking.\"\"\"\n\n    # Identification &amp; timing\n    id: str                        # Auto-generated: doc_a1b2c3d4\n    timestamp: float               # Unix timestamp\n\n    # Source information\n    source: str                    # File path, URL, etc.\n    source_type: str | None        # \"pdf\", \"markdown\", \"web\", \"api\"\n\n    # Content positioning (for chunked documents)\n    page: int | None               # Page number\n    chunk_index: int | None        # Chunk position (0-based)\n    chunk_total: int | None        # Total chunks from parent\n    start_char: int | None         # Start position in parent\n    end_char: int | None           # End position in parent\n\n    # Content metrics\n    token_count: int | None        # Token count (if computed)\n    char_count: int | None         # Character count (auto-populated)\n\n    # Provenance &amp; relationships\n    loader: str | None             # Loader that created document\n    created_by: str | None         # Agent/tool name\n    parent_id: str | None          # Parent document ID (for chunks)\n\n    # Custom fields (extensibility)\n    custom: dict[str, Any]         # User-defined metadata\n</code></pre> <p>Key Features:</p> <ol> <li>Type Safety: Properties with proper types, no dict key errors</li> <li>Auto-population: <code>char_count</code>, <code>id</code>, <code>timestamp</code> set automatically</li> <li>Provenance: Track <code>loader</code>, <code>created_by</code>, <code>parent_id</code> for observability</li> <li>Chunking-aware: <code>chunk_index</code>, <code>chunk_total</code>, <code>parent_id</code> for chunk relationships</li> <li>Extensible: Use <code>custom</code> dict for application-specific fields</li> </ol> <p>Example - Chunk Metadata:</p> <pre><code># Loaders automatically populate metadata\ndocs = await PDFMarkdownLoader().load(\"report.pdf\")\ndoc = docs[0]\n\nprint(doc.metadata.loader)       # \"PDFMarkdownLoader\"\nprint(doc.metadata.source_type)  # \"pdf\"\nprint(doc.metadata.page)         # 1\n\n# Splitters preserve and extend metadata\nsplitter = RecursiveCharacterSplitter(chunk_size=500)\nchunks = splitter.split_documents(docs)\n\nchunk = chunks[0]\nprint(chunk.metadata.parent_id)    # ID of original document\nprint(chunk.metadata.chunk_index)  # 0\nprint(chunk.metadata.chunk_total)  # 10\nprint(chunk.metadata.start_char)   # 0\nprint(chunk.metadata.end_char)     # 500\n</code></pre> <p>Serialization:</p> <pre><code># To dict (for storage/API)\ndata = doc.to_dict()\n# {\n#   \"text\": \"...\",\n#   \"metadata\": {\n#     \"id\": \"doc_...\",\n#     \"source\": \"file.txt\",\n#     \"char_count\": 1234,\n#     ...\n#   }\n# }\n\n# From dict (backward compatible - collects unknown fields into custom)\ndoc = Document.from_dict({\n    \"text\": \"...\",\n    \"metadata\": {\n        \"source\": \"file.txt\",\n        \"custom_field\": \"value\"  # Goes into metadata.custom\n    }\n})\nprint(doc.metadata.custom[\"custom_field\"])  # \"value\"\n</code></pre>"},{"location":"document/#document-loaders","title":"Document Loaders","text":""},{"location":"document/#documentloader","title":"DocumentLoader","text":"<p>The main loader class that auto-detects file types:</p> <pre><code>from cogent.documents.loaders import DocumentLoader\n\nloader = DocumentLoader()\n\n# Load single file\ndocs = await loader.load(\"document.pdf\")\n\n# Load directory\ndocs = await loader.load_directory(\"./docs\", glob=\"**/*.md\")\n\n# Load with options\ndocs = await loader.load(\"data.csv\", encoding=\"utf-8\")\n</code></pre>"},{"location":"document/#supported-formats","title":"Supported Formats","text":"Format Extensions Loader Text .txt, .rst, .log <code>TextLoader</code> Markdown .md <code>MarkdownLoader</code> HTML .html, .htm <code>HTMLLoader</code> PDF .pdf <code>PDFLoader</code> Word .docx <code>WordLoader</code> CSV .csv <code>CSVLoader</code> JSON .json, .jsonl <code>JSONLoader</code> Excel .xlsx <code>XLSXLoader</code> Code .py, .js, .ts, etc. <code>CodeLoader</code>"},{"location":"document/#pdfmarkdownloader","title":"PDFMarkdownLoader","text":"<p>High-performance PDF loader optimized for LLM/RAG with parallel processing:</p> <pre><code>from cogent.documents.loaders import PDFMarkdownLoader\n\nloader = PDFMarkdownLoader(\n    max_workers=4,      # CPU workers for parallel processing\n    batch_size=10,      # Pages per batch\n)\n\n# Standard API - returns list[Document] (consistent with other loaders)\ndocs = await loader.load(\"large_document.pdf\")\nprint(f\"Loaded {len(docs)} pages\")\n\n# With tracking - returns PDFProcessingResult with metrics\nresult = await loader.load(\"large_document.pdf\", tracking=True)\nprint(f\"Success rate: {result.success_rate:.0%}\")\nprint(f\"Time: {result.total_time_ms:.0f}ms\")\ndocs = result.documents\n</code></pre> <p>PDFProcessingResult (from <code>load(tracking=True)</code>): <pre><code>@dataclass\nclass PDFProcessingResult:\n    file_path: Path\n    status: PDFProcessingStatus\n    total_pages: int\n    successful_pages: int\n    failed_pages: int\n    empty_pages: int\n    page_results: list[PageResult]\n    total_time_ms: float\n\n    @property\n    def success_rate(self) -&gt; float:\n        \"\"\"Returns ratio 0.0-1.0 for percentage formatting.\"\"\"\n        ...\n\n    @property\n    def documents(self) -&gt; list[Document]:\n        \"\"\"Convert page results to Document list.\"\"\"\n        ...\n</code></pre></p>"},{"location":"document/#convenience-functions","title":"Convenience Functions","text":"<pre><code>from cogent.documents.loaders import load_documents, load_documents_sync\n\n# Async loading\ndocs = await load_documents(\"./data\")\n\n# Sync loading (for scripts)\ndocs = load_documents_sync(\"./data\")\n</code></pre>"},{"location":"document/#custom-loaders","title":"Custom Loaders","text":"<p>Register custom file handlers:</p> <pre><code>from cogent.documents.loaders import BaseLoader, register_loader\n\nclass XMLLoader(BaseLoader):\n    EXTENSIONS = [\".xml\"]\n\n    async def load(self, path, **kwargs) -&gt; list[Document]:\n        # Custom loading logic\n        ...\n\nregister_loader(XMLLoader)\n</code></pre>"},{"location":"document/#text-splitters","title":"Text Splitters","text":""},{"location":"document/#recursivecharactersplitter","title":"RecursiveCharacterSplitter","text":"<p>The recommended splitter for most use cases:</p> <pre><code>from cogent.documents.splitters import RecursiveCharacterSplitter\n\nsplitter = RecursiveCharacterSplitter(\n    chunk_size=1000,      # Target chunk size\n    chunk_overlap=200,    # Overlap between chunks\n    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],  # Hierarchy\n)\n\nchunks = splitter.split_text(text)\nchunks = splitter.split_documents(docs)\n</code></pre>"},{"location":"document/#available-splitters","title":"Available Splitters","text":"Splitter Use Case <code>RecursiveCharacterSplitter</code> General text, preserves structure <code>CharacterSplitter</code> Simple single-separator splitting <code>SentenceSplitter</code> Sentence boundary detection <code>MarkdownSplitter</code> Markdown structure-aware <code>HTMLSplitter</code> HTML tag-based splitting <code>CodeSplitter</code> Language-aware code splitting <code>SemanticSplitter</code> Embedding-based semantic chunking <code>TokenSplitter</code> Token count-based (for LLMs)"},{"location":"document/#sentencesplitter","title":"SentenceSplitter","text":"<p>Split by sentence boundaries:</p> <pre><code>from cogent.documents.splitters import SentenceSplitter\n\nsplitter = SentenceSplitter(\n    chunk_size=1000,\n    chunk_overlap=100,\n)\n\nchunks = splitter.split_text(text)\n</code></pre>"},{"location":"document/#markdownsplitter","title":"MarkdownSplitter","text":"<p>Preserve Markdown structure:</p> <pre><code>from cogent.documents.splitters import MarkdownSplitter\n\nsplitter = MarkdownSplitter(\n    chunk_size=1000,\n    headers_to_split_on=[\n        (\"#\", \"h1\"),\n        (\"##\", \"h2\"),\n        (\"###\", \"h3\"),\n    ],\n)\n\nchunks = splitter.split_text(markdown_text)\n</code></pre>"},{"location":"document/#codesplitter","title":"CodeSplitter","text":"<p>Language-aware code splitting:</p> <pre><code>from cogent.documents.splitters import CodeSplitter\n\nsplitter = CodeSplitter(\n    language=\"python\",\n    chunk_size=1000,\n)\n\nchunks = splitter.split_text(python_code)\n</code></pre> <p>Supported languages: Python, JavaScript, TypeScript, Java, C++, Go, Rust, Ruby, and more.</p>"},{"location":"document/#semanticsplitter","title":"SemanticSplitter","text":"<p>Split based on semantic similarity:</p> <pre><code>from cogent.documents.splitters import SemanticSplitter\n\nsplitter = SemanticSplitter(\n    embedding_model=my_embeddings,\n    breakpoint_threshold=0.5,  # Similarity threshold\n)\n\nchunks = await splitter.split_text(text)\n</code></pre>"},{"location":"document/#tokensplitter","title":"TokenSplitter","text":"<p>Split by token count (for LLM context limits):</p> <pre><code>from cogent.documents.splitters import TokenSplitter\n\nsplitter = TokenSplitter(\n    chunk_size=512,       # Max tokens per chunk\n    chunk_overlap=50,     # Token overlap\n    encoding=\"cl100k_base\",  # Tokenizer encoding\n)\n\nchunks = splitter.split_text(text)\n</code></pre>"},{"location":"document/#convenience-function","title":"Convenience Function","text":"<pre><code>from cogent.documents.splitters import split_text\n\nchunks = split_text(\n    text,\n    chunk_size=1000,\n    chunk_overlap=200,\n    splitter_type=\"recursive\",  # or \"sentence\", \"markdown\", etc.\n)\n</code></pre>"},{"location":"document/#document-type","title":"Document Type","text":"<p>The standard document container:</p> <pre><code>from cogent.documents import Document\n\ndoc = Document(\n    text=\"Document content...\",\n    metadata={\n        \"source\": \"file.pdf\",\n        \"page\": 1,\n        \"author\": \"John Doe\",\n    },\n)\n\n# Access\nprint(doc.text)\nprint(doc.metadata[\"source\"])\n</code></pre>"},{"location":"document/#textchunk","title":"TextChunk","text":"<p>Chunk with position information:</p> <pre><code>from cogent.documents import TextChunk\n\nchunk = TextChunk(\n    text=\"Chunk content...\",\n    start=0,\n    end=1000,\n    metadata={\"chunk_index\": 0},\n)\n</code></pre>"},{"location":"document/#document-summarization","title":"Document Summarization","text":"<p>For documents exceeding LLM context limits, use summarization strategies:</p>"},{"location":"document/#mapreducesummarizer","title":"MapReduceSummarizer","text":"<p>Parallel chunk summarization, then combine:</p> <pre><code>from cogent.documents.summarizer import MapReduceSummarizer\n\nsummarizer = MapReduceSummarizer(model=my_model)\nresult = await summarizer.summarize(long_text)\n</code></pre>"},{"location":"document/#refinesummarizer","title":"RefineSummarizer","text":"<p>Sequential refinement through chunks:</p> <pre><code>from cogent.documents.summarizer import RefineSummarizer\n\nsummarizer = RefineSummarizer(model=my_model)\nresult = await summarizer.summarize(long_text)\n</code></pre>"},{"location":"document/#hierarchicalsummarizer","title":"HierarchicalSummarizer","text":"<p>Tree-based recursive summarization:</p> <pre><code>from cogent.documents.summarizer import HierarchicalSummarizer\n\nsummarizer = HierarchicalSummarizer(\n    model=my_model,\n    levels=3,\n)\nresult = await summarizer.summarize(very_long_text)\n</code></pre>"},{"location":"document/#exports","title":"Exports","text":"<pre><code>from cogent.documents import (\n    # Types\n    Document,\n    TextChunk,\n    FileType,\n    SplitterType,\n    # Loaders\n    BaseLoader,\n    DocumentLoader,\n    TextLoader,\n    MarkdownLoader,\n    HTMLLoader,\n    PDFLoader,\n    PDFMarkdownLoader,\n    WordLoader,\n    CSVLoader,\n    JSONLoader,\n    XLSXLoader,\n    CodeLoader,\n    load_documents,\n    load_documents_sync,\n    register_loader,\n    # Splitters\n    BaseSplitter,\n    RecursiveCharacterSplitter,\n    CharacterSplitter,\n    SentenceSplitter,\n    MarkdownSplitter,\n    HTMLSplitter,\n    CodeSplitter,\n    SemanticSplitter,\n    TokenSplitter,\n    split_text,\n)\n\nfrom cogent.documents.loaders import (\n    # PDF LLM types\n    PDFProcessingResult,\n    PDFProcessingStatus,\n    PageResult,\n    PageStatus,\n    PDFConfig,\n    ProcessingMetrics,\n    OutputFormat,\n)\n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Get up and running with AgenticFlow in minutes.</p>"},{"location":"getting-started/#installation","title":"Installation","text":"<pre><code># Minimal installation (core only)\npip install git+https://github.com/milad-o/cogent.git\n\n# Or with uv (recommended)\nuv add git+https://github.com/milad-o/cogent.git\n</code></pre> <p>Optional dependency groups:</p> <p>Choose what you need:</p> <pre><code># Vector stores (FAISS, Qdrant)\nuv add \"cogent[vector-stores] @ git+https://github.com/milad-o/cogent.git\"\n\n# Retrieval (BM25, rerankers)\nuv add \"cogent[retrieval] @ git+https://github.com/milad-o/cogent.git\"\n\n# Database backends (SQLAlchemy + drivers)\nuv add \"cogent[database] @ git+https://github.com/milad-o/cogent.git\"\n\n# Infrastructure (Redis)\nuv add \"cogent[infrastructure] @ git+https://github.com/milad-o/cogent.git\"\n\n# Web tools (search, scraping)\nuv add \"cogent[web] @ git+https://github.com/milad-o/cogent.git\"\n\n# LLM providers (Anthropic, Azure, Cohere, Groq)\nuv add \"cogent[all-providers] @ git+https://github.com/milad-o/cogent.git\"\n\n# All backends\nuv add \"cogent[all-backend] @ git+https://github.com/milad-o/cogent.git\"\n\n# Everything\nuv add \"cogent[all] @ git+https://github.com/milad-o/cogent.git\"\n</code></pre> <p>Development installation:</p> <pre><code># Development + testing\nuv add --dev cogent[dev,test,test-backends,docs]\n</code></pre>"},{"location":"getting-started/#quick-start","title":"Quick Start","text":""},{"location":"getting-started/#your-first-agent","title":"Your First Agent","text":"<pre><code>import asyncio\nfrom cogent import Agent, tool\n\n@tool\ndef get_weather(city: str) -&gt; str:\n    \"\"\"Get current weather for a city.\"\"\"\n    return f\"Weather in {city}: 72\u00b0F, sunny\"\n\nasync def main():\n    # Simple string model (recommended)\n    agent = Agent(\n        name=\"Assistant\",\n        model=\"gpt4\",  # Auto-resolves to gpt-4o\n        tools=[get_weather],\n    )\n\n    result = await agent.run(\"What's the weather in Tokyo?\")\n    print(result.output)\n\nasyncio.run(main())\n</code></pre> <p>Other model options: <pre><code># With provider prefix\nagent = Agent(name=\"Assistant\", model=\"anthropic:claude\")\n\n# Medium-level: Factory function\nfrom cogent.models import create_chat\nagent = Agent(name=\"Assistant\", model=create_chat(\"gpt4\"))\n\n# Low-level: Full control\nfrom cogent.models import OpenAIChat\nagent = Agent(name=\"Assistant\", model=OpenAIChat(model=\"gpt-4o\", temperature=0.7))\n</code></pre></p>"},{"location":"getting-started/#with-capabilities","title":"With Capabilities","text":"<p>Instead of defining individual tools, use pre-built capabilities:</p> <pre><code>from cogent import Agent\nfrom cogent.capabilities import FileSystem, WebSearch, CodeSandbox\n\nagent = Agent(\n    name=\"Assistant\",\n    model=\"gpt4\",  # Simple string model\n    capabilities=[\n        FileSystem(allowed_paths=[\"./project\"]),\n        WebSearch(),\n        CodeSandbox(timeout=30),\n    ],\n)\n\nresult = await agent.run(\"Search for Python tutorials and create a summary file\")\n</code></pre>"},{"location":"getting-started/#core-concepts","title":"Core Concepts","text":""},{"location":"getting-started/#agents","title":"Agents","text":"<p>Autonomous entities that think, act, and communicate:</p> <pre><code>agent = Agent(\n    name=\"Researcher\",\n    model=\"gpt4\",  # String model - auto-resolves to gpt-4o\n    instructions=\"You are a thorough researcher.\",\n    tools=[search, summarize],\n    verbose=True,\n)\n</code></pre>"},{"location":"getting-started/#tools","title":"Tools","text":"<p>Define tools with the <code>@tool</code> decorator:</p> <pre><code>from cogent import tool\n\n@tool\ndef search(query: str, max_results: int = 10) -&gt; str:\n    \"\"\"Search the web for information.\n\n    Args:\n        query: Search query string.\n        max_results: Maximum results to return.\n    \"\"\"\n    return f\"Found {max_results} results for: {query}\"\n\n# Async tools supported\n@tool\nasync def fetch_data(url: str) -&gt; str:\n    \"\"\"Fetch data from URL.\"\"\"\n    async with httpx.AsyncClient() as client:\n        response = await client.get(url)\n        return response.text\n</code></pre> <p>Tool features: - Automatic schema extraction from type hints - Docstring \u2192 description - Sync and async support - Context injection via <code>ctx: RunContext</code></p>"},{"location":"getting-started/#multi-agent-flow","title":"Multi-Agent Flow","text":"<p>Coordinate multiple agents with built-in patterns:</p> <pre><code>from cogent.flow import pipeline, supervisor, mesh\n\nresearcher = Agent(name=\"Researcher\", model=model, instructions=\"Research thoroughly.\")\nwriter = Agent(name=\"Writer\", model=model, instructions=\"Write clearly.\")\neditor = Agent(name=\"Editor\", model=model, instructions=\"Review and polish.\")\n\n# Sequential processing\nflow = pipeline([researcher, writer, editor])\n\nresult = await flow.run(\"Create a blog post about quantum computing\")\n</code></pre> <p>Patterns: - Pipeline \u2014 Sequential agent execution - Supervisor \u2014 Leader agent delegates to workers - Mesh \u2014 Agents communicate peer-to-peer</p>"},{"location":"getting-started/#model-providers","title":"Model Providers","text":"<p>AgenticFlow supports all major LLM providers with native SDK integrations:</p> <pre><code>from cogent.models import (\n    OpenAIChat,\n    AnthropicChat,\n    GeminiChat,\n    GroqChat,\n    OllamaChat,\n)\n\n# OpenAI\nmodel = OpenAIChat(model=\"gpt-4o\")\n\n# Anthropic Claude\nmodel = AnthropicChat(model=\"claude-sonnet-4-20250514\")\n\n# Google Gemini\nmodel = GeminiChat(model=\"gemini-2.0-flash-exp\")\n\n# Groq (fast inference)\nmodel = GroqChat(model=\"llama-3.3-70b-versatile\")\n\n# Ollama (local models)\nmodel = OllamaChat(model=\"llama3.2\")\n\n# Or use factory function\nfrom cogent.models import create_chat\n\nmodel = create_chat(\"anthropic\", model=\"claude-sonnet-4-20250514\")\n</code></pre>"},{"location":"getting-started/#streaming","title":"Streaming","text":"<p>Stream responses token-by-token:</p> <pre><code>agent = Agent(\n    name=\"Writer\",\n    model=model,\n    stream=True,\n)\n\nasync for chunk in agent.run_stream(\"Write a poem about AI\"):\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"getting-started/#memory-context","title":"Memory &amp; Context","text":"<p>Agents maintain conversation history automatically:</p> <pre><code>agent = Agent(\n    name=\"Assistant\",\n    model=model,\n    memory_enabled=True,  # Default: True\n)\n\n# First interaction\nawait agent.run(\"My name is Alice\")\n\n# Later interaction - agent remembers\nawait agent.run(\"What's my name?\")  # \"Your name is Alice\"\n\n# Clear memory\nagent.clear_memory()\n</code></pre>"},{"location":"getting-started/#observability","title":"Observability","text":"<p>Track execution with built-in observability:</p> <pre><code>from cogent.observability import Observer\n\n# Pre-configured observers\nobserver = Observer.trace()      # Maximum detail\nobserver = Observer.verbose()    # Key events\nobserver = Observer.minimal()    # Errors only\n\nflow = Flow(\n    agents=[...],\n    observer=observer,\n)\n\n# Access execution traces\nfor event in observer.events():\n    print(f\"{event.type}: {event.message}\")\n</code></pre>"},{"location":"getting-started/#interceptors","title":"Interceptors","text":"<p>Control execution with middleware:</p> <pre><code>from cogent.interceptors import BudgetGuard, RateLimiter, PIIShield\n\nagent = Agent(\n    name=\"Assistant\",\n    model=model,\n    interceptors=[\n        BudgetGuard(max_tokens=10000, max_cost=0.50),  # Token/cost limits\n        RateLimiter(max_requests_per_minute=60),        # Rate limiting\n        PIIShield(redact=True),                         # PII protection\n    ],\n)\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you know the basics, explore:</p> <ul> <li>Agent Documentation \u2014 Deep dive into agents, instructions, and configuration</li> <li>Multi-Agent Flow \u2014 Build coordinated multi-agent systems</li> <li>Capabilities \u2014 Explore built-in tools and capabilities</li> <li>Graph Visualization \u2014 Visualize your agents and flows</li> <li>RAG &amp; Retrieval \u2014 Build retrieval-augmented generation systems</li> <li>Examples \u2014 See working examples</li> </ul>"},{"location":"getting-started/#need-help","title":"Need Help?","text":"<ul> <li>Documentation: https://milad-o.github.io/cogent</li> <li>Examples: https://github.com/milad-o/cogent/tree/main/examples</li> <li>Issues: https://github.com/milad-o/cogent/issues</li> </ul>"},{"location":"graph/","title":"Graph Module","text":"<p>The <code>cogent.graph</code> module provides unified visualization for agents, patterns, and flows.</p>"},{"location":"graph/#overview","title":"Overview","text":"<p>Visualize any entity as a diagram:</p> <pre><code>from cogent import Agent\n\nagent = Agent(name=\"assistant\", model=model, tools=[search, write])\n\n# Get graph view\nview = agent.graph()\n\n# Render in any format\nprint(view.mermaid())   # Mermaid code\nprint(view.ascii())     # Terminal-friendly\nprint(view.dot())       # Graphviz DOT\nprint(view.url())       # mermaid.ink URL\nprint(view.html())      # Embeddable HTML\n\n# Save to file\nview.save(\"diagram.png\")\nview.save(\"diagram.svg\")\n</code></pre>"},{"location":"graph/#graphview-api","title":"GraphView API","text":"<p>All entities return a <code>GraphView</code> from their <code>.graph()</code> method:</p>"},{"location":"graph/#rendering-methods","title":"Rendering Methods","text":"<pre><code>view = agent.graph()\n\n# Mermaid diagram code\nmermaid_code = view.mermaid()\nprint(mermaid_code)\n# graph TD\n#   assistant[assistant]\n#   search[\ud83d\udd27 search]\n#   write[\ud83d\udd27 write]\n#   assistant --&gt; search\n#   assistant --&gt; write\n\n# ASCII art for terminal\nascii_art = view.ascii()\nprint(ascii_art)\n# \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n# \u2502  assistant  \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n#    \u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510\n#    \u25bc       \u25bc\n# [search] [write]\n\n# Graphviz DOT format\ndot_code = view.dot()\n\n# mermaid.ink URL (shareable)\nurl = view.url()\nprint(url)  # https://mermaid.ink/img/...\n\n# Embeddable HTML\nhtml = view.html()\n</code></pre>"},{"location":"graph/#saving-diagrams","title":"Saving Diagrams","text":"<pre><code># Auto-detect format from extension\nview.save(\"diagram.png\")    # PNG image\nview.save(\"diagram.svg\")    # SVG vector\nview.save(\"diagram.mmd\")    # Mermaid source\nview.save(\"diagram.dot\")    # Graphviz DOT\nview.save(\"diagram.html\")   # HTML page\n\n# Explicit format\nview.save(\"output\", format=\"png\")\n</code></pre>"},{"location":"graph/#agent-graphs","title":"Agent Graphs","text":"<p>Visualize agent structure:</p> <pre><code>from cogent import Agent\nfrom cogent.capabilities import WebSearch, FileSystem\n\nagent = Agent(\n    name=\"researcher\",\n    model=model,\n    tools=[search, analyze, summarize],\n    capabilities=[WebSearch(), FileSystem()],\n)\n\nview = agent.graph()\nprint(view.mermaid())\n</code></pre> <p>Output: <pre><code>graph TD\n    researcher[\ud83e\udd16 researcher]\n\n    subgraph Tools\n        search[\ud83d\udd27 search]\n        analyze[\ud83d\udd27 analyze]\n        summarize[\ud83d\udd27 summarize]\n    end\n\n    subgraph Capabilities\n        WebSearch[\ud83c\udf10 WebSearch]\n        FileSystem[\ud83d\udcc1 FileSystem]\n    end\n\n    researcher --&gt; Tools\n    researcher --&gt; Capabilities</code></pre></p>"},{"location":"graph/#pattern-graphs","title":"Pattern Graphs","text":"<p>Visualize multi-agent patterns using <code>FlowGraph</code>:</p>"},{"location":"graph/#supervisor","title":"Supervisor","text":"<pre><code>from cogent.flow import supervisor\nfrom cogent.graph import FlowGraph\n\nflow = supervisor(coordinator=manager, workers=[analyst, writer, reviewer])\n\ngraph = FlowGraph.from_flow(flow)\nprint(graph.render())  # Mermaid by default\n</code></pre>"},{"location":"graph/#pipeline","title":"Pipeline","text":"<pre><code>from cogent.flow import pipeline\nfrom cogent.graph import FlowGraph\n\nflow = pipeline([researcher, writer, editor])\n\ngraph = FlowGraph.from_flow(flow)\nprint(graph.render())\n</code></pre>"},{"location":"graph/#mesh","title":"Mesh","text":"<pre><code>from cogent.flow import mesh\nfrom cogent.graph import FlowGraph\n\nflow = mesh([analyst1, analyst2, analyst3], max_rounds=3)\n\ngraph = FlowGraph.from_flow(flow)\nprint(graph.render())\n</code></pre>"},{"location":"graph/#flow-graphs","title":"Flow Graphs","text":"<p>Visualize custom event-driven flows:</p> <pre><code>from cogent import Flow, react_to\nfrom cogent.graph import FlowGraph\n\nflow = Flow(name=\"content-pipeline\")\nflow.register(researcher, [react_to(\"task.created\")])\nflow.register(writer, [react_to(\"researcher.completed\")])\n\ngraph = FlowGraph.from_flow(flow)\nprint(graph.render())\n</code></pre>"},{"location":"graph/#configuration","title":"Configuration","text":""},{"location":"graph/#graphconfig","title":"GraphConfig","text":"<pre><code>from cogent.graph import GraphConfig, GraphTheme, GraphDirection\n\nconfig = GraphConfig(\n    direction=GraphDirection.TOP_DOWN,  # or LEFT_RIGHT\n    theme=GraphTheme.DEFAULT,           # or DARK, FOREST, NEUTRAL\n    show_tools=True,\n    show_capabilities=True,\n    show_interceptors=False,\n    node_spacing=50,\n    rank_spacing=100,\n)\n\nview = agent.graph(config=config)\n</code></pre>"},{"location":"graph/#graphdirection","title":"GraphDirection","text":"Direction Description <code>TOP_DOWN</code> Vertical, top to bottom (TD) <code>LEFT_RIGHT</code> Horizontal, left to right (LR) <code>BOTTOM_UP</code> Vertical, bottom to top (BU) <code>RIGHT_LEFT</code> Horizontal, right to left (RL)"},{"location":"graph/#graphtheme","title":"GraphTheme","text":"Theme Description <code>DEFAULT</code> Standard colors <code>DARK</code> Dark background <code>FOREST</code> Green tones <code>NEUTRAL</code> Grayscale"},{"location":"graph/#execution-graphs","title":"Execution Graphs","text":"<p>Visualize execution traces:</p> <pre><code>from cogent.observability import ExecutionTracer\n\ntracer = ExecutionTracer()\nresult = await agent.run(\"Query\", tracer=tracer)\n\n# Get execution graph\nview = tracer.graph()\nprint(view.mermaid())\n</code></pre> <p>Output: <pre><code>graph TD\n    start([Start])\n    llm1[LLM Call]\n    tool1[search]\n    llm2[LLM Call]\n    end_node([End])\n\n    start --&gt; llm1\n    llm1 --&gt;|tool decision| tool1\n    tool1 --&gt; llm2\n    llm2 --&gt; end_node\n\n    style start fill:#90EE90\n    style end_node fill:#90EE90\n    style tool1 fill:#FFE4B5</code></pre></p>"},{"location":"graph/#ascii-rendering","title":"ASCII Rendering","text":"<p>Terminal-friendly diagrams:</p> <pre><code>view = topology.graph()\nprint(view.ascii())\n\n# Output:\n# \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n# \u2502   manager   \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n#        \u2502\n#   \u250c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2510\n#   \u25bc    \u25bc    \u25bc\n# [ana] [wri] [rev]\n</code></pre>"},{"location":"graph/#dag-ascii-rendering","title":"DAG ASCII Rendering","text":"<pre><code>from cogent.observability import render_dag_ascii\n\ndag = {\n    \"A\": [\"B\", \"C\"],\n    \"B\": [\"D\"],\n    \"C\": [\"D\"],\n    \"D\": [],\n}\n\nprint(render_dag_ascii(dag))\n</code></pre>"},{"location":"graph/#interactive-viewing","title":"Interactive Viewing","text":"<p>Open in browser:</p> <pre><code>view = agent.graph()\n\n# Open mermaid.ink in default browser\nview.open()\n\n# Or get URL to share\nurl = view.url()\nprint(f\"View at: {url}\")\n</code></pre>"},{"location":"graph/#integration","title":"Integration","text":""},{"location":"graph/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<pre><code>from cogent import Agent\n\nagent = Agent(name=\"assistant\", model=model)\nview = agent.graph()\n\n# Display inline in notebook\nview.display()\n\n# Or use IPython display\nfrom IPython.display import HTML\nHTML(view.html())\n</code></pre>"},{"location":"graph/#vs-code","title":"VS Code","text":"<pre><code># Save as .mmd file for Mermaid preview extension\nview.save(\"diagram.mmd\")\n</code></pre>"},{"location":"graph/#documentation","title":"Documentation","text":"<p><pre><code># My Agent Architecture\n\n```mermaid\n{view.mermaid()}\n</code></pre> <pre><code>---\n\n## KnowledgeGraph Visualization\n\nVisualize knowledge graphs with entity grouping and custom layouts:\n\n```python\nfrom cogent.capabilities import KnowledgeGraph\n\nkg = KnowledgeGraph()\n\n# Add entities and relationships\nkg.remember(\"Alice\", \"Person\", {\"role\": \"CEO\"})\nkg.remember(\"TechCorp\", \"Company\", {\"founded\": 2015})\nkg.connect(\"Alice\", \"works_at\", \"TechCorp\")\n\n# Visualize with layout options\nview = kg.visualize(\n    direction=\"LR\",          # Left-to-right (also: TB, BT, RL)\n    group_by_type=True,      # Group entities by type in subgraphs\n    show_attributes=False,   # Hide/show entity attributes\n    max_entities=None,       # Limit number of entities\n)\n\n# Use all GraphView methods\nprint(view.mermaid())\nview.save(\"knowledge_graph.png\")\nprint(view.url())  # Share with others\n</code></pre></p> <p>Entity Type Colors: - Person \u2192 Blue (#60a5fa) - Company/Organization \u2192 Green (#7eb36a) - Location \u2192 Orange (#f59e0b) - Event \u2192 Purple (#9b59b6) - Generic/Unknown \u2192 Gray (#94a3b8)</p> <p>Layout Options: <pre><code># Hierarchical top-down\nview = kg.visualize(direction=\"TB\", group_by_type=True)\n\n# Left-right flow (recommended for knowledge graphs)\nview = kg.visualize(direction=\"LR\", group_by_type=True)\n\n# Bottom-up\nview = kg.visualize(direction=\"BT\", group_by_type=False)\n\n# Right-left\nview = kg.visualize(direction=\"RL\", group_by_type=True)\n</code></pre></p> <p>Example Output: <pre><code>flowchart LR\n    subgraph type_Person[\"Person\"]\n        Alice(\"Alice\"):::person\n    end\n\n    subgraph type_Company[\"Company\"]\n        TechCorp(\"TechCorp\"):::org\n    end\n\n    Alice --&gt;|works_at| TechCorp\n\n    classDef person fill:#60a5fa,stroke:#3b82f6,color:#fff\n    classDef org fill:#7eb36a,stroke:#4a7a3d,color:#fff</code></pre></p>"},{"location":"graph/#api-reference","title":"API Reference","text":""},{"location":"graph/#graphview-methods","title":"GraphView Methods","text":"Method Returns Description <code>mermaid()</code> <code>str</code> Mermaid diagram code <code>ascii()</code> <code>str</code> ASCII art diagram <code>dot()</code> <code>str</code> Graphviz DOT code <code>url()</code> <code>str</code> mermaid.ink URL <code>html()</code> <code>str</code> Embeddable HTML <code>png()</code> <code>bytes</code> PNG image bytes <code>svg()</code> <code>bytes</code> SVG vector bytes <code>save(path)</code> <code>None</code> Save to file (auto-detects format) <code>open()</code> <code>None</code> Open in browser <code>display()</code> <code>None</code> Display in notebook"},{"location":"graph/#configuration-classes","title":"Configuration Classes","text":"Class Description <code>GraphConfig</code> Diagram configuration <code>GraphTheme</code> Color themes <code>GraphDirection</code> Diagram direction (TOP_DOWN, LEFT_RIGHT, BOTTOM_UP, RIGHT_LEFT) <code>NodeShape</code> Node shapes <code>EdgeType</code> Edge styles"},{"location":"interceptors/","title":"Interceptors Module","text":"<p>The <code>cogent.interceptors</code> module provides composable units that intercept agent execution for cross-cutting concerns like cost control, security, context management, and observability.</p>"},{"location":"interceptors/#overview","title":"Overview","text":"<p>Interceptors are middleware that wrap agent execution at specific phases: - Before LLM call - Modify context, filter tools, check budgets - After LLM call - Validate responses, mask PII, audit - Before tool call - Gate access, rate limit, retry logic - After tool call - Post-process results, aggregate data</p> <pre><code>from cogent import Agent\nfrom cogent.interceptors import BudgetGuard, PIIShield\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    intercept=[\n        BudgetGuard(max_model_calls=10, max_tool_calls=50),\n        PIIShield(patterns=[\"email\", \"ssn\"]),\n    ],\n)\n</code></pre>"},{"location":"interceptors/#core-concepts","title":"Core Concepts","text":""},{"location":"interceptors/#interceptor-lifecycle","title":"Interceptor Lifecycle","text":"<p>Interceptors run at specific phases:</p> <pre><code>User Query\n    \u2193\n[BEFORE_LLM] \u2190 Context modification, validation\n    \u2193\nLLM Call\n    \u2193\n[AFTER_LLM] \u2190 Response validation, PII masking\n    \u2193\n[BEFORE_TOOL] \u2190 Tool gating, rate limiting\n    \u2193\nTool Execution\n    \u2193\n[AFTER_TOOL] \u2190 Result post-processing\n    \u2193\nResponse to User\n</code></pre>"},{"location":"interceptors/#phase-enum","title":"Phase Enum","text":"<pre><code>from cogent.interceptors import Phase\n\nPhase.BEFORE_LLM    # Before sending to LLM\nPhase.AFTER_LLM     # After receiving LLM response\nPhase.BEFORE_TOOL   # Before tool execution\nPhase.AFTER_TOOL    # After tool execution\n</code></pre>"},{"location":"interceptors/#interceptresult","title":"InterceptResult","text":"<p>Interceptors return a result that can: - Continue - Proceed to next interceptor/phase - Modify - Change the data and continue - Stop - Halt execution with a response</p> <pre><code>from cogent.interceptors import InterceptResult\n\n# Continue unchanged\nreturn InterceptResult.continue_()\n\n# Modify and continue\nreturn InterceptResult.modify(new_messages=modified_messages)\n\n# Stop execution\nreturn InterceptResult.stop(response=\"Cannot proceed: budget exceeded\")\n</code></pre>"},{"location":"interceptors/#built-in-interceptors","title":"Built-in Interceptors","text":""},{"location":"interceptors/#budgetguard","title":"BudgetGuard","text":"<p>Control costs by limiting LLM and tool calls:</p> <pre><code>from cogent.interceptors import BudgetGuard\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    intercept=[\n        BudgetGuard(\n            max_model_calls=10,     # Max LLM invocations\n            max_tool_calls=50,      # Max tool executions\n            max_tokens=100000,      # Max token usage\n            on_exceeded=\"stop\",     # \"stop\" or \"warn\"\n        ),\n    ],\n)\n\n# Check budget status\nguard = agent.interceptors[0]\nprint(f\"Calls: {guard.model_calls}/{guard.max_model_calls}\")\nprint(f\"Tokens: {guard.tokens_used}/{guard.max_tokens}\")\n</code></pre>"},{"location":"interceptors/#piishield","title":"PIIShield","text":"<p>Detect and handle PII in inputs/outputs:</p> <pre><code>from cogent.interceptors import PIIShield, PIIAction\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    intercept=[\n        PIIShield(\n            patterns=[\"email\", \"phone\", \"ssn\", \"credit_card\"],\n            action=PIIAction.MASK,  # MASK, REDACT, or BLOCK\n        ),\n    ],\n)\n\n# Input: \"Contact john@email.com\"\n# Masked: \"Contact [EMAIL]\"\n</code></pre> <p>Actions:</p> Action Behavior <code>PIIAction.MASK</code> Replace with <code>[TYPE]</code> placeholder <code>PIIAction.REDACT</code> Remove entirely <code>PIIAction.BLOCK</code> Stop execution with error"},{"location":"interceptors/#contentfilter","title":"ContentFilter","text":"<p>Filter harmful or inappropriate content:</p> <pre><code>from cogent.interceptors import ContentFilter\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    intercept=[\n        ContentFilter(\n            block_patterns=[\"password\", \"secret key\"],\n            allow_patterns=[\"public api\"],  # Whitelist\n        ),\n    ],\n)\n</code></pre>"},{"location":"interceptors/#tokenlimiter","title":"TokenLimiter","text":"<p>Limit context size to fit model constraints:</p> <pre><code>from cogent.interceptors import TokenLimiter\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    intercept=[\n        TokenLimiter(\n            max_tokens=8000,        # Max context tokens\n            strategy=\"truncate\",   # \"truncate\" or \"summarize\"\n            keep_system=True,      # Always keep system message\n            keep_last_n=5,         # Keep last N messages\n        ),\n    ],\n)\n</code></pre>"},{"location":"interceptors/#contextcompressor","title":"ContextCompressor","text":"<p>Compress context to reduce token usage:</p> <pre><code>from cogent.interceptors import ContextCompressor\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    intercept=[\n        ContextCompressor(\n            model=model,           # LLM for summarization\n            trigger_tokens=6000,   # Compress when above this\n            target_tokens=3000,    # Target after compression\n        ),\n    ],\n)\n</code></pre>"},{"location":"interceptors/#tool-control","title":"Tool Control","text":""},{"location":"interceptors/#toolgate","title":"ToolGate","text":"<p>Control which tools are available:</p> <pre><code>from cogent.interceptors import ToolGate\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    tools=[search, write_file, delete_file],\n    intercept=[\n        ToolGate(\n            allow=[\"search\"],           # Only these tools\n            # Or: deny=[\"delete_file\"], # Block these tools\n        ),\n    ],\n)\n</code></pre> <p>Dynamic gating:</p> <pre><code>def gate_by_user(ctx: InterceptContext) -&gt; list[str]:\n    if ctx.run_context.get(\"is_admin\"):\n        return [\"*\"]  # All tools\n    return [\"search\", \"read_file\"]\n\nagent = Agent(\n    intercept=[ToolGate(allow=gate_by_user)],\n)\n</code></pre>"},{"location":"interceptors/#permissiongate","title":"PermissionGate","text":"<p>Role-based tool permissions:</p> <pre><code>from cogent.interceptors import PermissionGate\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    intercept=[\n        PermissionGate(\n            permissions={\n                \"admin\": [\"*\"],\n                \"user\": [\"search\", \"read\"],\n                \"guest\": [\"search\"],\n            },\n            get_role=lambda ctx: ctx.run_context.get(\"role\", \"guest\"),\n        ),\n    ],\n)\n</code></pre>"},{"location":"interceptors/#conversationgate","title":"ConversationGate","text":"<p>Enable tools based on conversation state:</p> <pre><code>from cogent.interceptors import ConversationGate\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    intercept=[\n        ConversationGate(\n            # Unlock tools after specific messages\n            unlock_on={\n                \"confirmed\": [\"execute_order\"],\n                \"authenticated\": [\"view_account\", \"transfer\"],\n            },\n        ),\n    ],\n)\n</code></pre>"},{"location":"interceptors/#resilience","title":"Resilience","text":""},{"location":"interceptors/#ratelimiter","title":"RateLimiter","text":"<p>Limit request rates:</p> <pre><code>from cogent.interceptors import RateLimiter\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    intercept=[\n        RateLimiter(\n            max_requests=10,       # Max requests\n            window_seconds=60,     # Per time window\n            on_exceeded=\"wait\",    # \"wait\" or \"error\"\n        ),\n    ],\n)\n</code></pre>"},{"location":"interceptors/#failover","title":"Failover","text":"<p>Automatic model failover:</p> <pre><code>from cogent.interceptors import Failover, FailoverTrigger\nfrom cogent.models import ChatModel\n\nagent = Agent(\n    name=\"assistant\",\n    model=primary_model,\n    intercept=[\n        Failover(\n            fallback_models=[\n                ChatModel(model=\"gpt-4o-mini\"),\n                ChatModel(model=\"gpt-3.5-turbo\"),\n            ],\n            triggers=[\n                FailoverTrigger.RATE_LIMIT,\n                FailoverTrigger.TIMEOUT,\n                FailoverTrigger.ERROR,\n            ],\n            max_retries=2,\n        ),\n    ],\n)\n</code></pre>"},{"location":"interceptors/#circuitbreaker","title":"CircuitBreaker","text":"<p>Prevent cascade failures:</p> <pre><code>from cogent.interceptors import CircuitBreaker\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    intercept=[\n        CircuitBreaker(\n            failure_threshold=5,   # Failures before opening\n            recovery_timeout=60,   # Seconds before retry\n            half_open_requests=2,  # Test requests when recovering\n        ),\n    ],\n)\n</code></pre>"},{"location":"interceptors/#toolguard","title":"ToolGuard","text":"<p>Per-tool retry and circuit breaker:</p> <pre><code>from cogent.interceptors import ToolGuard\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    intercept=[\n        ToolGuard(\n            tool_configs={\n                \"search\": {\n                    \"max_retries\": 3,\n                    \"backoff\": \"exponential\",\n                    \"circuit_breaker\": True,\n                },\n                \"database\": {\n                    \"max_retries\": 1,\n                    \"timeout\": 30,\n                },\n            },\n        ),\n    ],\n)\n</code></pre>"},{"location":"interceptors/#auditing","title":"Auditing","text":""},{"location":"interceptors/#auditor","title":"Auditor","text":"<p>Log all agent activity:</p> <pre><code>from cogent.interceptors import Auditor, AuditEventType\n\nasync def log_event(event):\n    print(f\"[{event.type}] {event.agent}: {event.data}\")\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    intercept=[\n        Auditor(\n            handler=log_event,\n            events=[\n                AuditEventType.LLM_REQUEST,\n                AuditEventType.LLM_RESPONSE,\n                AuditEventType.TOOL_CALL,\n                AuditEventType.TOOL_RESULT,\n            ],\n            include_content=True,  # Log message content\n        ),\n    ],\n)\n</code></pre>"},{"location":"interceptors/#prompt-adapters","title":"Prompt Adapters","text":""},{"location":"interceptors/#contextprompt","title":"ContextPrompt","text":"<p>Inject dynamic context into system prompt:</p> <pre><code>from cogent.interceptors import ContextPrompt\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    intercept=[\n        ContextPrompt(\n            template=\"\"\"Current time: {time}\nUser timezone: {timezone}\nUser preferences: {preferences}\"\"\",\n            get_context=lambda ctx: {\n                \"time\": datetime.now().isoformat(),\n                \"timezone\": ctx.run_context.get(\"timezone\", \"UTC\"),\n                \"preferences\": ctx.run_context.get(\"preferences\", {}),\n            },\n        ),\n    ],\n)\n</code></pre>"},{"location":"interceptors/#conversationprompt","title":"ConversationPrompt","text":"<p>Add conversation-aware context:</p> <pre><code>from cogent.interceptors import ConversationPrompt\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    intercept=[\n        ConversationPrompt(\n            summary_threshold=20,  # Summarize after N messages\n            include_summary=True,\n            model=model,\n        ),\n    ],\n)\n</code></pre>"},{"location":"interceptors/#lambdaprompt","title":"LambdaPrompt","text":"<p>Custom prompt modification:</p> <pre><code>from cogent.interceptors import LambdaPrompt\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    intercept=[\n        LambdaPrompt(\n            modifier=lambda messages, ctx: [\n                {**m, \"content\": m[\"content\"].upper()}\n                if m[\"role\"] == \"user\" else m\n                for m in messages\n            ],\n        ),\n    ],\n)\n</code></pre>"},{"location":"interceptors/#custom-interceptors","title":"Custom Interceptors","text":""},{"location":"interceptors/#basic-structure","title":"Basic Structure","text":"<pre><code>from cogent.interceptors import Interceptor, Phase, InterceptContext, InterceptResult\n\nclass MyInterceptor(Interceptor):\n    \"\"\"Custom interceptor example.\"\"\"\n\n    phases = [Phase.BEFORE_LLM, Phase.AFTER_LLM]\n\n    async def intercept(\n        self,\n        phase: Phase,\n        context: InterceptContext,\n    ) -&gt; InterceptResult:\n        if phase == Phase.BEFORE_LLM:\n            # Modify messages before LLM\n            messages = context.messages\n            messages.append({\"role\": \"system\", \"content\": \"Be concise.\"})\n            return InterceptResult.modify(new_messages=messages)\n\n        elif phase == Phase.AFTER_LLM:\n            # Log response\n            print(f\"Response: {context.response.content}\")\n            return InterceptResult.continue_()\n</code></pre>"},{"location":"interceptors/#interceptcontext","title":"InterceptContext","text":"<p>Available context in interceptors:</p> <pre><code>@dataclass\nclass InterceptContext:\n    agent: Agent                  # Current agent\n    phase: Phase                  # Current phase\n    messages: list[dict]          # Current messages\n    response: AIMessage | None    # LLM response (after phases)\n    tool_call: dict | None        # Tool call info (tool phases)\n    tool_result: Any | None       # Tool result (AFTER_TOOL)\n    run_context: RunContext       # User-provided context\n    metadata: dict                # Additional data\n</code></pre>"},{"location":"interceptors/#stateful-interceptors","title":"Stateful Interceptors","text":"<pre><code>class ConversationTracker(Interceptor):\n    \"\"\"Track conversation statistics.\"\"\"\n\n    phases = [Phase.AFTER_LLM]\n\n    def __init__(self):\n        self.message_count = 0\n        self.total_tokens = 0\n\n    async def intercept(\n        self,\n        phase: Phase,\n        context: InterceptContext,\n    ) -&gt; InterceptResult:\n        self.message_count += 1\n        if context.response and context.response.usage:\n            self.total_tokens += context.response.usage.get(\"total_tokens\", 0)\n        return InterceptResult.continue_()\n\n    def stats(self) -&gt; dict:\n        return {\n            \"messages\": self.message_count,\n            \"tokens\": self.total_tokens,\n        }\n</code></pre>"},{"location":"interceptors/#combining-interceptors","title":"Combining Interceptors","text":"<p>Interceptors execute in order. Use <code>StopExecution</code> to halt the chain:</p> <pre><code>from cogent.interceptors import StopExecution\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    intercept=[\n        # Order matters - these run sequentially\n        PIIShield(patterns=[\"ssn\"]),       # First: mask PII\n        BudgetGuard(max_model_calls=10),   # Second: check budget\n        ToolGate(allow=[\"search\"]),        # Third: filter tools\n        Auditor(handler=log),              # Last: audit all activity\n    ],\n)\n\n# If BudgetGuard exceeds limit, it raises StopExecution\n# and Auditor never runs for that call\n</code></pre>"},{"location":"interceptors/#api-reference","title":"API Reference","text":""},{"location":"interceptors/#core-classes","title":"Core Classes","text":"Class Description <code>Interceptor</code> Base class for all interceptors <code>InterceptContext</code> Context passed to interceptors <code>InterceptResult</code> Return type from intercept method <code>Phase</code> Enum of interception phases <code>StopExecution</code> Exception to halt execution"},{"location":"interceptors/#built-in-interceptors_1","title":"Built-in Interceptors","text":"Category Interceptors Budget <code>BudgetGuard</code> Security <code>PIIShield</code>, <code>ContentFilter</code> Context <code>TokenLimiter</code>, <code>ContextCompressor</code> Gates <code>ToolGate</code>, <code>PermissionGate</code>, <code>ConversationGate</code> Rate Limit <code>RateLimiter</code>, <code>ThrottleInterceptor</code> Resilience <code>Failover</code>, <code>CircuitBreaker</code>, <code>ToolGuard</code> Audit <code>Auditor</code> Prompts <code>ContextPrompt</code>, <code>ConversationPrompt</code>, <code>LambdaPrompt</code>"},{"location":"memory/","title":"Memory Module","text":"<p>The <code>cogent.memory</code> module provides a memory-first architecture where memory is a first-class citizen that can be wired to any entity (Agent, Team, Flow).</p>"},{"location":"memory/#overview","title":"Overview","text":"<p>Memory enables agents to: - Persist knowledge across conversations - Share state between agents - Perform semantic search over memories - Scope memories by user, team, or conversation</p> <pre><code>from cogent import Agent\nfrom cogent.memory import Memory\n\n# Basic in-memory storage\nmemory = Memory()\nawait memory.remember(\"user_preference\", \"dark mode\")\nvalue = await memory.recall(\"user_preference\")\n\n# Wire to an agent\nagent = Agent(name=\"assistant\", model=model, memory=memory)\n</code></pre>"},{"location":"memory/#core-classes","title":"Core Classes","text":""},{"location":"memory/#memory","title":"Memory","text":"<p>The main memory interface with simple remember/recall API:</p> <pre><code>from cogent.memory import Memory\n\nmemory = Memory()\n\n# Remember a value\nawait memory.remember(\"key\", \"value\")\nawait memory.remember(\"user.name\", \"Alice\")\nawait memory.remember(\"conversation.topic\", \"AI research\")\n\n# Recall a value\nname = await memory.recall(\"user.name\")  # \"Alice\"\nmissing = await memory.recall(\"unknown\")  # None\nmissing = await memory.recall(\"unknown\", default=\"N/A\")  # \"N/A\"\n\n# Check existence\nexists = await memory.exists(\"user.name\")  # True\n\n# Delete a memory\nawait memory.forget(\"user.name\")\n\n# List all keys\nkeys = await memory.list_keys()  # [\"conversation.topic\"]\n\n# Clear all memories\nawait memory.clear()\n</code></pre>"},{"location":"memory/#scoped-memory","title":"Scoped Memory","text":"<p>Create isolated memory views for users, teams, or conversations:</p> <pre><code>from cogent.memory import Memory\n\nmemory = Memory()\n\n# Create scoped views\nuser_mem = memory.scoped(\"user:alice\")\nteam_mem = memory.scoped(\"team:research\")\nconv_mem = memory.scoped(\"conv:thread-123\")\n\n# Each scope is isolated\nawait user_mem.remember(\"preference\", \"compact\")\nawait team_mem.remember(\"preference\", \"detailed\")\n\nuser_pref = await user_mem.recall(\"preference\")  # \"compact\"\nteam_pref = await team_mem.recall(\"preference\")  # \"detailed\"\n\n# Scopes can be nested\nproject_mem = team_mem.scoped(\"project:alpha\")\nawait project_mem.remember(\"status\", \"active\")\n</code></pre>"},{"location":"memory/#shared-memory-between-agents","title":"Shared Memory Between Agents","text":"<p>Wire the same memory to multiple agents for shared knowledge:</p> <pre><code>from cogent import Agent\nfrom cogent.memory import Memory\n\n# Shared memory instance\nshared = Memory()\n\n# Both agents share the same memory\nresearcher = Agent(name=\"researcher\", model=model, memory=shared)\nwriter = Agent(name=\"writer\", model=model, memory=shared)\n\n# Researcher stores findings\nawait shared.remember(\"findings\", \"Key insight: AI adoption is growing\")\n\n# Writer can access them\nfindings = await shared.recall(\"findings\")\n</code></pre>"},{"location":"memory/#storage-backends","title":"Storage Backends","text":""},{"location":"memory/#inmemorystore-default","title":"InMemoryStore (Default)","text":"<p>Fast, no-persistence storage for development and testing:</p> <pre><code>from cogent.memory import Memory, InMemoryStore\n\n# Default - uses InMemoryStore\nmemory = Memory()\n\n# Explicit\nmemory = Memory(store=InMemoryStore())\n</code></pre>"},{"location":"memory/#sqlalchemystore","title":"SQLAlchemyStore","text":"<p>Persistent storage with SQLAlchemy 2.0 async support:</p> <pre><code>from cogent.memory import Memory, SQLAlchemyStore\n\n# SQLite (local file)\nstore = SQLAlchemyStore(\"sqlite+aiosqlite:///./memory.db\")\nmemory = Memory(store=store)\n\n# PostgreSQL\nstore = SQLAlchemyStore(\n    \"postgresql+asyncpg://user:pass@localhost/db\",\n    pool_size=10,\n)\nmemory = Memory(store=store)\n\n# Initialize tables (run once)\nawait store.initialize()\n\n# Cleanup\nawait store.close()\n</code></pre> <p>Context manager for cleanup:</p> <pre><code>async with SQLAlchemyStore(\"sqlite+aiosqlite:///./data.db\") as store:\n    memory = Memory(store=store)\n    await memory.remember(\"key\", \"value\")\n</code></pre>"},{"location":"memory/#redisstore","title":"RedisStore","text":"<p>Distributed cache with native TTL support:</p> <pre><code>from cogent.memory import Memory, RedisStore\n\nstore = RedisStore(\n    url=\"redis://localhost:6379\",\n    prefix=\"myapp:\",  # Key prefix\n    default_ttl=3600,  # 1 hour default TTL\n)\nmemory = Memory(store=store)\n\n# With TTL per key\nawait memory.remember(\"session\", {\"user\": \"alice\"}, ttl=1800)\n</code></pre>"},{"location":"memory/#semantic-search","title":"Semantic Search","text":"<p>Add a vector store to enable semantic search over memories:</p> <pre><code>from cogent.memory import Memory, SQLAlchemyStore\nfrom cogent.vectorstore import VectorStore\n\n# Memory with semantic search\nmemory = Memory(\n    store=SQLAlchemyStore(\"sqlite+aiosqlite:///./data.db\"),\n    vectorstore=VectorStore(),\n)\n\n# Store with text content (auto-embedded)\nawait memory.remember(\"doc1\", \"Python is a programming language\")\nawait memory.remember(\"doc2\", \"Machine learning uses algorithms\")\nawait memory.remember(\"doc3\", \"JavaScript runs in browsers\")\n\n# Semantic search\nresults = await memory.search(\"AI and coding\", k=2)\nfor result in results:\n    print(f\"{result.key}: {result.value}\")\n</code></pre>"},{"location":"memory/#memory-tools","title":"Memory Tools","text":"<p>Memory automatically exposes tools to agents for autonomous memory management:</p> <pre><code>from cogent import Agent\nfrom cogent.memory import Memory\n\n# Memory is always agentic - tools auto-added\nmemory = Memory()\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    memory=memory,\n)\n\n# Agent has 5 memory tools available:\n# 1. remember(key, value) - Save facts to long-term memory\n# 2. recall(key) - Retrieve specific facts\n# 3. forget(key) - Remove facts\n# 4. search_memories(query) - Search long-term facts (semantic when vectorstore available)\n# 5. search_conversation(query) - Search conversation history\n\n# Agent can now use memory tools autonomously\nresult = await agent.run(\"Remember that my name is Alice\")\nresult = await agent.run(\"What's my name?\")\n</code></pre>"},{"location":"memory/#available-tools","title":"Available Tools","text":"<p>1. remember(key, value) - Save important facts <pre><code># Agent automatically calls when user shares information\nawait agent.run(\"My favorite language is Python\")\n# \u2192 Agent calls: remember(\"favorite_language\", \"Python\")\n</code></pre></p> <p>2. recall(key) - Retrieve specific saved facts <pre><code>await agent.run(\"What's my favorite language?\")\n# \u2192 Agent calls: recall(\"favorite_language\")\n</code></pre></p> <p>3. forget(key) - Remove facts (when user requests) <pre><code>await agent.run(\"Forget my favorite language\")\n# \u2192 Agent calls: forget(\"favorite_language\")\n</code></pre></p> <p>4. search_memories(query, k=5) - Search long-term facts <pre><code># Semantic search when vectorstore configured\nmemory = Memory(vectorstore=VectorStore())\nawait agent.run(\"What do you know about my hobbies?\")\n# \u2192 Agent calls: search_memories(\"hobbies\")\n\n# Falls back to keyword search without vectorstore\nmemory = Memory()  # No vectorstore\n# \u2192 Uses keyword matching on keys/values\n</code></pre></p> <p>5. search_conversation(query, max_results=5) - Search conversation history <pre><code># Critical for long conversations exceeding context window\nawait agent.run(\"What were the three projects I mentioned earlier?\")\n# \u2192 Agent calls: search_conversation(\"three projects\")\n</code></pre></p>"},{"location":"memory/#when-tools-are-used","title":"When Tools Are Used","text":"<p>The agent's system prompt instructs it to:</p> <ol> <li>At conversation start \u2192 <code>search_memories(\"user\")</code> to recall context</li> <li>When user shares info \u2192 <code>remember(key, value)</code> immediately</li> <li>When asked about something \u2192 Search before saying \"I don't know\"</li> <li>For facts \u2192 <code>search_memories(query)</code> or <code>recall(key)</code></li> <li>For past conversation \u2192 <code>search_conversation(query)</code></li> <li>In long conversations \u2192 Use <code>search_conversation()</code> to find earlier context</li> </ol> <p>Shorthand - <code>memory=True</code> creates a Memory instance:</p> <pre><code># Shorthand for Memory()\nagent = Agent(name=\"assistant\", model=model, memory=True)\n</code></pre>"},{"location":"memory/#usage-patterns","title":"Usage Patterns","text":""},{"location":"memory/#conversation-history","title":"Conversation History","text":"<pre><code>from cogent.memory import Memory\n\nmemory = Memory()\n\nasync def chat(user_id: str, message: str) -&gt; str:\n    user_mem = memory.scoped(f\"user:{user_id}\")\n\n    # Load history\n    history = await user_mem.recall(\"history\", default=[])\n    history.append({\"role\": \"user\", \"content\": message})\n\n    # Get response (using agent)\n    response = await agent.run(message, history=history)\n\n    # Save updated history\n    history.append({\"role\": \"assistant\", \"content\": response})\n    await user_mem.remember(\"history\", history)\n\n    return response\n</code></pre>"},{"location":"memory/#team-knowledge-base","title":"Team Knowledge Base","text":"<pre><code>from cogent.memory import Memory, SQLAlchemyStore\nfrom cogent.vectorstore import VectorStore\n\n# Persistent team memory with search\nteam_memory = Memory(\n    store=SQLAlchemyStore(\"sqlite+aiosqlite:///./team.db\"),\n    vectorstore=VectorStore(),\n)\n\n# Store team knowledge\nawait team_memory.remember(\"policy:vacation\", \"Employees get 20 days PTO\")\nawait team_memory.remember(\"policy:remote\", \"Remote work allowed 3 days/week\")\nawait team_memory.remember(\"contact:hr\", \"hr@company.com\")\n\n# Search policies\nresults = await team_memory.search(\"time off work\", k=3)\n</code></pre>"},{"location":"memory/#agent-with-persistent-context","title":"Agent with Persistent Context","text":"<pre><code>from cogent import Agent\nfrom cogent.memory import Memory, SQLAlchemyStore\n\nstore = SQLAlchemyStore(\"sqlite+aiosqlite:///./agent.db\")\nmemory = Memory(store=store)\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    memory=memory,\n    instructions=\"\"\"You have access to persistent memory.\n    Use it to remember user preferences and context.\"\"\",\n)\n\n# First conversation\nawait agent.run(\"My favorite color is blue\")\n\n# Later conversation (same agent)\nawait agent.run(\"What's my favorite color?\")  # Recalls \"blue\"\n</code></pre>"},{"location":"memory/#store-protocol","title":"Store Protocol","text":"<p>Implement custom storage backends:</p> <pre><code>from typing import Protocol, Any\n\nclass Store(Protocol):\n    \"\"\"Protocol for memory storage backends.\"\"\"\n\n    async def get(self, key: str) -&gt; Any | None:\n        \"\"\"Get a value by key.\"\"\"\n        ...\n\n    async def set(self, key: str, value: Any, ttl: int | None = None) -&gt; None:\n        \"\"\"Set a value with optional TTL.\"\"\"\n        ...\n\n    async def delete(self, key: str) -&gt; bool:\n        \"\"\"Delete a key. Returns True if existed.\"\"\"\n        ...\n\n    async def exists(self, key: str) -&gt; bool:\n        \"\"\"Check if key exists.\"\"\"\n        ...\n\n    async def keys(self, pattern: str = \"*\") -&gt; list[str]:\n        \"\"\"List keys matching pattern.\"\"\"\n        ...\n\n    async def clear(self) -&gt; None:\n        \"\"\"Clear all keys.\"\"\"\n        ...\n</code></pre> <p>Custom implementation example:</p> <pre><code>class DynamoDBStore:\n    \"\"\"Custom DynamoDB backend.\"\"\"\n\n    def __init__(self, table_name: str):\n        self.table_name = table_name\n        self.client = boto3.resource(\"dynamodb\")\n        self.table = self.client.Table(table_name)\n\n    async def get(self, key: str) -&gt; Any | None:\n        response = self.table.get_item(Key={\"pk\": key})\n        item = response.get(\"Item\")\n        return item[\"value\"] if item else None\n\n    async def set(self, key: str, value: Any, ttl: int | None = None) -&gt; None:\n        item = {\"pk\": key, \"value\": value}\n        if ttl:\n            item[\"ttl\"] = int(time.time()) + ttl\n        self.table.put_item(Item=item)\n\n    # ... implement other methods\n\n# Use custom store\nmemory = Memory(store=DynamoDBStore(\"my-memories\"))\n</code></pre>"},{"location":"memory/#api-reference","title":"API Reference","text":""},{"location":"memory/#memory_1","title":"Memory","text":"Method Description <code>remember(key, value, ttl?)</code> Store a value <code>recall(key, default?)</code> Retrieve a value <code>forget(key)</code> Delete a value <code>exists(key)</code> Check if key exists <code>list_keys(pattern?)</code> List matching keys <code>clear()</code> Clear all memories <code>scoped(prefix)</code> Create scoped view <code>search(query, k?)</code> Semantic search (requires vectorstore)"},{"location":"memory/#stores","title":"Stores","text":"Store Use Case <code>InMemoryStore</code> Development, testing, ephemeral <code>SQLAlchemyStore</code> Persistent, ACID, SQL databases <code>RedisStore</code> Distributed, TTL, high-throughput"},{"location":"models/","title":"Models Module","text":"<p>The <code>cogent.models</code> module provides a 3-tier API for working with LLMs - from simple string-based models to full control with direct SDK access.</p>"},{"location":"models/#3-tier-model-api","title":"\ud83c\udfaf 3-Tier Model API","text":"<p>AgenticFlow offers three levels of abstraction - choose based on your needs:</p>"},{"location":"models/#tier-1-high-level-string-models-recommended","title":"Tier 1: High-Level (String Models) \u2b50 Recommended","text":"<p>The simplest way to get started. Just use model name strings:</p> <pre><code>from cogent import Agent\n\n# Auto-resolves to gpt-4o\nagent = Agent(\"Helper\", model=\"gpt4\")\n\n# Auto-resolves to gemini-2.5-flash\nagent = Agent(\"Helper\", model=\"gemini\")\n\n# Auto-resolves to claude-sonnet-4\nagent = Agent(\"Helper\", model=\"claude\")\n\n# Provider prefix for explicit control\nagent = Agent(\"Helper\", model=\"anthropic:claude-opus-4\")\nagent = Agent(\"Helper\", model=\"openai:gpt-4o\")\n</code></pre> <p>30+ Model Aliases: - <code>gpt4</code>, <code>gpt4-mini</code>, <code>gpt4-turbo</code>, <code>gpt35</code> - <code>claude</code>, <code>claude-opus</code>, <code>claude-haiku</code> - <code>gemini</code>, <code>gemini-flash</code>, <code>gemini-pro</code> - <code>llama</code>, <code>llama-70b</code>, <code>llama-8b</code>, <code>mixtral</code> - <code>ollama</code></p> <p>API Key Loading (Priority Order): 1. Explicit <code>api_key=</code> parameter (highest) 2. Environment variables (includes <code>.env</code> when loaded) 3. Config file <code>cogent.toml</code> / <code>cogent.yaml</code> or <code>~/.cogent/config.*</code> (lowest)</p>"},{"location":"models/#tier-2-medium-level-factory-functions","title":"Tier 2: Medium-Level (Factory Functions)","text":"<p>For when you need a model instance without an agent. Supports 4 flexible usage patterns:</p> <pre><code>from cogent.models import create_chat\n\n# Pattern 1: Model name only (auto-detects provider)\nllm = create_chat(\"gpt-4o\")              # OpenAI\nllm = create_chat(\"gemini-2.5-pro\")      # Google Gemini\nllm = create_chat(\"claude-sonnet-4\")     # Anthropic\nllm = create_chat(\"llama-3.1-8b-instant\")  # Groq\nllm = create_chat(\"mistral-small-latest\")  # Mistral\n\n# Pattern 2: Provider:model syntax (explicit provider prefix)\nllm = create_chat(\"openai:gpt-4o\")\nllm = create_chat(\"gemini:gemini-2.5-flash\")\nllm = create_chat(\"anthropic:claude-sonnet-4-20250514\")\n\n# Pattern 3: Separate provider and model arguments\nllm = create_chat(\"openai\", \"gpt-4o\")\nllm = create_chat(\"gemini\", \"gemini-2.5-pro\")\nllm = create_chat(\"anthropic\", \"claude-sonnet-4\")\n\n# Pattern 4: With additional configuration\nllm = create_chat(\"gpt-4o\", temperature=0.7, max_tokens=1000)\nllm = create_chat(\"openai\", \"gpt-4o\", api_key=\"sk-custom...\")\n\n# Use the model\nresponse = await llm.ainvoke(\"What is 2+2?\")\nprint(response.content)\n</code></pre> <p>Auto-Detection: Patterns 1 and 2 automatically detect the provider from model name prefixes: - OpenAI: <code>gpt-</code>, <code>o1-</code>, <code>o3-</code>, <code>o4-</code>, <code>text-embedding-</code>, <code>gpt-audio</code>, <code>gpt-realtime</code>, <code>sora-</code> - Gemini: <code>gemini-</code>, <code>text-embedding-</code> - Anthropic: <code>claude-</code> - Mistral: <code>mistral-</code>, <code>ministral-</code>, <code>magistral-</code>, <code>devstral-</code>, <code>codestral-</code>, <code>voxtral-</code>, <code>ocr-</code> - Cohere: <code>command-</code>, <code>c4ai-aya-</code>, <code>embed-</code>, <code>rerank-</code> - Groq: <code>llama-</code>, <code>mixtral-</code>, <code>qwen-</code>, <code>deepseek-</code>, <code>gemma-</code> - Cloudflare: <code>@cf/</code></p>"},{"location":"models/#tier-3-low-level-direct-model-classes","title":"Tier 3: Low-Level (Direct Model Classes)","text":"<p>For maximum control over model configuration:</p> <pre><code>from cogent.models import OpenAIChat, AnthropicChat, GeminiChat\n\n# Full control over all parameters\nmodel = OpenAIChat(\n    model=\"gpt-4o\",\n    temperature=0.7,\n    max_tokens=2000,\n    api_key=\"sk-...\",\n    organization=\"org-...\",\n)\n\nmodel = GeminiChat(\n    model=\"gemini-2.5-flash\",\n    temperature=0.9,\n    api_key=\"...\",\n)\n\nmodel = AnthropicChat(\n    model=\"claude-sonnet-4-20250514\",\n    max_tokens=4096,\n    api_key=\"sk-ant-...\",\n)\n</code></pre> <p>When to Use Each Tier:</p> Tier Use Case Example Tier 1 (Strings) Quick prototyping, simple agents <code>Agent(model=\"gpt4\")</code> Tier 2 (Factory) Reusable model instances <code>create_chat(\"claude\")</code> Tier 3 (Direct) Custom config, advanced features <code>OpenAIChat(temperature=0.9)</code>"},{"location":"models/#configuration","title":"Configuration","text":""},{"location":"models/#env-file-recommended-for-development","title":".env File (Recommended for Development)","text":"<p>Create a <code>.env</code> file in your project root:</p> <pre><code># .env\nOPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\nGEMINI_API_KEY=AIza...\nGROQ_API_KEY=gsk_...\n</code></pre> <p>AgenticFlow automatically loads <code>.env</code> files using <code>python-dotenv</code>.</p>"},{"location":"models/#model-overrides-environment-config","title":"Model Overrides (Environment + Config)","text":"<p>You can override default chat or embedding models via env vars or config files.</p> <p>Environment variables (highest): <pre><code>OPENAI_CHAT_MODEL=gpt-4.1\nOPENAI_EMBEDDING_MODEL=text-embedding-3-large\nGEMINI_CHAT_MODEL=gemini-2.5-flash\nGEMINI_EMBEDDING_MODEL=gemini-embedding-001\nMISTRAL_CHAT_MODEL=mistral-small-latest\nMISTRAL_EMBEDDING_MODEL=mistral-embed\nGROQ_CHAT_MODEL=llama-3.1-8b-instant\nCOHERE_CHAT_MODEL=command-a-03-2025\nCOHERE_EMBEDDING_MODEL=embed-english-v3.0\nCLOUDFLARE_CHAT_MODEL=@cf/meta/llama-3.1-8b-instruct\nCLOUDFLARE_EMBEDDING_MODEL=@cf/baai/bge-base-en-v1.5\nGITHUB_CHAT_MODEL=gpt-4.1\nGITHUB_EMBEDDING_MODEL=text-embedding-3-large\nOLLAMA_CHAT_MODEL=qwen2.5:7b\nOLLAMA_EMBEDDING_MODEL=nomic-embed-text\n</code></pre></p> <p>Config file (fallback): <pre><code>[models.openai]\nchat_model = \"gpt-4.1\"\nembedding_model = \"text-embedding-3-large\"\n</code></pre></p>"},{"location":"models/#config-file-recommended-for-production","title":"Config File (Recommended for Production)","text":"<p>Create a config file at one of these locations:</p> <p>TOML Format (<code>cogent.toml</code> or <code>~/.cogent/config.toml</code>):</p> <pre><code>[models]\ndefault = \"gpt4\"\n\n[models.openai]\napi_key = \"sk-...\"\norganization = \"org-...\"\n\n[models.anthropic]\napi_key = \"sk-ant-...\"\n\n[models.gemini]\napi_key = \"...\"\n\n[models.groq]\napi_key = \"gsk_...\"\n</code></pre> <p>YAML Format (<code>cogent.yaml</code> or <code>~/.cogent/config.yaml</code>):</p> <pre><code>models:\n  default: gpt4\n\n  openai:\n    api_key: sk-...\n    organization: org-...\n\n  anthropic:\n    api_key: sk-ant-...\n\n  gemini:\n    api_key: ...\n</code></pre>"},{"location":"models/#environment-variables","title":"Environment Variables","text":"<pre><code>export OPENAI_API_KEY=sk-...\nexport ANTHROPIC_API_KEY=sk-ant-...\nexport GEMINI_API_KEY=AIza...\nexport GROQ_API_KEY=gsk_...\n</code></pre>"},{"location":"models/#provider-support","title":"Provider Support","text":"<p>All chat models now accept multiple input formats for maximum convenience:</p>"},{"location":"models/#1-simple-string-most-convenient","title":"1. Simple String (Most Convenient)","text":"<pre><code>response = await model.ainvoke(\"What is the capital of France?\")\n</code></pre>"},{"location":"models/#2-list-of-dicts-standard-format","title":"2. List of Dicts (Standard Format)","text":"<pre><code>response = await model.ainvoke([\n    {\"role\": \"system\", \"content\": \"You are helpful\"},\n    {\"role\": \"user\", \"content\": \"Hello\"},\n])\n</code></pre>"},{"location":"models/#3-message-objects-type-safe","title":"3. Message Objects (Type-Safe)","text":"<pre><code>from cogent.core.messages import SystemMessage, HumanMessage\n\nresponse = await model.ainvoke([\n    SystemMessage(content=\"You are helpful\"),\n    HumanMessage(content=\"Hello\"),\n])\n</code></pre>"},{"location":"models/#openai","title":"OpenAI","text":"<pre><code>from cogent.models import OpenAIChat, OpenAIEmbedding\n\n# Tier 1: Simple string\nagent = Agent(\"Helper\", model=\"gpt4\")\n\n# Tier 2: Factory\nmodel = create_chat(\"gpt4\")\nmodel = create_chat(\"openai\", \"gpt-4o\")\n\n# Tier 3: Direct\nmodel = OpenAIChat(\n    model=\"gpt-4o\",\n    temperature=0.7,\n    max_tokens=2000,\n    api_key=\"sk-...\",  # Or OPENAI_API_KEY env var\n)\n\n# Embeddings\nembeddings = OpenAIEmbedding(model=\"text-embedding-3-small\")\n\n# Primary API with metadata\nresult = await embeddings.embed([\"Hello world\"])\nprint(result.embeddings)  # Vectors\nprint(result.metadata)    # Full metadata\n\n# Convenience for single text\nresult = await embeddings.embed(\"Query\")\nvector = result.embeddings[0]\n</code></pre> <p>With tools:</p> <pre><code>from cogent.tools import tool\n\n@tool\ndef search(query: str) -&gt; str:\n    \"\"\"Search the web.\"\"\"\n    return f\"Results for: {query}\"\n\nmodel = ChatModel(model=\"gpt-4o\")\nbound = model.bind_tools([search])\n\nresponse = await bound.ainvoke([\n    {\"role\": \"user\", \"content\": \"Search for AI news\"}\n])\n\nif response.tool_calls:\n    for call in response.tool_calls:\n        print(f\"Tool: {call['name']}, Args: {call['args']}\")\n</code></pre> <p>Responses API (Beta):</p> <p>OpenAI's Responses API is optimized for tool use and structured outputs. Use the <code>use_responses_api=True</code> parameter:</p> <pre><code>from cogent.models.openai import OpenAIChat\n\n# Standard Chat Completions API (default)\nmodel = OpenAIChat(model=\"gpt-4o\")\n\n# Responses API (optimized for tool use)\nmodel = OpenAIChat(model=\"gpt-4o\", use_responses_api=True)\n\n# Works seamlessly with tools\nbound = model.bind_tools([search_tool, calc_tool])\nresponse = await bound.ainvoke(messages)\n</code></pre> <p>The Responses API provides better performance for multi-turn tool conversations while maintaining the same interface.</p>"},{"location":"models/#azure-openai","title":"Azure OpenAI","text":"<p>Enterprise Azure deployments with Azure AD support:</p> <pre><code>from cogent.models.azure import AzureEntraAuth, AzureOpenAIChat, AzureOpenAIEmbedding\n\n# With API key\nmodel = AzureOpenAIChat(\n    azure_endpoint=\"https://your-resource.openai.azure.com\",\n    deployment=\"gpt-4o\",\n    api_key=\"your-api-key\",\n    api_version=\"2024-02-01\",\n)\n\n# With Entra ID (DefaultAzureCredential)\nmodel = AzureOpenAIChat(\n    azure_endpoint=\"https://your-resource.openai.azure.com\",\n    deployment=\"gpt-4o\",\n    entra=AzureEntraAuth(method=\"default\"),  # Uses DefaultAzureCredential\n)\n\n# With Entra ID (Managed Identity)\n# - System-assigned MI: omit client_id\n# - User-assigned MI: set client_id (recommended when multiple identities exist)\nmodel = AzureOpenAIChat(\n    azure_endpoint=\"https://your-resource.openai.azure.com\",\n    deployment=\"gpt-4o\",\n    entra=AzureEntraAuth(\n        method=\"managed_identity\",\n        client_id=\"&lt;USER_ASSIGNED_MANAGED_IDENTITY_CLIENT_ID&gt;\",\n    ),\n)\n\n# Embeddings\nembeddings = AzureOpenAIEmbedding(\n    azure_endpoint=\"https://your-resource.openai.azure.com\",\n    deployment=\"text-embedding-ada-002\",\n    entra=AzureEntraAuth(method=\"default\"),\n)\n\nresult = await embeddings.embed([\"Document text\"])\n</code></pre> <p>Environment variables:</p> <pre><code>AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com\nAZURE_OPENAI_API_VERSION=2024-02-01\nAZURE_OPENAI_DEPLOYMENT=gpt-4o\nAZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-ada-002\n\n# Auth selection\nAZURE_OPENAI_AUTH_TYPE=managed_identity  # api_key | default | managed_identity | client_secret\n\n# API key auth\n# AZURE_OPENAI_API_KEY=your-api-key\n\n# Managed identity auth (user-assigned MI)\n# AZURE_OPENAI_CLIENT_ID=...\n\n# Service principal auth (client secret)\n# AZURE_OPENAI_TENANT_ID=...\n# AZURE_OPENAI_CLIENT_ID=...\n# AZURE_OPENAI_CLIENT_SECRET=...\n</code></pre>"},{"location":"models/#anthropic","title":"Anthropic","text":"<p>Claude models with native SDK:</p> <pre><code>from cogent.models.anthropic import AnthropicChat\n\nmodel = AnthropicChat(\n    model=\"claude-sonnet-4-20250514\",\n    max_tokens=4096,\n    api_key=\"sk-ant-...\",  # Or ANTHROPIC_API_KEY env var\n)\n\nresponse = await model.ainvoke([\n    {\"role\": \"user\", \"content\": \"Explain quantum computing\"}\n])\n</code></pre> <p>Claude-specific features:</p> <pre><code># System message\nresponse = await model.ainvoke(\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    system=\"You are a helpful coding assistant.\",\n)\n\n# With tools\nmodel = AnthropicChat(model=\"claude-sonnet-4-20250514\")\nbound = model.bind_tools([search_tool])\n</code></pre>"},{"location":"models/#groq","title":"Groq","text":"<p>Ultra-fast inference for supported models:</p> <pre><code>from cogent.models.groq import GroqChat\n\nmodel = GroqChat(\n    model=\"llama-3.3-70b-versatile\",\n    api_key=\"gsk_...\",  # Or GROQ_API_KEY env var\n)\n\nresponse = await model.ainvoke([\n    {\"role\": \"user\", \"content\": \"Write a haiku about coding\"}\n])\n</code></pre> <p>Available models:</p> Model Description <code>llama-3.3-70b-versatile</code> Llama 3.3 70B <code>llama-3.1-8b-instant</code> Fast Llama 3.1 8B <code>mixtral-8x7b-32768</code> Mixtral 8x7B <code>gemma2-9b-it</code> Gemma 2 9B <p>Responses API (Beta):</p> <p>Groq also supports OpenAI's Responses API for optimized tool use:</p> <pre><code>from cogent.models.groq import GroqChat\n\n# Standard Chat Completions API (default)\nmodel = GroqChat(model=\"llama-3.3-70b-versatile\")\n\n# Responses API (optimized for tool use)\nmodel = GroqChat(model=\"llama-3.3-70b-versatile\", use_responses_api=True)\n\n# Works seamlessly with tools\nbound = model.bind_tools([search_tool])\nresponse = await bound.ainvoke(messages)\n</code></pre>"},{"location":"models/#google-gemini","title":"Google Gemini","text":"<p>Google's Gemini models:</p> <pre><code>from cogent.models.gemini import GeminiChat, GeminiEmbedding\n\nmodel = GeminiChat(\n    model=\"gemini-2.0-flash\",\n    api_key=\"...\",  # Or GOOGLE_API_KEY env var\n)\n\nresponse = await model.ainvoke([\n    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n])\n\n# Embeddings\nembeddings = GeminiEmbedding(model=\"text-embedding-004\")\n</code></pre>"},{"location":"models/#ollama","title":"Ollama","text":"<p>Local models via Ollama:</p> <pre><code>from cogent.models.ollama import OllamaChat, OllamaEmbedding\n\n# Chat (requires `ollama run llama3.2`)\nmodel = OllamaChat(\n    model=\"llama3.2\",\n    base_url=\"http://localhost:11434\",\n)\n\nresponse = await model.ainvoke([\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n])\n\n# Embeddings\nembeddings = OllamaEmbedding(model=\"nomic-embed-text\")\n</code></pre>"},{"location":"models/#custom-endpoints","title":"Custom Endpoints","text":"<p>Any OpenAI-compatible endpoint (vLLM, Together AI, etc.):</p> <pre><code>from cogent.models.custom import CustomChat, CustomEmbedding\n\n# vLLM\nmodel = CustomChat(\n    base_url=\"http://localhost:8000/v1\",\n    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n)\n\n# Together AI\nmodel = CustomChat(\n    base_url=\"https://api.together.xyz/v1\",\n    model=\"meta-llama/Llama-3-70b-chat-hf\",\n    api_key=\"...\",\n)\n\n# Custom embeddings\nembeddings = CustomEmbedding(\n    base_url=\"http://localhost:8000/v1\",\n    model=\"BAAI/bge-small-en-v1.5\",\n)\n</code></pre>"},{"location":"models/#factory-function","title":"Factory Function","text":"<p>Create models dynamically by provider:</p> <pre><code>from cogent.models import create_chat, create_embedding\n\n# OpenAI\nmodel = create_chat(\"openai\", model=\"gpt-4o\")\n\n# Azure\nmodel = create_chat(\n    \"azure\",\n    deployment=\"gpt-4o\",\n    azure_endpoint=\"https://your-resource.openai.azure.com\",\n    entra=AzureEntraAuth(method=\"default\"),\n)\n\n# Anthropic\nmodel = create_chat(\"anthropic\", model=\"claude-sonnet-4-20250514\")\n\n# Groq\nmodel = create_chat(\"groq\", model=\"llama-3.3-70b-versatile\")\n\n# Gemini\nmodel = create_chat(\"gemini\", model=\"gemini-2.0-flash\")\n\n# Ollama\nmodel = create_chat(\"ollama\", model=\"llama3.2\")\n\n# Custom\nmodel = create_chat(\n    \"custom\",\n    base_url=\"http://localhost:8000/v1\",\n    model=\"my-model\",\n)\n</code></pre>"},{"location":"models/#mock-models","title":"Mock Models","text":"<p>For testing without API calls:</p> <pre><code>from cogent.models import MockChatModel, MockEmbedding\n\n# Predictable responses\nmodel = MockChatModel(responses=[\"Hello!\", \"How can I help?\"])\n\nresponse = await model.ainvoke([{\"role\": \"user\", \"content\": \"Hi\"}])\nprint(response.content)  # \"Hello!\"\n\nresponse = await model.ainvoke([{\"role\": \"user\", \"content\": \"Help\"}])\nprint(response.content)  # \"How can I help?\"\n\n# Mock embeddings\nembeddings = MockEmbedding(dimension=384)\nvectors = await embeddings.embed_documents([\"test\"])\nprint(len(vectors[0]))  # 384\n</code></pre>"},{"location":"models/#streaming","title":"Streaming","text":"<p>All models support streaming with complete metadata:</p> <pre><code>from cogent.models import ChatModel\n\nmodel = ChatModel(model=\"gpt-4o\")\n\nasync for chunk in model.astream([\n    {\"role\": \"user\", \"content\": \"Write a story\"}\n]):\n    print(chunk.content, end=\"\", flush=True)\n\n    # Access metadata in all chunks\n    if chunk.metadata:\n        print(f\"\\nModel: {chunk.metadata.model}\")\n        print(f\"Response ID: {chunk.metadata.response_id}\")\n\n        # Token usage available in final chunk\n        if chunk.metadata.tokens:\n            print(f\"Tokens: {chunk.metadata.tokens.total_tokens}\")\n            print(f\"Finish: {chunk.metadata.finish_reason}\")\n</code></pre>"},{"location":"models/#streaming-metadata","title":"Streaming Metadata","text":"<p>All 10 chat providers return complete metadata during streaming:</p> Provider Model Finish Reason Token Usage Notes OpenAI \u2705 \u2705 \u2705 Uses <code>stream_options={\"include_usage\": True}</code> Gemini \u2705 \u2705 \u2705 Extracts from <code>usage_metadata</code> Groq \u2705 \u2705 \u2705 Compatible with OpenAI pattern Mistral \u2705 \u2705 \u2705 Metadata accumulation Cohere \u2705 \u2705 \u2705 Event-based streaming (<code>message-end</code>) Anthropic \u2705 \u2705 \u2705 Snapshot-based metadata Cloudflare \u2705 \u2705 \u2705 Stream options support Ollama \u2705 \u2705 \u2705 Local model metadata Azure OpenAI \u2705 \u2705 \u2705 Stream options support Azure AI Foundry / GitHub \u2705 \u2705 \u2705 Stream options via model_extras <p>Metadata Structure:</p> <pre><code>@dataclass\nclass MessageMetadata:\n    id: str | None              # Response ID\n    timestamp: str | None       # ISO 8601 timestamp\n    model: str | None           # Model name/version\n    tokens: TokenUsage | None   # Token counts\n    finish_reason: str | None   # stop, length, error\n    response_id: str | None     # Provider response ID\n    duration: float | None      # Request duration (ms)\n    correlation_id: str | None  # For tracing\n\n@dataclass\nclass TokenUsage:\n    prompt_tokens: int\n    completion_tokens: int\n    total_tokens: int\n</code></pre> <p>Streaming Pattern:</p> <ol> <li>Content chunks \u2014 Include partial metadata (model, response_id, timestamp)</li> <li>Final chunk \u2014 Empty content with complete metadata (finish_reason, tokens)</li> </ol> <pre><code># Example streaming flow\nasync for chunk in model.astream(messages):\n    # Chunks 1-N: Content with partial metadata\n    if chunk.content:\n        print(chunk.content, end=\"\")\n\n    # Final chunk: Complete metadata\n    if chunk.metadata and chunk.metadata.finish_reason:\n        print(f\"\\n\\nCompleted with {chunk.metadata.tokens.total_tokens} tokens\")\n</code></pre>"},{"location":"models/#embeddings","title":"Embeddings","text":"<p>All 9 embedding providers support a standardized API with rich metadata and flexible usage patterns:</p> <pre><code>from cogent.models import OpenAIEmbedding, GeminiEmbedding, OllamaEmbedding\n\nembedder = OpenAIEmbedding(model=\"text-embedding-3-small\")\n\n# Primary API: embed() / aembed() - Returns EmbeddingResult with full metadata\nresult = await embedder.aembed([\"Hello world\", \"AgenticFlow\"])\nprint(result.embeddings)            # list[list[float]] - the actual vectors\nprint(result.metadata.model)        # \"text-embedding-3-small\"\nprint(result.metadata.tokens)       # TokenUsage(prompt=4, completion=0, total=4)\nprint(result.metadata.dimensions)   # 1536\nprint(result.metadata.duration)     # 0.181 seconds\nprint(result.metadata.num_texts)    # 2\n\n# Convenience: embed_one() / aembed_one() - Single text, returns vector only\nvector = await embedder.aembed_one(\"Single text\")\nprint(len(vector))  # 1536\n\n# Sync versions\nresult = embedder.embed([\"Text 1\", \"Text 2\"])\nvector = embedder.embed_one(\"Single text\")\n\n# VectorStore protocol: embed_texts() / embed_query() - Async, no metadata\nvectors = await embedder.embed_texts([\"Doc1\", \"Doc2\"])  # list[list[float]]\nquery_vec = await embedder.embed_query(\"Search query\")  # list[float]\n</code></pre> <p>Standardized API Summary:</p> Method Input Returns Async Metadata <code>embed(texts)</code> <code>list[str]</code> <code>EmbeddingResult</code> \u274c \u2705 <code>aembed(texts)</code> <code>list[str]</code> <code>EmbeddingResult</code> \u2705 \u2705 <code>embed_one(text)</code> <code>str</code> <code>list[float]</code> \u274c \u274c <code>aembed_one(text)</code> <code>str</code> <code>list[float]</code> \u2705 \u274c <code>embed_texts(texts)</code> <code>list[str]</code> <code>list[list[float]]</code> \u2705 \u274c <code>embed_query(text)</code> <code>str</code> <code>list[float]</code> \u2705 \u274c <code>dimension</code> property <code>int</code> - -"},{"location":"models/#embedding-metadata","title":"Embedding Metadata","text":"<p>All 9 embedding providers return complete metadata:</p> Provider Token Usage Notes OpenAI \u2705 Extracts from <code>response.usage.prompt_tokens</code> Cohere \u2705 Extracts from <code>response.meta.billed_units.input_tokens</code> Mistral \u2705 Uses OpenAI SDK, provides token counts Azure OpenAI \u2705 Extracts from <code>response.usage</code> like OpenAI Gemini \u274c API doesn't provide token counts for embeddings Ollama \u274c Local embeddings, no token tracking Cloudflare \u274c API doesn't track tokens Mock \u274c Test embedding, no real tokens Custom \u26a1 Conditional - depends on underlying API <p>Metadata Structure:</p> <pre><code>@dataclass\nclass EmbeddingMetadata:\n    id: str                     # Unique request ID\n    timestamp: str              # ISO 8601 timestamp\n    model: str | None           # Model name/version\n    tokens: TokenUsage | None   # Token usage (if available)\n    duration: float             # Request duration (seconds)\n    dimensions: int | None      # Vector dimensions\n    num_texts: int              # Number of texts embedded\n\n@dataclass\nclass EmbeddingResult:\n    embeddings: list[list[float]]  # The actual embedding vectors\n    metadata: EmbeddingMetadata    # Complete metadata\n</code></pre> <p>Usage Examples:</p> <pre><code># Use case 1: Need metadata for cost tracking\nresult = await embedder.aembed([\"Text 1\", \"Text 2\"])\nvectors = result.embeddings\ntokens = result.metadata.tokens  # Track token usage for billing\nduration = result.metadata.duration  # Monitor performance\n\n# Use case 2: Simple embedding without metadata\nvector = await embedder.aembed_one(\"Query text\")  # Just returns the vector\n\n# Use case 3: VectorStore integration (protocol compliance)\n# These methods are used internally by VectorStore\nvectors = await embedder.embed_texts([\"Document 1\", \"Document 2\"])\nquery_vec = await embedder.embed_query(\"Search query\")\n\n# Use case 4: Sync batch embedding\nresult = embedder.embed(large_batch)  # Sync version for compatibility\n</code></pre> <p>Observability Benefits:</p> <ul> <li>Cost tracking \u2014 Monitor token usage across providers</li> <li>Performance \u2014 Track request duration and batch sizes</li> <li>Debugging \u2014 Trace requests with unique IDs and timestamps</li> <li>Model versioning \u2014 Know which embedding model version was used</li> <li>Capacity planning \u2014 Understand dimensions and text counts</li> </ul>"},{"location":"models/#streaming_1","title":"Streaming","text":"<p>All models support streaming with complete metadata:</p> <pre><code>from cogent.models import ChatModel\n\nmodel = ChatModel(model=\"gpt-4o\")\n\nasync for chunk in model.astream([\n    {\"role\": \"user\", \"content\": \"Write a story\"}\n]):\n    print(chunk.content, end=\"\", flush=True)\n\n    # Access metadata in all chunks\n    if chunk.metadata:\n        print(f\"\\nModel: {chunk.metadata.model}\")\n        print(f\"Response ID: {chunk.metadata.response_id}\")\n\n        # Token usage available in final chunk\n        if chunk.metadata.tokens:\n            print(f\"Tokens: {chunk.metadata.tokens.total_tokens}\")\n            print(f\"Finish: {chunk.metadata.finish_reason}\")\n</code></pre>"},{"location":"models/#streaming-metadata_1","title":"Streaming Metadata","text":"<p>All 10 chat providers return complete metadata during streaming:</p> Provider Model Finish Reason Token Usage Notes OpenAI \u2705 \u2705 \u2705 Uses <code>stream_options={\"include_usage\": True}</code> Gemini \u2705 \u2705 \u2705 Extracts from <code>usage_metadata</code> Groq \u2705 \u2705 \u2705 Compatible with OpenAI pattern"},{"location":"models/#base-classes","title":"Base Classes","text":""},{"location":"models/#basechatmodel","title":"BaseChatModel","text":"<p>Protocol for all chat models:</p> <pre><code>from cogent.models.base import BaseChatModel\n\nclass BaseChatModel(Protocol):\n    async def ainvoke(\n        self,\n        messages: list[dict],\n        **kwargs,\n    ) -&gt; AIMessage: ...\n\n    async def astream(\n        self,\n        messages: list[dict],\n        **kwargs,\n    ) -&gt; AsyncIterator[AIMessage]: ...\n\n    def bind_tools(\n        self,\n        tools: list[BaseTool],\n    ) -&gt; BaseChatModel: ...\n</code></pre>"},{"location":"models/#aimessage","title":"AIMessage","text":"<p>Response type from chat models:</p> <pre><code>from cogent.models.base import AIMessage\n\n@dataclass\nclass AIMessage:\n    content: str\n    tool_calls: list[dict] | None = None\n    usage: dict | None = None  # {\"input_tokens\": ..., \"output_tokens\": ...}\n    raw: Any = None  # Original provider response\n</code></pre>"},{"location":"models/#baseembedding","title":"BaseEmbedding","text":"<p>Standardized protocol for all embedding models:</p> <pre><code>from cogent.models.base import BaseEmbedding\nfrom cogent.core.messages import EmbeddingResult\n\nclass BaseEmbedding(ABC):\n    # Primary methods - return full metadata\n    @abstractmethod\n    def embed(self, texts: list[str]) -&gt; EmbeddingResult:\n        \"\"\"Embed texts synchronously with metadata.\"\"\"\n        ...\n\n    @abstractmethod\n    async def aembed(self, texts: list[str]) -&gt; EmbeddingResult:\n        \"\"\"Embed texts asynchronously with metadata.\"\"\"\n        ...\n\n    # Convenience methods - single text, no metadata\n    def embed_one(self, text: str) -&gt; list[float]:\n        \"\"\"Embed single text synchronously, returns vector only.\"\"\"\n        ...\n\n    async def aembed_one(self, text: str) -&gt; list[float]:\n        \"\"\"Embed single text asynchronously, returns vector only.\"\"\"\n        ...\n\n    # VectorStore protocol - async, no metadata\n    async def embed_texts(self, texts: list[str]) -&gt; list[list[float]]:\n        \"\"\"Embed texts for VectorStore (async, returns vectors only).\"\"\"\n        ...\n\n    async def embed_query(self, text: str) -&gt; list[float]:\n        \"\"\"Embed query for VectorStore (async, returns vector only).\"\"\"\n        ...\n\n    @property\n    def dimension(self) -&gt; int:\n        \"\"\"Return embedding dimension.\"\"\"\n        ...\n</code></pre> <p>All 9 providers implement this API: - OpenAIEmbedding - AzureOpenAIEmbedding - OllamaEmbedding - CohereEmbedding - GeminiEmbedding - CloudflareEmbedding - MistralEmbedding - CustomEmbedding - MockEmbedding</p>"},{"location":"models/#api-reference","title":"API Reference","text":""},{"location":"models/#chatmodel-aliases","title":"ChatModel Aliases","text":"Alias Actual Class <code>ChatModel</code> <code>OpenAIChat</code> <code>EmbeddingModel</code> <code>OpenAIEmbedding</code>"},{"location":"models/#provider-classes","title":"Provider Classes","text":"Provider Chat Class Embedding Class OpenAI <code>OpenAIChat</code> <code>OpenAIEmbedding</code> Azure <code>AzureOpenAIChat</code> <code>AzureOpenAIEmbedding</code> Anthropic <code>AnthropicChat</code> - Groq <code>GroqChat</code> - Gemini <code>GeminiChat</code> <code>GeminiEmbedding</code> Ollama <code>OllamaChat</code> <code>OllamaEmbedding</code> Custom <code>CustomChat</code> <code>CustomEmbedding</code>"},{"location":"models/#factory-functions","title":"Factory Functions","text":"Function Description <code>create_chat(provider, **kwargs)</code> Create chat model for any provider <code>create_embedding(provider, **kwargs)</code> Create embedding model for any provider"},{"location":"observability/","title":"Observability Module","text":"<p>The <code>cogent.observability</code> module provides comprehensive monitoring, tracing, metrics, and progress output for understanding system behavior at runtime.</p>"},{"location":"observability/#overview","title":"Overview","text":"<p>The observability module includes: - TraceBus - Central pub/sub for all events - Observer - Unified observability for agents and flows - Tracer - Distributed tracing with spans - Metrics - Counters, gauges, histograms - Progress - Rich terminal output and progress tracking - Dashboard - Real-time monitoring UI</p> <pre><code>from cogent import Agent\nfrom cogent.observability import Observer\n\n# Simple: verbose output\nagent = Agent(name=\"assistant\", model=model, verbose=True)\n\n# Advanced: full observability\nobserver = Observer.trace()\nresult = await agent.run(\"Hello\", observer=observer)\n</code></pre>"},{"location":"observability/#observer","title":"Observer","text":"<p>Unified observability interface with preset levels:</p> <pre><code>from cogent.observability import Observer\n\n# Preset levels (recommended)\nobserver = Observer.silent()    # No output\nobserver = Observer.progress()  # Basic progress\nobserver = Observer.verbose()   # Show agent outputs\nobserver = Observer.debug()     # Include tool calls (excludes raw LLM content)\nobserver = Observer.trace()     # Maximum detail + graph (excludes raw LLM content)\n\n# Use with agents\nresult = await agent.run(\"Query\", observer=observer)\n\n# Use with flows\nresult = await flow.run(\"Task\", observer=observer)\n</code></pre>"},{"location":"observability/#observabilitylevel","title":"ObservabilityLevel","text":"<pre><code>from cogent.observability import ObservabilityLevel\n\nObservabilityLevel.SILENT   # No output\nObservabilityLevel.MINIMAL  # Start/complete only\nObservabilityLevel.NORMAL   # Standard progress\nObservabilityLevel.VERBOSE  # Show outputs\nObservabilityLevel.DEBUG    # Tool calls + thinking\nObservabilityLevel.TRACE    # Everything + execution graph\n</code></pre>"},{"location":"observability/#custom-observer","title":"Custom Observer","text":"<pre><code>from cogent.observability import Observer, Channel\n\nobserver = Observer(\n    level=ObservabilityLevel.DEBUG,\n    channels=[\n        Channel.CONSOLE,    # Terminal output\n        Channel.FILE,       # Log to file\n        Channel.WEBSOCKET,  # Real-time streaming\n    ],\n    file_path=\"agent.log\",\n)\n</code></pre>"},{"location":"observability/#modular-channels-opt-in-observability","title":"Modular Channels (Opt-in Observability)","text":"<p>The observability system uses channels to let you subscribe to specific event categories. This keeps output clean and focused on what matters to you:</p> <pre><code>from cogent.observability import Observer, Channel\n\n# Subscribe to specific channels\nobserver = Observer(\n    level=ObservabilityLevel.DEBUG,\n    channels=[\n        Channel.AGENTS,     # Agent lifecycle events\n        Channel.TOOLS,      # Tool calls and results\n        Channel.TASKS,      # Task execution\n    ],\n)\n\n# Available channels:\n# - Channel.AGENTS: Agent thinking, acting, status\n# - Channel.TOOLS: Tool calls, results, errors\n# - Channel.MESSAGES: Inter-agent communication\n# - Channel.TASKS: Task lifecycle\n# - Channel.LLM: Raw LLM request/response (opt-in)\n# - Channel.STREAMING: Token-by-token output\n# - Channel.MEMORY: Memory operations\n# - Channel.RETRIEVAL: RAG retrieval\n# - Channel.DOCUMENTS: Document loading/splitting\n# - Channel.MCP: Model Context Protocol\n# - Channel.REACTIVE: Reactive flow events\n# - Channel.SYSTEM: System-level events\n# - Channel.RESILIENCE: Retries, circuit breakers\n# - Channel.ALL: Everything\n</code></pre> <p>LLM Events are Opt-in: By default, LLM request/response events show subtle presence (just that a request/response occurred). To see full details (prompts, responses, content), explicitly subscribe to <code>Channel.LLM</code>:</p> <pre><code># Default behavior: LLM events show subtle presence only\nobserver = Observer(\n    level=ObservabilityLevel.DEBUG,\n    channels=[Channel.AGENTS, Channel.TOOLS],\n)\n# \u2192 LLM request (5 messages, 3 tools)\n# \u2190 LLM response 1.2s, 2 tools\n\n# Opt-in for LLM details (explicit channel subscription required)\nobserver = Observer(\n    level=ObservabilityLevel.DEBUG,\n    channels=[Channel.AGENTS, Channel.TOOLS, Channel.LLM],  # Explicitly add LLM channel\n)\n# Now you'll see prompts, system messages, response content\n\n# Note: Observer.debug() and Observer.trace() do NOT include Channel.LLM by default\n# This is intentional - LLM content requires explicit opt-in for privacy\n</code></pre> <p>This modular design ensures: - \u2705 Clean, focused output by default - \u2705 Opt-in to detailed LLM debugging when needed - \u2705 No noise from internal LLM calls unless you want it - \u2705 Easy to filter what you care about</p>"},{"location":"observability/#observing-event-driven-flows","title":"Observing Event-Driven Flows","text":"<p>The event-driven Flow paradigm is fully integrated with the observability system, providing deep visibility into event processing, reactor activations, and flow execution.</p>"},{"location":"observability/#understanding-the-dual-bus-architecture","title":"Understanding the Dual-Bus Architecture","text":"<p>AgenticFlow uses two separate event systems for clean separation of concerns:</p>"},{"location":"observability/#1-eventbus-orchestration","title":"1. EventBus (Orchestration)","text":"<p>Module: <code>cogent.events.EventBus</code> Purpose: Core orchestration and flow control Events: <code>task.created</code>, <code>agent.done</code>, <code>research.complete</code>, custom events Used by: Flow, reactors, agent coordination Consumers: Reactors registered via <code>flow.register()</code></p> <pre><code>from cogent.events import EventBus\n\n# Orchestration bus - handles flow logic\nevents = EventBus()\nawait events.publish(\"task.created\", {\"id\": \"123\"})\n</code></pre>"},{"location":"observability/#2-tracebus-observability","title":"2. TraceBus (Observability)","text":"<p>Module: <code>cogent.observability.TraceBus</code> Purpose: Telemetry, tracing, and monitoring Events: <code>TraceType</code> enum values (REACTIVE_, AGENT_, TASK_) Used by: Observer, metrics, logging, exporters Consumers*: Observer handlers, dashboards, log files</p> <pre><code>from cogent.observability import TraceBus\n\n# Observability bus - separate from orchestration\ntraces = TraceBus()\nbus.subscribe(TraceType.REACTIVE_FLOW_STARTED, on_flow_start)\n</code></pre>"},{"location":"observability/#why-two-buses","title":"Why Two Buses?","text":"Reason Benefit Separation of Concerns Orchestration logic \u2260 observability logic Performance Observability can be disabled without affecting flow Flexibility Different event schemas and lifecycles Extensibility Each bus can evolve independently"},{"location":"observability/#how-they-connect","title":"How They Connect","text":"<p>The Flow automatically bridges the two systems: - Flow publishes orchestration events to EventBus (reactors respond) - Flow's <code>_observe()</code> method emits to TraceBus (observers see it) - No direct coupling between buses - Observer attaches to TraceBus automatically</p> <pre><code>from cogent import Flow, Agent\nfrom cogent.observability import Observer\n\nobserver = Observer.trace()\nflow = Flow(observer=observer)  # Observer attaches to TraceBus\n\n# When you register reactors:\nflow.register(agent, on=\"task.created\")  # Listens to EventBus\n\n# When flow runs:\n# 1. EventBus: task.created \u2192 agent reactor\n# 2. TraceBus: REACTIVE_AGENT_TRIGGERED \u2192 observer\n</code></pre>"},{"location":"observability/#flow-trace-events","title":"Flow Trace Events","text":"<p>The Flow emits detailed trace events for every step of execution:</p> TraceType Description When Emitted Key Data <code>REACTIVE_FLOW_STARTED</code> Flow execution begins <code>flow.run()</code> called <code>task</code>, <code>agents</code>, <code>flow_id</code> <code>REACTIVE_EVENT_EMITTED</code> Event published to flow Event enters system <code>event_type</code>, <code>data</code>, <code>source</code> <code>REACTIVE_EVENT_PROCESSED</code> Event matched and handled After reactor processes <code>event_type</code>, <code>reactor</code>, <code>duration_ms</code> <code>REACTIVE_AGENT_TRIGGERED</code> Agent reactor activated Agent starts processing <code>agent</code>, <code>event</code>, <code>trigger</code> <code>REACTIVE_AGENT_COMPLETED</code> Agent finished successfully Agent returns result <code>agent</code>, <code>output</code>, <code>duration_ms</code> <code>REACTIVE_AGENT_FAILED</code> Agent encountered error Agent raises exception <code>agent</code>, <code>error</code>, <code>traceback</code> <code>REACTIVE_NO_MATCH</code> No reactors matched event Event processed but no match <code>event_type</code>, <code>available_reactors</code> <code>REACTIVE_ROUND_STARTED</code> New processing round begins Start of event loop iteration <code>round</code>, <code>pending_events</code> <code>REACTIVE_ROUND_COMPLETED</code> Round finished All events in round processed <code>round</code>, <code>events_processed</code>, <code>duration_ms</code> <code>REACTIVE_FLOW_COMPLETED</code> Flow execution finished Flow terminates successfully <code>output</code>, <code>total_events</code>, <code>total_rounds</code> <code>REACTIVE_FLOW_FAILED</code> Flow execution failed Flow terminates with error <code>error</code>, <code>partial_output</code>, <code>events_processed</code>"},{"location":"observability/#observer-levels-for-flows","title":"Observer Levels for Flows","text":"<p>Different observer levels reveal different aspects of flow execution:</p>"},{"location":"observability/#silent","title":"SILENT","text":"<p>No output whatsoever.</p> <pre><code>observer = Observer.silent()\nflow = Flow(observer=observer)\nawait flow.run(\"task\")\n# \u2192 (no output)\n</code></pre>"},{"location":"observability/#progress","title":"PROGRESS","text":"<p>Basic flow progress only - good for production monitoring.</p> <pre><code>observer = Observer.progress()\nflow = Flow(observer=observer)\nawait flow.run(\"task\")\n</code></pre> <p>Output: <pre><code>\u26a1 Flow started (3 agents registered)\n\u23f1\ufe0f  Round 1...\n\u23f1\ufe0f  Round 2...\n\u2705 Flow completed in 2.3s\n</code></pre></p>"},{"location":"observability/#verbose","title":"VERBOSE","text":"<p>Flow progress + agent outputs - shows what's happening.</p> <pre><code>observer = Observer.verbose()\nflow = Flow(observer=observer)\nawait flow.run(\"task\")\n</code></pre> <p>Output: <pre><code>\u26a1 Flow started\n\ud83d\udce4 Event emitted: task.created\n\ud83e\udd16 researcher triggered by task.created\n\ud83d\udcdd researcher: \"Based on my research...\"\n\ud83d\udce4 Event emitted: research.done\n\ud83e\udd16 writer triggered by research.done\n\ud83d\udcdd writer: \"Here's the article...\"\n\u2705 Flow completed\n</code></pre></p>"},{"location":"observability/#debug","title":"DEBUG","text":"<p>Detailed execution - includes events, conditions, reactor matching.</p> <pre><code>observer = Observer.debug()\nflow = Flow(observer=observer)\nawait flow.run(\"task\")\n</code></pre> <p>Output: <pre><code>\u26a1 REACTIVE_FLOW_STARTED\n  task: \"Write about quantum computing\"\n  agents: [researcher, writer]\n\n\ud83d\udce4 REACTIVE_EVENT_EMITTED: task.created\n  data: {type: \"research\"}\n\n\ud83d\udd0d Matching reactors...\n  \u2713 researcher matches (priority: 0)\n\n\ud83e\udd16 REACTIVE_AGENT_TRIGGERED: researcher\n  trigger: on=\"task.created\"\n  condition: None\n\n\ud83d\udcac Agent thinking...\n\n\ud83d\udcdd REACTIVE_AGENT_COMPLETED: researcher\n  output: \"Based on my research...\"\n  duration: 1.2s\n\n\ud83d\udce4 REACTIVE_EVENT_EMITTED: research.done\n  auto_emit: True\n\n\u23f1\ufe0f  REACTIVE_ROUND_COMPLETED\n  round: 1\n  events_processed: 1\n  duration: 1.2s\n</code></pre></p>"},{"location":"observability/#trace","title":"TRACE","text":"<p>Everything + execution graphs and full event history.</p> <pre><code>observer = Observer.trace()\nflow = Flow(observer=observer)\nawait flow.run(\"task\")\n\n# Access full trace history\nfor trace in observer.traces:\n    if trace.type.startswith(\"reactive\"):\n        print(f\"{trace.timestamp}: {trace.type}\")\n        print(f\"  Data: {trace.data}\")\n</code></pre>"},{"location":"observability/#common-observability-patterns","title":"Common Observability Patterns","text":""},{"location":"observability/#1-debugging-event-flow","title":"1. Debugging Event Flow","text":"<p>See which events triggered which reactors:</p> <pre><code>observer = Observer.debug()\nflow = Flow(observer=observer)\n\nresult = await flow.run(\"task\")\n\n# Filter reactive events\nreactive_events = [\n    observed.event for observed in observer.events()\n    if observed.event.type.value.startswith(\"reactive\")\n]\n\nfor event in reactive_events:\n    print(f\"{event.type}: {event.data.get('event_type', 'N/A')}\")\n</code></pre>"},{"location":"observability/#2-tracking-performance","title":"2. Tracking Performance","text":"<p>Identify slow reactors and bottlenecks:</p> <pre><code>observer = Observer.trace()\nflow = Flow(observer=observer)\n\nresult = await flow.run(\"task\")\n\n# Find slow agent executions\nslow_agents = [\n    observed.event for observed in observer.events()\n    if observed.event.type == TraceType.REACTIVE_AGENT_COMPLETED\n    and observed.event.data.get(\"duration_ms\", 0) &gt; 1000  # &gt; 1 second\n]\n\nfor event in slow_agents:\n    agent = event.data[\"agent\"]\n    duration = event.data[\"duration_ms\"]\n    print(f\"{agent} took {duration:.0f}ms\")\n</code></pre>"},{"location":"observability/#3-understanding-event-chains","title":"3. Understanding Event Chains","text":"<p>See how events flow through the system:</p> <pre><code>observer = Observer.trace()\nflow = Flow(observer=observer)\n\nresult = await flow.run(\"task\")\n\n# Build event chain\nevents = [\n    observed.event for observed in observer.events()\n    if observed.event.type == TraceType.REACTIVE_EVENT_EMITTED\n]\n\nprint(\"Event Chain:\")\nfor i, event in enumerate(events, 1):\n    event_type = event.data[\"event_type\"]\n    source = event.data.get(\"source\", \"system\")\n    print(f\"{i}. {event_type} (from {source})\")\n</code></pre>"},{"location":"observability/#4-detecting-issues","title":"4. Detecting Issues","text":"<p>Find events that didn't match any reactors:</p> <pre><code>observer = Observer.debug()\nflow = Flow(observer=observer)\n\nresult = await flow.run(\"task\")\n\n# Find unmatched events\nunmatched = [\n    observed.event for observed in observer.events()\n    if observed.event.type == TraceType.REACTIVE_NO_MATCH\n]\n\nif unmatched:\n    print(\"\u26a0\ufe0f  Events with no matching reactors:\")\n    for event in unmatched:\n        event_type = event.data[\"event_type\"]\n        available = event.data.get(\"available_reactors\", [])\n        print(f\"  - {event_type} (available: {available})\")\n</code></pre>"},{"location":"observability/#5-exporting-traces","title":"5. Exporting Traces","text":"<p>Save flow execution for later analysis:</p> <pre><code>observer = Observer.trace()\nflow = Flow(observer=observer)\n\nresult = await flow.run(\"task\")\n\n# Export to JSON\nimport json\nfrom pathlib import Path\n\ntraces_data = [\n    {\n        \"type\": observed.event.type.value,\n        \"timestamp\": observed.event.timestamp.isoformat(),\n        \"data\": observed.event.data,\n    }\n    for observed in observer.events()\n    if observed.event.type.value.startswith(\"reactive\")\n]\n\nPath(\"flow_traces.json\").write_text(json.dumps(traces_data, indent=2))\nprint(f\"\u2705 Exported {len(traces_data)} traces\")\n</code></pre>"},{"location":"observability/#working-with-multiple-flows","title":"Working with Multiple Flows","text":"<p>Share an observer across multiple flow executions:</p> <pre><code>observer = Observer.debug()\n\n# Flow 1\nflow1 = Flow(observer=observer)\nresult1 = await flow1.run(\"task 1\")\n\n# Flow 2\nflow2 = Flow(observer=observer)\nresult2 = await flow2.run(\"task 2\")\n\n# Observer sees both flows\nall_flows = [\n    observed.event for observed in observer.events()\n    if observed.event.type == TraceType.REACTIVE_FLOW_STARTED\n]\n\nprint(f\"Total flows observed: {len(all_flows)}\")\n</code></pre>"},{"location":"observability/#best-practices","title":"Best Practices","text":"<ol> <li>Start with PROGRESS: Use <code>Observer.progress()</code> for production</li> <li>DEBUG for development: Use <code>Observer.debug()</code> during development</li> <li>TRACE for troubleshooting: Use <code>Observer.trace()</code> when debugging issues</li> <li>Filter traces: Don't process all traces - filter by type</li> <li>Export for analysis: Save traces to JSON for offline analysis</li> <li>Monitor performance: Track <code>duration_ms</code> in traces to find bottlenecks</li> <li>Check for NO_MATCH: Indicates misconfigured reactors</li> </ol>"},{"location":"observability/#example-full-flow-observability","title":"Example: Full Flow Observability","text":"<pre><code>from cogent import Agent, Flow\nfrom cogent.observability import Observer, TraceType\n\n# Setup\nmodel = get_model()\nresearcher = Agent(name=\"researcher\", model=model)\nwriter = Agent(name=\"writer\", model=model)\n\nobserver = Observer.debug()\nflow = Flow(observer=observer)\n\nflow.register(researcher, on=\"task.created\", emits=\"research.done\")\nflow.register(writer, on=\"research.done\", emits=\"flow.done\")\n\n# Execute\nresult = await flow.run(\"Write about quantum computing\")\n\n# Analyze\nprint(\"\\n=== Execution Summary ===\")\nprint(f\"Output: {result.output[:100]}...\")\nprint(f\"Events processed: {len([o for o in observer.events() if 'EVENT' in o.event.type.value])}\")\nprint(f\"Agents triggered: {len([o for o in observer.events() if o.event.type == TraceType.REACTIVE_AGENT_TRIGGERED])}\")\n\n# Performance analysis\nagent_times = {}\nfor observed in observer.events():\n    if observed.event.type == TraceType.REACTIVE_AGENT_COMPLETED:\n        agent = observed.event.data[\"agent\"]\n        duration = observed.event.data[\"duration_ms\"]\n        agent_times[agent] = duration\n\nprint(\"\\n=== Performance ===\")\nfor agent, duration in agent_times.items():\n    print(f\"{agent}: {duration:.0f}ms\")\n\n# Event chain\nprint(\"\\n=== Event Chain ===\")\nevents = [\n    observed.event.data[\"event_type\"]\n    for observed in observer.events()\n    if observed.event.type == TraceType.REACTIVE_EVENT_EMITTED\n]\nprint(\" \u2192 \".join(events))\n</code></pre>"},{"location":"observability/#tracebus","title":"TraceBus","text":"<p>Central pub/sub system for all framework events:</p> <pre><code>from cogent.observability import TraceBus, Event\nfrom cogent.core import TraceType\n\nbus = TraceBus()\n\n# Subscribe to specific event type\ndef on_task_complete(trace: Trace):\n    print(f\"Task completed: {event.data['task_id']}\")\n\nbus.subscribe(TraceType.TASK_COMPLETED, on_task_complete)\n\n# Subscribe to multiple types\nbus.subscribe_many(\n    [TraceType.TASK_STARTED, TraceType.TASK_COMPLETED],\n    log_task_events,\n)\n\n# Subscribe to ALL events\nbus.subscribe_all(lambda e: print(e))\n\n# Publish events\nawait bus.publish(Event(\n    type=TraceType.TASK_STARTED,\n    data={\"task_id\": \"123\", \"agent\": \"worker\"},\n))\n\n# Simple publish API\nawait bus.publish(\"task.completed\", {\"task_id\": \"123\"})\n</code></pre>"},{"location":"observability/#async-handlers","title":"Async Handlers","text":"<p>Both sync and async handlers are supported:</p> <pre><code># Sync handler\ndef sync_handler(trace: Trace):\n    print(event.data)\n\n# Async handler\nasync def async_handler(trace: Trace):\n    await send_notification(event.data)\n\nbus.subscribe(TraceType.TASK_COMPLETED, sync_handler)\nbus.subscribe(TraceType.TASK_COMPLETED, async_handler)\n</code></pre>"},{"location":"observability/#event-history","title":"Event History","text":"<p>Query past events:</p> <pre><code># Get event history\nevents = bus.get_history(\n    event_type=TraceType.TASK_COMPLETED,\n    limit=10,\n)\n\n# Filter by custom function\ntask_events = bus.get_history(\n    filter_fn=lambda e: e.data.get(\"task_id\") == \"123\"\n)\n</code></pre>"},{"location":"observability/#global-tracebus","title":"Global TraceBus","text":"<pre><code>from cogent.observability import get_trace_bus, set_trace_bus\n\n# Get the global bus\nbus = get_trace_bus()\n\n# Set a custom global bus\ncustom_bus = TraceBus(max_history=50000)\nset_trace_bus(custom_bus)\n</code></pre>"},{"location":"observability/#trace_1","title":"Trace","text":"<p>Immutable event records:</p> <pre><code>from cogent.observability import Trace\n\nevent = Event(\n    type=TraceType.TASK_COMPLETED,\n    data={\"task_id\": \"123\", \"result\": \"success\"},\n    source=\"agent:researcher\",\n    correlation_id=\"req-456\",\n)\n\nprint(event.id)          # Unique event ID\nprint(event.type)        # TraceType enum\nprint(event.data)        # Event payload\nprint(event.timestamp)   # When it occurred\nprint(event.source)      # What emitted it\nprint(event.correlation_id)  # For tracing\n</code></pre>"},{"location":"observability/#event-handlers","title":"Event Handlers","text":"<p>Pre-built handlers for common use cases:</p>"},{"location":"observability/#consoleeventhandler","title":"ConsoleEventHandler","text":"<pre><code>from cogent.observability import ConsoleEventHandler\n\nhandler = ConsoleEventHandler(\n    format=\"[{timestamp}] {type}: {data}\",\n    colored=True,\n)\n\nbus.subscribe_all(handler)\n</code></pre>"},{"location":"observability/#fileeventhandler","title":"FileEventHandler","text":"<pre><code>from cogent.observability import FileEventHandler\n\nhandler = FileEventHandler(\n    path=\"events.jsonl\",\n    format=\"json\",  # or \"text\"\n    rotate_size_mb=100,\n)\n\nbus.subscribe_all(handler)\n</code></pre>"},{"location":"observability/#filteringeventhandler","title":"FilteringEventHandler","text":"<pre><code>from cogent.observability import FilteringEventHandler\n\nhandler = FilteringEventHandler(\n    wrapped=ConsoleEventHandler(),\n    include_types=[TraceType.TASK_COMPLETED, TraceType.TASK_FAILED],\n    exclude_data_keys=[\"sensitive_field\"],\n)\n\nbus.subscribe_all(handler)\n</code></pre>"},{"location":"observability/#metricseventhandler","title":"MetricsEventHandler","text":"<pre><code>from cogent.observability import MetricsEventHandler\n\nhandler = MetricsEventHandler()\nbus.subscribe_all(handler)\n\n# Get metrics\nprint(handler.metrics)\n# {\n#     \"task_completed\": 42,\n#     \"task_failed\": 3,\n#     \"tool_called\": 156,\n# }\n</code></pre>"},{"location":"observability/#tracer","title":"Tracer","text":"<p>Distributed tracing with spans:</p> <pre><code>from cogent.observability import Tracer, SpanKind\n\ntracer = Tracer(service_name=\"my-agent\")\n\nasync with tracer.start_span(\"process_request\", kind=SpanKind.SERVER) as span:\n    span.set_attribute(\"user_id\", \"123\")\n\n    # Nested spans\n    async with tracer.start_span(\"query_database\") as db_span:\n        db_span.set_attribute(\"query\", \"SELECT ...\")\n        result = await db.query(...)\n\n    async with tracer.start_span(\"call_llm\") as llm_span:\n        llm_span.set_attribute(\"model\", \"gpt-4o\")\n        response = await llm.invoke(...)\n</code></pre>"},{"location":"observability/#span-context","title":"Span Context","text":"<pre><code>from cogent.observability import SpanContext\n\n# Get current span context\nctx = tracer.get_current_context()\n\n# Propagate to other services\nheaders = {\"traceparent\": ctx.to_header()}\n\n# Create span from incoming context\nincoming_ctx = SpanContext.from_header(request.headers[\"traceparent\"])\nasync with tracer.start_span(\"handle\", context=incoming_ctx) as span:\n    ...\n</code></pre>"},{"location":"observability/#execution-tracer","title":"Execution Tracer","text":"<p>Detailed execution tracing for debugging:</p> <pre><code>from cogent.observability import ExecutionTracer, TraceLevel\n\ntracer = ExecutionTracer(level=TraceLevel.DETAILED)\n\nresult = await agent.run(\"Query\", tracer=tracer)\n\n# Access trace\ntrace = tracer.get_trace()\nprint(f\"Nodes: {len(trace.nodes)}\")\nprint(f\"Duration: {trace.duration_ms}ms\")\n\n# Export trace\ntrace.to_json(\"trace.json\")\ntrace.to_html(\"trace.html\")\n</code></pre>"},{"location":"observability/#tracingobserver","title":"TracingObserver","text":"<p>Combine observability with tracing:</p> <pre><code>from cogent.observability import TracingObserver\n\nobserver = TracingObserver(\n    level=ObservabilityLevel.DEBUG,\n    export_on_complete=True,\n    export_path=\"traces/\",\n)\n\nresult = await flow.run(\"Task\", observer=observer)\n</code></pre>"},{"location":"observability/#metrics","title":"Metrics","text":"<p>Collect and export metrics:</p> <pre><code>from cogent.observability import MetricsCollector, Counter, Gauge, Histogram\n\ncollector = MetricsCollector()\n\n# Counter - monotonically increasing\nrequests = collector.counter(\"requests_total\", \"Total requests\")\nrequests.inc()\nrequests.inc(5)\n\n# Gauge - can go up and down\nactive = collector.gauge(\"active_agents\", \"Currently active agents\")\nactive.set(3)\nactive.inc()\nactive.dec()\n\n# Histogram - distribution of values\nlatency = collector.histogram(\n    \"request_latency_ms\",\n    \"Request latency\",\n    buckets=[10, 50, 100, 500, 1000],\n)\nlatency.observe(42.5)\n\n# Timer context manager\ntimer = collector.timer(\"operation_duration\")\nwith timer:\n    await do_work()\n</code></pre>"},{"location":"observability/#export-metrics","title":"Export Metrics","text":"<pre><code># Prometheus format\nprint(collector.export_prometheus())\n\n# JSON format\nprint(collector.export_json())\n\n# Start Prometheus endpoint\nawait collector.start_server(port=9090)\n</code></pre>"},{"location":"observability/#progress-output","title":"Progress Output","text":"<p>Rich terminal output for agent execution:</p>"},{"location":"observability/#quick-start","title":"Quick Start","text":"<pre><code>from cogent import Agent\n\n# Simple verbose flag\nagent = Agent(name=\"assistant\", model=model, verbose=True)\n\n# Or configure output\nfrom cogent.observability import configure_output, Verbosity\n\nconfigure_output(\n    verbosity=Verbosity.DETAILED,\n    show_timing=True,\n    show_tokens=True,\n)\n</code></pre>"},{"location":"observability/#outputconfig","title":"OutputConfig","text":"<pre><code>from cogent.observability import OutputConfig, Verbosity, OutputFormat, Theme\n\nconfig = OutputConfig(\n    verbosity=Verbosity.DETAILED,\n    format=OutputFormat.RICH,     # or TEXT, JSON, MINIMAL\n    theme=Theme.DARK,             # or LIGHT, MONOCHROME\n    show_thinking=True,\n    show_tool_calls=True,\n    show_timing=True,\n    show_tokens=True,\n)\n\nobserver = Observer(output_config=config)\n</code></pre>"},{"location":"observability/#verbosity-levels","title":"Verbosity Levels","text":"Level Shows <code>SILENT</code> Nothing <code>MINIMAL</code> Start/complete only <code>NORMAL</code> Progress + results <code>DETAILED</code> + Tool calls <code>DEBUG</code> + Thinking/reasoning <code>TRACE</code> Everything"},{"location":"observability/#progresstracker","title":"ProgressTracker","text":"<pre><code>from cogent.observability import ProgressTracker, ProgressEvent\n\ntracker = ProgressTracker()\n\n# Manual progress updates\ntracker.update(ProgressEvent(\n    type=\"task_started\",\n    agent=\"researcher\",\n    message=\"Starting research...\",\n))\n\ntracker.update(ProgressEvent(\n    type=\"tool_called\",\n    agent=\"researcher\",\n    tool=\"search\",\n    args={\"query\": \"AI news\"},\n))\n\ntracker.complete(result=\"Research complete\")\n</code></pre>"},{"location":"observability/#dashboard","title":"Dashboard","text":"<p>Real-time monitoring UI:</p> <pre><code>from cogent.observability import Dashboard, DashboardConfig\n\nconfig = DashboardConfig(\n    port=8080,\n    refresh_rate_ms=1000,\n    show_events=True,\n    show_metrics=True,\n    show_traces=True,\n)\n\ndashboard = Dashboard(config)\nawait dashboard.start()\n\n# Dashboard available at http://localhost:8080\n</code></pre>"},{"location":"observability/#websocket-streaming","title":"WebSocket Streaming","text":"<p>Real-time event streaming:</p> <pre><code>from cogent.observability import WebSocketServer, start_websocket_server\n\n# Start server\nserver = await start_websocket_server(port=8765)\n\n# Or use the handler directly with your own server\nfrom cogent.observability import websocket_handler\n\n# In your WebSocket endpoint:\nasync def handle_client(websocket):\n    await websocket_handler(websocket)\n</code></pre>"},{"location":"observability/#client-connection","title":"Client Connection","text":"<pre><code>// JavaScript client\nconst ws = new WebSocket('ws://localhost:8765');\n\nws.onmessage = (event) =&gt; {\n    const data = JSON.parse(event.data);\n    console.log('Event:', data.type, data.data);\n};\n</code></pre>"},{"location":"observability/#inspectors","title":"Inspectors","text":"<p>Inspect system state at runtime:</p> <pre><code>from cogent.observability import SystemInspector, AgentInspector\n\n# System-wide inspection\ninspector = SystemInspector()\nprint(inspector.summary())\nprint(inspector.active_agents())\nprint(inspector.recent_events(limit=10))\n\n# Agent-specific inspection\nagent_inspector = AgentInspector(agent)\nprint(agent_inspector.state())\nprint(agent_inspector.history())\nprint(agent_inspector.tools())\n</code></pre>"},{"location":"observability/#api-reference","title":"API Reference","text":""},{"location":"observability/#core-classes","title":"Core Classes","text":"Class Description <code>Observer</code> Unified observability interface <code>TraceBus</code> Central pub/sub system <code>Event</code> Immutable event record <code>Tracer</code> Distributed tracing <code>MetricsCollector</code> Metrics collection <code>ProgressTracker</code> Progress output <code>Dashboard</code> Real-time monitoring UI"},{"location":"observability/#event-handlers_1","title":"Event Handlers","text":"Class Description <code>ConsoleEventHandler</code> Print to terminal <code>FileEventHandler</code> Log to file <code>FilteringEventHandler</code> Filter events <code>MetricsEventHandler</code> Collect metrics"},{"location":"observability/#tracing","title":"Tracing","text":"Class Description <code>Span</code> A single traced operation <code>SpanContext</code> Context for distributed tracing <code>SpanKind</code> Type of span (SERVER, CLIENT, etc.) <code>ExecutionTracer</code> Detailed execution tracing <code>TracingObserver</code> Combined observer + tracer"},{"location":"observability/#metrics_1","title":"Metrics","text":"Class Description <code>Counter</code> Monotonically increasing counter <code>Gauge</code> Value that can go up/down <code>Histogram</code> Distribution of values <code>Timer</code> Duration measurement"},{"location":"observability/#progress_1","title":"Progress","text":"Class Description <code>OutputConfig</code> Output configuration <code>Verbosity</code> Verbosity levels <code>OutputFormat</code> Output formats (RICH, TEXT, JSON) <code>Theme</code> Color themes <code>ProgressEvent</code> Progress update event"},{"location":"resilience/","title":"Tool Resilience &amp; Recovery","text":"<p>AgenticFlow provides production-grade resilience features that automatically handle transient failures, prevent cascading failures, and enable graceful degradation.</p>"},{"location":"resilience/#quick-start","title":"Quick Start","text":"<pre><code>from cogent import Agent\nfrom cogent.agent.resilience import ResilienceConfig\n\n# Default: 3 retries with exponential backoff (ENABLED by default)\nagent = Agent(name=\"MyAgent\", model=model, tools=[...])\n\n# Aggressive: 5 retries for flaky services\nagent = Agent(\n    name=\"ReliableAgent\",\n    model=model,\n    tools=[...],\n    resilience=ResilienceConfig.aggressive()\n)\n\n# Fast-fail: No retries for time-sensitive operations\nagent = Agent(\n    name=\"FastAgent\",\n    model=model,\n    tools=[...],\n    resilience=ResilienceConfig.fast_fail()\n)\n</code></pre>"},{"location":"resilience/#default-configuration","title":"Default Configuration","text":"<p>Agents have resilience ENABLED by default with sensible production settings:</p> <ul> <li>Max Retries: 3 attempts per tool call</li> <li>Strategy: Exponential backoff with jitter</li> <li>Timeout: 60 seconds per tool call  </li> <li>Circuit Breaker: Enabled (protects against cascading failures)</li> <li>Failure Learning: Enabled (learns from patterns)</li> </ul>"},{"location":"resilience/#when-resilience-applies","title":"When Resilience Applies","text":""},{"location":"resilience/#direct-tool-calls-agentact","title":"\u2705 Direct Tool Calls (<code>agent.act()</code>)","text":"<p>The resilience layer automatically retries failed tool calls:</p> <pre><code># Resilience applies here - automatic retry with exponential backoff\nresult = await agent.act(\n    tool_name=\"web_search\",\n    args={\"query\": \"Python tutorials\"},\n    use_resilience=True  # Default\n)\n</code></pre> <p>Behavior: Tool failures are retried transparently. The LLM never sees transient errors.</p>"},{"location":"resilience/#llm-driven-execution-agentrun","title":"\ud83e\udd16 LLM-Driven Execution (<code>agent.run()</code>)","text":"<p>When the LLM decides which tools to call, resilience does NOT apply by default:</p> <pre><code># LLM sees tool errors and decides retry strategy\nresult = await agent.run(\"Search for Python tutorials\")\n</code></pre> <p>Behavior: Tool errors are returned to the LLM, which can decide whether to retry, try a different approach, or give up.</p> <p>Why? This gives the LLM flexibility to adapt its strategy based on errors rather than blindly retrying.</p>"},{"location":"resilience/#workflows-programmatic-usage","title":"\ud83d\udd27 Workflows &amp; Programmatic Usage","text":"<p>Use <code>.act()</code> in workflows and scripts to get automatic resilience:</p> <pre><code># In a workflow - resilience applies to each tool call\nasync def data_pipeline(agent: Agent):\n    raw_data = await agent.act(\"fetch_data\", {\"source\": \"api\"})\n    cleaned = await agent.act(\"clean_data\", {\"data\": raw_data})\n    result = await agent.act(\"analyze_data\", {\"data\": cleaned})\n    return result\n</code></pre>"},{"location":"resilience/#resilience-features","title":"Resilience Features","text":""},{"location":"resilience/#1-automatic-retry-with-exponential-backoff","title":"1. Automatic Retry with Exponential Backoff","text":"<pre><code>from cogent.agent.resilience import RetryPolicy, RetryStrategy\n\npolicy = RetryPolicy(\n    max_retries=5,\n    strategy=RetryStrategy.EXPONENTIAL_JITTER,\n    base_delay=0.5,  # Start with 0.5s delay\n    max_delay=30.0,  # Cap at 30s\n    jitter_factor=0.3,  # Add 30% randomness\n)\n\nconfig = ResilienceConfig(retry_policy=policy)\nagent = Agent(name=\"Agent\", model=model, resilience=config)\n</code></pre> <p>Available Strategies: - <code>EXPONENTIAL_JITTER</code>: Exponential backoff with random jitter (recommended) - <code>EXPONENTIAL</code>: Pure exponential backoff (2^attempt * base_delay) - <code>LINEAR</code>: Linear increase (attempt * base_delay) - <code>FIXED</code>: Fixed delay between retries - <code>NONE</code>: No backoff (fail fast)</p>"},{"location":"resilience/#2-circuit-breaker","title":"2. Circuit Breaker","text":"<p>Prevents cascading failures by temporarily disabling failing tools:</p> <pre><code>config = ResilienceConfig(\n    circuit_breaker_enabled=True,  # Default\n    circuit_breaker_config=CircuitBreaker(\n        failure_threshold=3,  # Open after 3 failures\n        reset_timeout=30.0,   # Wait 30s before testing recovery\n    )\n)\n</code></pre> <p>States: - CLOSED: Normal operation, requests pass through - OPEN: Blocking requests after repeated failures - HALF_OPEN: Testing if service recovered</p>"},{"location":"resilience/#3-fallback-tool-chains","title":"3. Fallback Tool Chains","text":"<p>Gracefully degrade to backup tools when primary fails:</p> <pre><code>agent = Agent(\n    name=\"Agent\",\n    model=model,\n    tools=[primary_search, backup_search],\n    resilience=ResilienceConfig(fallback_enabled=True),\n)\n\n# Register fallback chain\nagent.config = agent.config.with_fallbacks({\n    \"primary_search\": [\"backup_search\"]\n})\nagent._setup_resilience()  # Re-init resilience with new config\n\n# If primary_search fails after retries, backup_search is tried\nresult = await agent.act(\"primary_search\", {\"query\": \"test\"})\n</code></pre>"},{"location":"resilience/#4-failure-learning","title":"4. Failure Learning","text":"<p>Learn from failures to adapt future behavior:</p> <pre><code>config = ResilienceConfig(\n    learning_enabled=True,  # Default\n)\n</code></pre> <p>The resilience layer tracks: - Common failure patterns - Error message patterns (rate limits, server errors, etc.) - Recovery methods that worked - Tool reliability over time</p>"},{"location":"resilience/#pre-built-configurations","title":"Pre-Built Configurations","text":""},{"location":"resilience/#default-balanced","title":"Default / Balanced","text":"<pre><code>ResilienceConfig()  # or ResilienceConfig.balanced()\n</code></pre> <ul> <li>3 retries</li> <li>Exponential backoff with jitter</li> <li>60s timeout</li> <li>Circuit breaker enabled</li> <li>Failure learning enabled</li> </ul> <p>Use for: General purpose workflows, balanced reliability/latency</p>"},{"location":"resilience/#aggressive","title":"Aggressive","text":"<pre><code>ResilienceConfig.aggressive()\n</code></pre> <ul> <li>5 retries</li> <li>Exponential backoff with jitter</li> <li>120s timeout</li> <li>Circuit breaker enabled</li> <li>Failure learning enabled</li> <li>Fallback enabled</li> </ul> <p>Use for: Flaky external APIs, critical data retrieval, background jobs</p>"},{"location":"resilience/#fast-fail","title":"Fast-Fail","text":"<pre><code>ResilienceConfig.fast_fail()\n</code></pre> <ul> <li>0 retries</li> <li>10s timeout</li> <li>No circuit breaker</li> <li>No learning</li> <li>No fallbacks</li> </ul> <p>Use for: Real-time paths, user-facing operations, testing</p>"},{"location":"resilience/#custom-configuration","title":"Custom Configuration","text":"<p>Full control over all resilience parameters:</p> <pre><code>from cogent.agent.resilience import (\n    ResilienceConfig,\n    RetryPolicy,\n    RetryStrategy,\n    CircuitBreaker,\n    RecoveryAction,\n)\n\ncustom_config = ResilienceConfig(\n    retry_policy=RetryPolicy(\n        max_retries=4,\n        strategy=RetryStrategy.EXPONENTIAL_JITTER,\n        base_delay=1.0,\n        max_delay=60.0,\n        jitter_factor=0.25,\n    ),\n    circuit_breaker_enabled=True,\n    circuit_breaker_config=CircuitBreaker(\n        failure_threshold=5,\n        success_threshold=2,\n        reset_timeout=60.0,\n    ),\n    fallback_enabled=True,\n    learning_enabled=True,\n    timeout_seconds=90.0,\n    on_failure=RecoveryAction.SKIP,  # or RETRY, FALLBACK, ABORT, ADAPT\n)\n\nagent = Agent(name=\"Agent\", model=model, resilience=custom_config)\n</code></pre>"},{"location":"resilience/#per-tool-configuration","title":"Per-Tool Configuration","text":"<p>Override resilience settings for specific tools:</p> <pre><code>config = ResilienceConfig()\n\n# Add per-tool overrides\nconfig.tool_configs[\"expensive_tool\"] = {\n    \"timeout_seconds\": 300.0,  # 5 min timeout\n    \"retry_policy\": RetryPolicy.no_retry(),  # Don't retry\n}\n\nconfig.tool_configs[\"flaky_api\"] = {\n    \"retry_policy\": RetryPolicy.aggressive(),  # More retries\n}\n</code></pre>"},{"location":"resilience/#monitoring-resilience","title":"Monitoring Resilience","text":"<p>Track retries and recovery with observability:</p> <pre><code>from cogent.observability import Observer, ObservabilityLevel\n\nobserver = Observer(\n    level=ObservabilityLevel.DEBUG,  # See retry events\n    show_timestamps=True,\n    show_duration=True,\n)\nagent.add_observer(observer)\n\n# Output shows retry attempts:\n# [12:00:01] \ud83d\udd04 Retry 1/3 for web_search in 1.2s\n# [12:00:03] \ud83d\udd04 Retry 2/3 for web_search in 2.5s\n# [12:00:06] \u2705 Recovered via retry\n</code></pre>"},{"location":"resilience/#example-production-api-integration","title":"Example: Production API Integration","text":"<pre><code>from cogent import Agent\nfrom cogent.agent.resilience import ResilienceConfig, RetryPolicy\nfrom cogent.tools import tool\n\n@tool\nasync def external_api_call(query: str) -&gt; dict:\n    \"\"\"Call external API (may have transient failures).\"\"\"\n    # API implementation\n    pass\n\n# Production agent with resilience\nagent = Agent(\n    name=\"APIAgent\",\n    model=model,\n    tools=[external_api_call],\n    resilience=ResilienceConfig(\n        retry_policy=RetryPolicy(\n            max_retries=5,\n            base_delay=1.0,\n            max_delay=30.0,\n        ),\n        circuit_breaker_enabled=True,\n        learning_enabled=True,\n        timeout_seconds=120.0,\n    ),\n)\n\n# Direct call - automatic retry on transient failures\nresult = await agent.act(\n    tool_name=\"external_api_call\",\n    args={\"query\": \"production data\"},\n)\n</code></pre>"},{"location":"resilience/#best-practices","title":"Best Practices","text":"<ol> <li>Use <code>.act()</code> for reliability: Direct tool calls get automatic resilience</li> <li>Choose the right config: Default for most cases, Aggressive for flaky services, Fast-fail for testing</li> <li>Monitor with Observer: Track retry patterns and adjust configuration</li> <li>Set appropriate timeouts: Balance between giving tools time and failing fast</li> <li>Use fallbacks for critical paths: Define backup tools for important operations</li> <li>Test resilience: Verify retry behavior with simulated failures</li> <li>Circuit breakers prevent cascades: Let failing services recover instead of hammering them</li> </ol>"},{"location":"resilience/#troubleshooting","title":"Troubleshooting","text":""},{"location":"resilience/#tool-keeps-failing-despite-retries","title":"Tool keeps failing despite retries","text":"<p>Check: - Tool signature matches between primary and fallback - Timeout is sufficient for tool execution - Error is retryable (not ValueError, TypeError, etc.) - Circuit breaker hasn't opened</p>"},{"location":"resilience/#resilience-not-applying","title":"Resilience not applying","text":"<p>Ensure: - Using <code>agent.act()</code> not <code>agent.run()</code> for direct resilience - <code>use_resilience=True</code> (default) - Agent has resilience config set - Tool is registered with agent</p>"},{"location":"resilience/#too-many-retries-slowing-down","title":"Too many retries slowing down","text":"<p>Consider: - Reduce <code>max_retries</code> - Use <code>Fast-fail</code> config for testing - Check if error is actually retryable - Increase <code>base_delay</code> to space out retries</p>"},{"location":"resilience/#see-also","title":"See Also","text":"<ul> <li>Flow Checkpointing - Flow-level crash recovery</li> <li>tool_resilience.py - Complete working examples</li> <li>docs/observability.md - Monitoring and tracing</li> <li>docs/tools.md - Creating tools</li> </ul>"},{"location":"resilience/#flow-level-checkpointing","title":"Flow-Level Checkpointing","text":"<p>While this document covers tool-level resilience (retries, circuit breakers), AgenticFlow also provides flow-level checkpointing for crash recovery in multi-agent orchestration.</p>"},{"location":"resilience/#tool-resilience-vs-flow-checkpointing","title":"Tool Resilience vs Flow Checkpointing","text":"Feature Tool Resilience Flow Checkpointing Scope Individual tool calls Entire flow execution Purpose Handle transient failures Crash recovery Mechanism Retry + backoff Save/resume state Use Case Flaky APIs, network errors Long-running pipelines Default Enabled (3 retries) Disabled (opt-in)"},{"location":"resilience/#when-to-use-each","title":"When to Use Each","text":"<p>Tool Resilience (this document): - API calls fail intermittently - Network timeouts - Rate limiting errors - Temporary service unavailability</p> <p>Flow Checkpointing (flow.md): - Long-running multi-agent pipelines - Critical workflows that must complete - Distributed systems that may crash - Expensive computations to avoid re-running</p>"},{"location":"resilience/#quick-example-combined-resilience","title":"Quick Example: Combined Resilience","text":"<pre><code>from cogent import Agent, Flow\nfrom cogent.agent.resilience import ResilienceConfig\nfrom cogent.flow import FlowConfig, pipeline\nfrom cogent.flow.checkpointer import FileCheckpointer\n\n# Agents with tool-level resilience\nresearcher = Agent(\n    name=\"researcher\",\n    model=model,\n    tools=[web_search],\n    resilience=ResilienceConfig.aggressive(),  # Retry flaky searches\n)\n\nwriter = Agent(\n    name=\"writer\",\n    model=model,\n    resilience=ResilienceConfig(),  # Default resilience\n)\n\n# Flow with crash recovery\nflow = pipeline(\n    [researcher, writer],\n    config=FlowConfig(\n        checkpoint_every=1,  # Save after each agent\n        flow_id=\"content-001\",\n    ),\n    observer=None,\n)\nflow.checkpointer = FileCheckpointer()\n\n# Benefits:\n# - Transient API failures auto-retry (tool resilience)\n# - Flow crashes can resume from checkpoint (flow checkpointing)\nresult = await flow.run(\"Create article about AI\")\n</code></pre> <p>See Flow Checkpointing for full documentation and examples/flow/checkpointing_demo.py for a working example.</p>"},{"location":"retrievers/","title":"Retriever Guide","text":"<p>AgenticFlow provides a comprehensive retrieval system with multiple strategies for different use cases. This guide covers all available retrievers and when to use each.</p>"},{"location":"retrievers/#unified-api","title":"Unified API","text":"<p>All retrievers share a unified <code>retrieve()</code> API with optional scoring:</p> <pre><code># Get documents only (default)\ndocs = await retriever.retrieve(\"query\", k=5)\n\n# Get documents with relevance scores\nresults = await retriever.retrieve(\"query\", k=5, include_scores=True)\nfor r in results:\n    print(f\"{r.score:.3f}: {r.document.text[:50]}\")\n\n# With metadata filter\nresults = await retriever.retrieve(\n    \"query\",\n    k=10,\n    filter={\"category\": \"docs\"},\n    include_scores=True,\n)\n\n# Retriever-specific args (e.g., TimeBasedIndex)\nresults = await retriever.retrieve(\n    \"recent news\",\n    k=5,\n    time_range=TimeRange.last_days(30),\n    include_scores=True,\n)\n</code></pre>"},{"location":"retrievers/#overview","title":"Overview","text":"Category Retriever Best For Core <code>DenseRetriever</code> Semantic similarity search <code>BM25Retriever</code> Keyword/lexical matching <code>EnsembleRetriever</code> Combining multiple retrievers (dense + sparse) <code>HybridRetriever</code> Metadata filtering + content search Contextual <code>ParentDocumentRetriever</code> Precise chunks \u2192 full context <code>SentenceWindowRetriever</code> Sentence-level \u2192 paragraph context LLM-Powered <code>SummaryIndex</code> Document summaries <code>TreeIndex</code> Hierarchical summary tree <code>KeywordTableIndex</code> Keyword extraction + lookup <code>KnowledgeGraphIndex</code> Entity-based retrieval <code>SelfQueryRetriever</code> Natural language \u2192 filters Specialized <code>HierarchicalIndex</code> Structured docs (markdown/html) <code>TimeBasedIndex</code> Recency-aware retrieval <code>MultiRepresentationIndex</code> Multiple embeddings per doc"},{"location":"retrievers/#core-retrievers","title":"Core Retrievers","text":""},{"location":"retrievers/#denseretriever","title":"DenseRetriever","text":"<p>Semantic search using vector embeddings. The most common retriever for general RAG applications.</p> <pre><code>from cogent.retriever import DenseRetriever\nfrom cogent.vectorstore import VectorStore\n\n# Create vectorstore and retriever\nvectorstore = VectorStore(embeddings=embeddings)\nawait vectorstore.add_texts([\n    \"Python is a programming language\",\n    \"Machine learning uses algorithms\",\n    \"Neural networks learn from data\",\n])\n\nretriever = DenseRetriever(vectorstore)\nresults = await retriever.retrieve(\"AI and deep learning\", k=2)\n</code></pre> <p>When to use: - General semantic search - Finding conceptually similar content - When exact keyword matching isn't required</p>"},{"location":"retrievers/#bm25retriever","title":"BM25Retriever","text":"<p>Lexical retrieval using the BM25 algorithm. Fast, interpretable, and excellent for keyword queries.</p> <pre><code>from cogent.retriever import BM25Retriever\nfrom cogent.vectorstore import Document\n\n# Create documents\ndocuments = [\n    Document(text=\"Python programming tutorial\", metadata={\"type\": \"tutorial\"}),\n    Document(text=\"JavaScript web development\", metadata={\"type\": \"tutorial\"}),\n    Document(text=\"Machine learning with Python\", metadata={\"type\": \"guide\"}),\n]\n\n# Create BM25 retriever with documents\nretriever = BM25Retriever(documents, k1=1.5, b=0.75)\n\n# Or add documents later\nretriever = BM25Retriever()\nretriever.add_documents(documents)\n\n# Keyword-based search\nresults = await retriever.retrieve(\"Python tutorial\", k=2)\n</code></pre> <p>When to use: - Exact keyword matching is important - Domain-specific terminology - Fast, interpretable results needed - No embedding model available</p>"},{"location":"retrievers/#hybridretriever","title":"HybridRetriever","text":"<p>Combines metadata search with content search. Wraps any retriever and boosts/filters by metadata fields.</p> <pre><code>from cogent.retriever import HybridRetriever, DenseRetriever, MetadataMatchMode\n\n# Wrap any content retriever\ncontent_retriever = DenseRetriever(vectorstore)\n\nhybrid = HybridRetriever(\n    retriever=content_retriever,\n    metadata_fields=[\"category\", \"author\", \"department\"],\n    metadata_weight=0.3,   # 30% from metadata match\n    content_weight=0.7,    # 70% from content match\n    mode=MetadataMatchMode.BOOST,  # or ALL, ANY\n)\n\n# Query searches both metadata and content\nresults = await hybrid.retrieve(\"machine learning best practices\", k=5)\n\n# Each result has enriched metadata\nfor r in results:\n    print(f\"Content score: {r.metadata['content_score']}\")\n    print(f\"Metadata score: {r.metadata['metadata_score']}\")\n</code></pre> <p>Matching modes: - <code>BOOST</code>: Metadata matches increase score (no filtering) - <code>ALL</code>: Only return docs matching ALL metadata terms - <code>ANY</code>: Return docs matching ANY metadata term</p> <p>When to use: - Documents have rich metadata (author, category, date) - Users search by both content and attributes - Want to boost relevant metadata matches</p>"},{"location":"retrievers/#ensembleretriever","title":"EnsembleRetriever","text":"<p>Combine any number of retrievers with configurable fusion strategies.</p> <pre><code>from cogent.retriever import (\n    EnsembleRetriever,\n    DenseRetriever,\n    BM25Retriever,\n)\n\n# Combine multiple retrievers\nensemble = EnsembleRetriever(\n    retrievers=[\n        DenseRetriever(vectorstore_openai),    # OpenAI embeddings\n        DenseRetriever(vectorstore_cohere),    # Cohere embeddings\n        BM25Retriever(documents),              # Lexical\n    ],\n    weights=[0.4, 0.4, 0.2],\n    fusion=\"rrf\",  # or \"linear\", \"max\", \"voting\"\n)\n\nresults = await ensemble.retrieve(\"query\", k=10)\n</code></pre> <p>Fusion strategies: - <code>rrf</code> (Reciprocal Rank Fusion): Best for diverse retrievers (default) - <code>linear</code>: Weighted score combination - <code>max</code>: Take highest score per document - <code>voting</code>: Count how many retrievers found each doc</p> <p>Tip: The RAG capability accepts <code>retrievers=</code> directly and creates an EnsembleRetriever internally.</p>"},{"location":"retrievers/#contextual-retrievers","title":"Contextual Retrievers","text":""},{"location":"retrievers/#parentdocumentretriever","title":"ParentDocumentRetriever","text":"<p>Index small chunks for precise matching, but return full parent documents for context.</p> <pre><code>from cogent.retriever import ParentDocumentRetriever\n\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    chunk_size=500,     # Small chunks for precise matching\n    chunk_overlap=50,\n)\n\n# Add full documents (automatically chunked)\nawait retriever.add_documents(large_documents)\n\n# Search finds chunks, returns parents\nresults = await retriever.retrieve(\"specific concept\", k=3)\n# Each result is a full document, not a chunk\n</code></pre> <p>When to use: - LLM needs more context than a single chunk - Documents have interconnected information - You want precise matching with comprehensive results</p>"},{"location":"retrievers/#sentencewindowretriever","title":"SentenceWindowRetriever","text":"<p>Index individual sentences, but return with surrounding context.</p> <pre><code>from cogent.retriever import SentenceWindowRetriever\n\nretriever = SentenceWindowRetriever(\n    vectorstore=vectorstore,\n    window_size=2,  # 2 sentences before and after\n)\n\nawait retriever.add_documents(documents)\n\n# Precise sentence match with context\nresults = await retriever.retrieve(\"specific fact\", k=3, include_scores=True)\nfor r in results:\n    print(f\"Matched: {r.metadata['matched_sentence']}\")\n    print(f\"Context: {r.document.text}\")  # Full window\n</code></pre> <p>When to use: - Need precise sentence-level matching - Want to return paragraph-level context - Fact-checking or citation tasks</p>"},{"location":"retrievers/#llm-powered-indexes","title":"LLM-Powered Indexes","text":""},{"location":"retrievers/#summaryindex","title":"SummaryIndex","text":"<p>Generate LLM summaries of documents for efficient high-level retrieval.</p> <pre><code>from cogent.retriever import SummaryIndex\n\nindex = SummaryIndex(\n    llm=model,\n    vectorstore=vectorstore,\n    extract_entities=True,   # For knowledge graph\n    extract_keywords=True,\n)\n\nawait index.add_documents(long_documents)\n\n# Search by summary\nresults = await index.retrieve(\"machine learning concepts\", k=3)\n\n# Access extracted entities for KG integration\nfor doc_id, summary in index.summaries.items():\n    print(f\"Keywords: {summary.keywords}\")\n    print(f\"Entities: {summary.entities}\")\n</code></pre> <p>When to use: - Long documents that don't fit in embeddings well - Need document-level topics quickly - Building knowledge graphs from documents</p>"},{"location":"retrievers/#treeindex","title":"TreeIndex","text":"<p>Hierarchical tree of summaries for very large documents or corpora.</p> <pre><code>from cogent.retriever import TreeIndex\n\nindex = TreeIndex(\n    llm=model,\n    vectorstore=vectorstore,\n    max_children=5,      # Children per node\n    max_depth=3,         # Tree depth\n)\n\nawait index.add_documents(very_large_documents)\n\n# Efficient tree traversal\nresults = await index.retrieve(\"specific topic\", k=5)\n</code></pre> <p>When to use: - Very large documents (books, manuals) - Corpus-level search across many documents - When full indexing is too slow/expensive</p>"},{"location":"retrievers/#keywordtableindex","title":"KeywordTableIndex","text":"<p>Extract keywords with LLM and build inverted index for fast lookup.</p> <pre><code>from cogent.retriever import KeywordTableIndex\n\nindex = KeywordTableIndex(\n    llm=model,\n    max_keywords_per_doc=10,\n)\n\nawait index.add_documents(documents)\n\n# Fast keyword-based lookup\nresults = await index.retrieve(\"Python machine learning\", k=5)\n\n# Access keyword table\nprint(index.keyword_table)  # {\"python\": [doc_ids...], \"ml\": [...]}\n</code></pre> <p>When to use: - Domain with specific terminology - Fast keyword lookup needed - Interpretable retrieval wanted</p>"},{"location":"retrievers/#selfqueryretriever","title":"SelfQueryRetriever","text":"<p>LLM parses natural language queries into semantic search + metadata filters.</p> <pre><code>from cogent.retriever import SelfQueryRetriever, AttributeInfo\n\nretriever = SelfQueryRetriever(\n    vectorstore=vectorstore,\n    llm=model,\n    attribute_info=[\n        AttributeInfo(\"category\", \"Document category\", \"string\"),\n        AttributeInfo(\"year\", \"Publication year\", \"integer\"),\n        AttributeInfo(\"author\", \"Author name\", \"string\"),\n    ],\n)\n\n# Natural language with implicit filters\nresults = await retriever.retrieve(\n    \"research papers about AI from 2024 by OpenAI\"\n)\n# LLM extracts: semantic=\"AI research papers\"\n#              filter={\"year\": 2024, \"author\": \"OpenAI\"}\n</code></pre> <p>When to use: - Users query in natural language - Documents have filterable metadata - Want to combine semantic + structured search</p>"},{"location":"retrievers/#specialized-indexes","title":"Specialized Indexes","text":""},{"location":"retrievers/#hierarchicalindex","title":"HierarchicalIndex","text":"<p>Respect and leverage document structure (headers, sections).</p> <pre><code>from cogent.retriever import HierarchicalIndex\n\nindex = HierarchicalIndex(\n    vectorstore=vectorstore,\n    llm=model,\n    structure_type=\"markdown\",  # or \"html\"\n    top_k_sections=3,\n    chunks_per_section=3,\n)\n\nawait index.add_documents(structured_docs)\n\n# Find section first, then relevant chunks\nresults = await index.retrieve(\"installation\", k=5)\nfor r in results:\n    print(f\"Section: {r.metadata['section_title']}\")\n    print(f\"Path: {r.metadata['hierarchy_path']}\")\n</code></pre> <p>When to use: - Well-structured documents (docs, manuals, specs) - Want to respect document organization - Need section-level context</p>"},{"location":"retrievers/#timebasedindex","title":"TimeBasedIndex","text":"<p>Prioritize recent information with time-decay scoring.</p> <pre><code>from cogent.retriever import TimeBasedIndex, TimeRange, DecayFunction\n\nindex = TimeBasedIndex(\n    vectorstore=vectorstore,\n    decay_function=DecayFunction.EXPONENTIAL,\n    decay_rate=0.01,  # Halve score every ~70 days\n    auto_extract_timestamps=True,\n)\n\nawait index.add_documents(news_articles)\n\n# Recent docs score higher\nresults = await index.retrieve(\"market trends\", k=5)\n\n# Filter by time range\nresults = await index.retrieve(\n    \"company policy\",\n    time_range=TimeRange.last_days(30),\n)\n\n# Point-in-time query\nresults = await index.retrieve(\n    \"regulations\",\n    time_range=TimeRange.year(2023),\n)\n</code></pre> <p>Decay functions: - <code>EXPONENTIAL</code>: Smooth decay over time - <code>LINEAR</code>: Linear decrease - <code>STEP</code>: Full score within window, zero outside - <code>LOGARITHMIC</code>: Slow initial decay - <code>NONE</code>: No decay, just filtering</p> <p>When to use: - News, articles, changelogs - Evolving knowledge bases - Time-sensitive information</p>"},{"location":"retrievers/#multirepresentationindex","title":"MultiRepresentationIndex","text":"<p>Store multiple embeddings per document for diverse query handling.</p> <pre><code>from cogent.retriever import MultiRepresentationIndex, QueryType\n\nindex = MultiRepresentationIndex(\n    vectorstore=vectorstore,\n    llm=model,\n    representations=[\"original\", \"summary\", \"detailed\", \"questions\"],\n)\n\nawait index.add_documents(documents)\n\n# Auto-detect query type\nresults = await index.retrieve(\"What is machine learning?\")\n\n# Force specific representation\nresults = await index.retrieve(\n    \"backpropagation gradient calculation\",\n    query_type=QueryType.SPECIFIC,  # Uses detailed representation\n)\n\n# Search all and fuse\nresults = await index.retrieve(\n    \"AI applications\",\n    search_all=True,\n)\n</code></pre> <p>Representations: - <code>original</code>: Raw document embedding - <code>summary</code>: Conceptual summary - <code>detailed</code>: Technical details - <code>keywords</code>: Key terms - <code>questions</code>: Hypothetical Q&amp;A - <code>entities</code>: Named entities</p> <p>When to use: - Diverse query styles expected - Technical/specialized domains - Want maximum recall</p>"},{"location":"retrievers/#rerankers","title":"Rerankers","text":"<p>Rerankers improve retrieval quality by re-scoring initial results.</p> <pre><code>from cogent.retriever import (\n    DenseRetriever,\n    CrossEncoderReranker,\n    CohereReranker,\n    LLMReranker,\n)\n\n# Initial retrieval\nretriever = DenseRetriever(vectorstore)\ninitial_docs = await retriever.retrieve(query, k=20)  # Get documents\n\n# Rerank with cross-encoder (local)\nreranker = CrossEncoderReranker(model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\nreranked = await reranker.rerank(query, initial_docs, top_n=5)\n\n# Or Cohere Rerank API\nreranker = CohereReranker(api_key=\"...\")\nreranked = await reranker.rerank(query, initial_docs, top_n=5)\n\n# Or any LLM\nreranker = LLMReranker(llm=model)\nreranked = await reranker.rerank(query, initial_docs, top_n=5)\n</code></pre> <p>Available rerankers: - <code>CrossEncoderReranker</code>: Local cross-encoder models - <code>FlashRankReranker</code>: Lightweight, fast local reranker - <code>CohereReranker</code>: Cohere Rerank API - <code>LLMReranker</code>: Any LLM for pointwise scoring - <code>ListwiseLLMReranker</code>: LLM ranks all docs at once</p>"},{"location":"retrievers/#utilities","title":"Utilities","text":""},{"location":"retrievers/#fusion-functions","title":"Fusion Functions","text":"<pre><code>from cogent.retriever import fuse_results, FusionStrategy\n\n# Fuse results from multiple retrievers\nfused = fuse_results(\n    [results_1, results_2, results_3],\n    strategy=FusionStrategy.RRF,\n    weights=[0.5, 0.3, 0.2],\n    k=10,\n)\n</code></pre>"},{"location":"retrievers/#score-normalization","title":"Score Normalization","text":"<pre><code>from cogent.retriever import normalize_scores\n\n# Normalize scores to 0-1 range\nnormalized = normalize_scores(results)\n</code></pre>"},{"location":"retrievers/#deduplication","title":"Deduplication","text":"<pre><code>from cogent.retriever import deduplicate_results\n\n# Remove duplicate documents\nunique = deduplicate_results(results, by=\"content\")  # or \"id\"\n</code></pre>"},{"location":"retrievers/#citations-and-formatting","title":"Citations and Formatting","text":"<p>For RAG applications, use these utilities to prepare results for LLM prompts:</p> <pre><code>from cogent.retriever import (\n    add_citations,\n    format_context,\n    format_citations_reference,\n    filter_by_score,\n    top_k,\n)\n\n# Retrieve results\nresults = await retriever.retrieve(query, k=10, include_scores=True)\n\n# Filter low-quality results\nresults = filter_by_score(results, min_score=0.5)\nresults = top_k(results, k=5)\n\n# Add citation markers \u00ab1\u00bb, \u00ab2\u00bb, etc.\nresults = add_citations(results)\n# results[0].metadata[\"citation\"] == \"\u00ab1\u00bb\"\n\n# Format as context string for LLM prompt\ncontext = format_context(results)\n# Output:\n# \u00ab1\u00bb [Source: doc.pdf]\n# This is the first chunk of text...\n#\n# ---\n#\n# \u00ab2\u00bb [Source: other.pdf]\n# This is the second chunk...\n\n# Generate citations reference section\nreference = format_citations_reference(results)\n# Output:\n# Sources:\n# \u00ab1\u00bb doc.pdf: This is a preview of the first document...\n# \u00ab2\u00bb other.pdf: This is a preview of the second...\n</code></pre> <p>Example RAG prompt construction:</p> <pre><code>query = \"What are the key findings?\"\nresults = await retriever.retrieve(query, k=5, include_scores=True)\nresults = filter_by_score(results, min_score=0.5)\nresults = add_citations(results)\ncontext = format_context(results)\n\nprompt = f\"\"\"Based on the following context, answer the question.\nUse citation markers like \u00ab1\u00bb to reference sources.\n\nContext:\n{context}\n\nQuestion: {query}\n\nAnswer:\"\"\"\n</code></pre>"},{"location":"retrievers/#choosing-a-retriever","title":"Choosing a Retriever","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    What's your use case?                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u25bc                 \u25bc                 \u25bc\n     General RAG        Specialized        Advanced\n            \u2502                 \u2502                 \u2502\n            \u25bc                 \u2502                 \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502                 \u2502\n    \u2502 HybridRetriever\u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2524                 \u2502\n    \u2502   (default)    \u2502        \u2502                 \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502                 \u2502\n                              \u25bc                 \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n                    \u2502  Time-sensitive?    \u2502    \u2502\n                    \u2502  \u2192 TimeBasedIndex   \u2502    \u2502\n                    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524    \u2502\n                    \u2502  Structured docs?   \u2502    \u2502\n                    \u2502  \u2192 HierarchicalIndex\u2502    \u2502\n                    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524    \u2502\n                    \u2502  Need full context? \u2502    \u2502\n                    \u2502  \u2192 ParentDocument   \u2502    \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n                                               \u25bc\n                              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                              \u2502 Multiple embedding models?  \u2502\n                              \u2502 \u2192 EnsembleRetriever         \u2502\n                              \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n                              \u2502 Natural language filters?   \u2502\n                              \u2502 \u2192 SelfQueryRetriever        \u2502\n                              \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n                              \u2502 Very long documents?        \u2502\n                              \u2502 \u2192 SummaryIndex / TreeIndex  \u2502\n                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"retrievers/#performance-tips","title":"Performance Tips","text":"<ol> <li>Start with EnsembleRetriever(dense + sparse) - Best default for most cases</li> <li>Use rerankers - Cheap way to improve quality</li> <li>Retrieve more, rerank less - Get top 20-50, rerank to top 5</li> <li>Cache embeddings - Reuse for similar queries</li> <li>Batch operations - Add documents in batches</li> <li>Add HybridRetriever for metadata - When you have structured metadata to filter/boost</li> </ol>"},{"location":"skills/","title":"Skills","text":"<p>Skills are event-triggered behavioral specializations that dynamically modify an agent's context, prompts, and tools based on incoming events.</p> <p>Unlike tools (which are code-based functions), skills are prompt-driven specializations that inject temporary capabilities into agents when matching event patterns occur.</p>"},{"location":"skills/#overview","title":"Overview","text":"<p>Skills provide: - Dynamic prompt injection based on events - Temporary tool availability during specific contexts - Priority-based layering of multiple skills - Conditional activation with event filters - Context enrichment for domain-specific knowledge</p> <p>Use Skills when you need: - Role-based expertise that activates on demand - Context-aware behavior changes - Temporary tool access for specific scenarios - Event-driven prompt engineering</p>"},{"location":"skills/#creating-skills","title":"Creating Skills","text":"<p>Use the <code>skill()</code> function to define event-triggered specializations:</p> <pre><code>from cogent import skill, tool\nfrom cogent.events import has_data\n\n@tool\ndef run_python(code: str) -&gt; str:\n    \"\"\"Execute Python code and return output.\"\"\"\n    # Implementation here\n    return \"Output: ...\"\n\n@tool\ndef lint_code(code: str) -&gt; str:\n    \"\"\"Lint Python code and report issues.\"\"\"\n    return \"\u2713 Linting passed\"\n\n# Define a Python expert skill\npython_skill = skill(\n    \"python_expert\",\n    on=\"code.write\",\n    when=has_data(\"language\", \"python\"),\n    prompt=\"\"\"You are a Python expert. Follow these guidelines:\n    - Use type hints on all functions\n    - Follow PEP 8 conventions\n    - Include docstrings with Args/Returns\n    - Prefer composition over inheritance\n    - Use modern Python 3.13+ features\"\"\",\n    tools=[run_python, lint_code],\n    priority=10,\n)\n\n# Define a debugging skill\ndebugger_skill = skill(\n    \"debugger\",\n    on=\"error.*\",\n    prompt=\"\"\"You are a debugging specialist. Analyze errors systematically:\n    1. Read logs to understand what happened\n    2. Inspect relevant variables\n    3. Form a hypothesis about root cause\n    4. Propose fix with confidence level\"\"\",\n    tools=[read_logs, inspect_variables],\n    priority=20,\n)\n</code></pre>"},{"location":"skills/#skill-parameters","title":"Skill Parameters","text":"Parameter Type Description <code>name</code> <code>str</code> Unique skill identifier <code>on</code> <code>str \\| EventPattern</code> Event pattern to match (e.g., <code>\"code.*\"</code>, <code>\"error.network\"</code>) <code>when</code> <code>Callable[[Event], bool]</code> Optional condition filter for fine-grained control <code>prompt</code> <code>str</code> Prompt text injected into agent context when skill activates <code>tools</code> <code>list[Callable]</code> Tools temporarily added to agent during skill activation <code>context_enricher</code> <code>Callable[[Event, dict], dict]</code> Function to enrich context with dynamic data <code>priority</code> <code>int</code> Higher priority skills apply first (default: 0)"},{"location":"skills/#registering-skills","title":"Registering Skills","text":"<p>Skills are registered on a <code>Flow</code> and automatically activate when matching events occur:</p> <pre><code>from cogent import Flow, Agent, react_to\n\n# Create flow\nflow = Flow()\n\n# Register skills\nflow.register_skill(python_skill)\nflow.register_skill(debugger_skill)\n\n# Register agents that react to events\ncoder = Agent(\n    name=\"coder\",\n    model=\"gpt4\",\n    system_prompt=\"You are a helpful coding assistant.\",\n)\nflow.register(coder, [react_to(\"code.*\")])\n\ndebugger = Agent(\n    name=\"debugger\",\n    model=\"gpt4\",\n    system_prompt=\"You are a debugging expert.\",\n)\nflow.register(debugger, [react_to(\"error.*\")])\n\n# When events fire, matching skills inject prompts/tools automatically\nresult = await flow.run(\n    \"Write a fibonacci function\",\n    initial_event=\"code.write\",\n    initial_data={\"language\": \"python\"},\n)\n# The coder agent receives python_skill prompt and tools\n</code></pre>"},{"location":"skills/#event-pattern-matching","title":"Event Pattern Matching","text":"<p>Skills activate when their <code>on</code> pattern matches the event type:</p> <pre><code># Exact match\nskill(\"exact\", on=\"code.write\", ...)\n\n# Wildcard match (all subtypes)\nskill(\"wildcard\", on=\"code.*\", ...)\n\n# Root wildcard (all events)\nskill(\"global\", on=\"*\", ...)\n</code></pre>"},{"location":"skills/#conditional-activation","title":"Conditional Activation","text":"<p>Use <code>when</code> parameter for fine-grained control:</p> <pre><code>from cogent.events import has_data, matches\n\n# Activate only for Python code\npython_skill = skill(\n    \"python_expert\",\n    on=\"code.write\",\n    when=has_data(\"language\", \"python\"),\n    ...\n)\n\n# Activate only for high-priority errors\ncritical_skill = skill(\n    \"critical_handler\",\n    on=\"error.*\",\n    when=lambda e: e.data.get(\"severity\") == \"critical\",\n    ...\n)\n\n# Combine multiple conditions\nadvanced_skill = skill(\n    \"advanced\",\n    on=\"task.*\",\n    when=lambda e: e.data.get(\"priority\") &gt; 5 and e.data.get(\"assigned\"),\n    ...\n)\n</code></pre>"},{"location":"skills/#context-enrichers","title":"Context Enrichers","text":"<p>Context enrichers allow you to inject dynamic data into the agent's execution context based on the triggering event:</p> <pre><code>def enrich_with_history(event, context):\n    \"\"\"Add ticket history to context.\"\"\"\n    ticket_id = event.data.get(\"ticket_id\")\n    context[\"history\"] = fetch_ticket_history(ticket_id)\n    context[\"related_tickets\"] = find_related_tickets(ticket_id)\n    return context\n\nsupport_skill = skill(\n    \"support\",\n    on=\"ticket.*\",\n    prompt=\"You are a support specialist. Use the ticket history to provide context-aware assistance.\",\n    context_enricher=enrich_with_history,\n)\n</code></pre> <pre><code>def enrich_with_codebase(event, context):\n    \"\"\"Add relevant code files to context.\"\"\"\n    repo = event.data.get(\"repository\")\n    file_path = event.data.get(\"file\")\n    context[\"codebase\"] = load_relevant_files(repo, file_path)\n    context[\"dependencies\"] = get_dependencies(file_path)\n    return context\n\ncode_review_skill = skill(\n    \"code_reviewer\",\n    on=\"code.review\",\n    prompt=\"Review code with attention to dependencies and related files.\",\n    context_enricher=enrich_with_codebase,\n    priority=15,\n)\n</code></pre>"},{"location":"skills/#priority-and-layering","title":"Priority and Layering","text":"<p>When multiple skills match the same event, they are applied in priority order (highest first):</p> <pre><code># Base coding skill (low priority)\nbase_skill = skill(\n    \"base_coder\",\n    on=\"code.*\",\n    prompt=\"Write clean, maintainable code.\",\n    priority=1,\n)\n\n# Language-specific skill (medium priority)\npython_skill = skill(\n    \"python_expert\",\n    on=\"code.write\",\n    when=has_data(\"language\", \"python\"),\n    prompt=\"Use Python best practices and type hints.\",\n    priority=10,\n)\n\n# Framework-specific skill (high priority)\nfastapi_skill = skill(\n    \"fastapi_expert\",\n    on=\"code.write\",\n    when=has_data(\"framework\", \"fastapi\"),\n    prompt=\"Follow FastAPI patterns: Pydantic models, dependency injection, async.\",\n    priority=20,\n)\n\n# For a code.write event with framework=fastapi and language=python:\n# All three skills activate and prompts are injected in order:\n# 1. fastapi_skill (priority 20)\n# 2. python_skill (priority 10)\n# 3. base_skill (priority 1)\n</code></pre>"},{"location":"skills/#skill-composition-patterns","title":"Skill Composition Patterns","text":""},{"location":"skills/#domain-expert-skills","title":"Domain Expert Skills","text":"<p>Create specialized knowledge domains:</p> <pre><code>security_skill = skill(\n    \"security_expert\",\n    on=\"code.review\",\n    when=has_data(\"check_security\", True),\n    prompt=\"\"\"Review code for security vulnerabilities:\n    - SQL injection risks\n    - XSS vulnerabilities\n    - Authentication/authorization flaws\n    - Sensitive data exposure\"\"\",\n    tools=[security_scanner, vulnerability_checker],\n)\n\nperformance_skill = skill(\n    \"performance_expert\",\n    on=\"code.optimize\",\n    prompt=\"\"\"Optimize code for performance:\n    - Identify bottlenecks\n    - Suggest algorithm improvements\n    - Recommend caching strategies\n    - Analyze time/space complexity\"\"\",\n    tools=[profiler, benchmark_runner],\n)\n</code></pre>"},{"location":"skills/#workflow-stage-skills","title":"Workflow Stage Skills","text":"<p>Skills for different stages of a process:</p> <pre><code>planning_skill = skill(\n    \"planner\",\n    on=\"task.plan\",\n    prompt=\"Break down the task into actionable steps with clear success criteria.\",\n    priority=10,\n)\n\nimplementation_skill = skill(\n    \"implementer\",\n    on=\"task.execute\",\n    prompt=\"Implement the planned steps systematically, testing as you go.\",\n    tools=[execute_code, run_tests],\n    priority=10,\n)\n\nreview_skill = skill(\n    \"reviewer\",\n    on=\"task.review\",\n    prompt=\"Review implementation against requirements, check edge cases.\",\n    tools=[lint_checker, test_runner],\n    priority=10,\n)\n</code></pre>"},{"location":"skills/#contextual-tool-access","title":"Contextual Tool Access","text":"<p>Provide tools only when needed:</p> <pre><code># Database tools only during data operations\ndb_skill = skill(\n    \"database_access\",\n    on=\"data.*\",\n    prompt=\"You have database access. Query efficiently and handle transactions properly.\",\n    tools=[query_db, update_db, begin_transaction, rollback],\n)\n\n# External API tools only during integration work\napi_skill = skill(\n    \"api_integration\",\n    on=\"integration.*\",\n    prompt=\"You can make external API calls. Handle rate limits and errors gracefully.\",\n    tools=[call_api, check_rate_limit, retry_request],\n)\n</code></pre>"},{"location":"skills/#complete-example","title":"Complete Example","text":"<p>Here's a complete example showing skills in action:</p> <pre><code>from cogent import Agent, Flow, Observer, react_to, skill, tool\n\n# Define tools\n@tool\ndef run_python(code: str) -&gt; str:\n    \"\"\"Execute Python code.\"\"\"\n    # Implementation\n    return \"Execution result...\"\n\n@tool\ndef lint_code(code: str) -&gt; str:\n    \"\"\"Lint Python code.\"\"\"\n    return \"\u2713 No issues found\"\n\n@tool\ndef read_logs(path: str) -&gt; str:\n    \"\"\"Read log files.\"\"\"\n    return \"Log contents...\"\n\n@tool\ndef inspect_vars(var_name: str) -&gt; str:\n    \"\"\"Inspect variable values.\"\"\"\n    return f\"{var_name} = ...\"\n\n# Define skills\npython_skill = skill(\n    \"python_expert\",\n    on=\"code.write\",\n    when=lambda e: e.data.get(\"language\") == \"python\",\n    prompt=\"You are a Python expert. Write type-annotated, PEP 8 compliant code.\",\n    tools=[run_python, lint_code],\n    priority=10,\n)\n\ndebug_skill = skill(\n    \"debugger\",\n    on=\"error.*\",\n    prompt=\"You are debugging. Be systematic: read logs \u2192 inspect vars \u2192 hypothesize \u2192 fix.\",\n    tools=[read_logs, inspect_vars],\n    priority=20,\n)\n\n# Set up flow\nasync def main():\n    observer = Observer.progress()\n    flow = Flow(observer=observer)\n\n    # Register skills\n    flow.register_skill(python_skill)\n    flow.register_skill(debug_skill)\n\n    # Register agents\n    coder = Agent(\n        name=\"coder\",\n        model=\"gpt4\",\n        system_prompt=\"You are a helpful coding assistant.\",\n    )\n    flow.register(coder, [react_to(\"code.write\")])\n\n    debugger_agent = Agent(\n        name=\"debugger_agent\",\n        model=\"gpt4\",\n        system_prompt=\"You are a debugging expert.\",\n    )\n    flow.register(debugger_agent, [react_to(\"error.*\")])\n\n    # Run with code.write event \u2192 python_skill activates\n    result = await flow.run(\n        \"Write a function to calculate fibonacci numbers\",\n        initial_event=\"code.write\",\n        initial_data={\"language\": \"python\"},\n    )\n    print(result.output)\n\n    # Run with error event \u2192 debug_skill activates\n    result = await flow.run(\n        \"Investigate the API connection failure\",\n        initial_event=\"error.network\",\n        initial_data={\"endpoint\": \"/api/v1/users\"},\n    )\n    print(result.output)\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())\n</code></pre>"},{"location":"skills/#best-practices","title":"Best Practices","text":""},{"location":"skills/#1-keep-skills-focused","title":"1. Keep Skills Focused","text":"<p>Each skill should have a single, clear purpose:</p> <pre><code># \u2705 Good: Focused skill\npython_skill = skill(\n    \"python_expert\",\n    on=\"code.write\",\n    prompt=\"Python best practices: type hints, PEP 8, docstrings.\",\n    ...\n)\n\n# \u274c Bad: Too broad\neverything_skill = skill(\n    \"do_everything\",\n    on=\"*\",\n    prompt=\"Do everything perfectly in all domains...\",\n    ...\n)\n</code></pre>"},{"location":"skills/#2-use-appropriate-priorities","title":"2. Use Appropriate Priorities","text":"<p>Higher priority for more specific skills:</p> <pre><code>general_skill = skill(\"general\", on=\"code.*\", priority=1)\nlanguage_skill = skill(\"python\", on=\"code.*\", priority=10)\nframework_skill = skill(\"fastapi\", on=\"code.*\", priority=20)\n</code></pre>"},{"location":"skills/#3-combine-when-conditions-wisely","title":"3. Combine When Conditions Wisely","text":"<p>Use <code>when</code> for conditions that can't be expressed in event patterns:</p> <pre><code>skill(\n    \"senior_dev\",\n    on=\"code.review\",\n    when=lambda e: e.data.get(\"complexity\") == \"high\",\n    ...\n)\n</code></pre>"},{"location":"skills/#4-enrich-context-efficiently","title":"4. Enrich Context Efficiently","text":"<p>Don't over-fetch in context enrichers:</p> <pre><code># \u2705 Good: Fetch only what's needed\ndef enrich(event, context):\n    context[\"recent_history\"] = fetch_last_10_events()\n    return context\n\n# \u274c Bad: Fetching too much\ndef enrich(event, context):\n    context[\"all_history\"] = fetch_entire_database()  # Too much!\n    return context\n</code></pre>"},{"location":"skills/#5-document-skill-activation","title":"5. Document Skill Activation","text":"<p>Use clear names and docstrings:</p> <pre><code>@dataclass\nclass PythonExpertSkill:\n    \"\"\"Activates on Python code write events.\n\n    Injects Python best practices and provides linting/execution tools.\n    \"\"\"\n</code></pre>"},{"location":"skills/#see-also","title":"See Also","text":"<ul> <li>Flow Documentation - Complete Flow system guide</li> <li>Events Documentation - Event patterns and matching</li> <li>Tools Documentation - Creating and using tools</li> <li>Examples: skills.py - Working example code</li> </ul>"},{"location":"skills/#api-reference","title":"API Reference","text":""},{"location":"skills/#skill","title":"<code>skill()</code>","text":"<pre><code>def skill(\n    name: str,\n    on: str | EventPattern,\n    *,\n    when: Callable[[Event], bool] | None = None,\n    prompt: str | None = None,\n    tools: list[Callable] | None = None,\n    context_enricher: Callable[[Event, dict], dict] | None = None,\n    priority: int = 0,\n) -&gt; Skill:\n    \"\"\"Create an event-triggered behavioral specialization.\n\n    Args:\n        name: Unique identifier for the skill\n        on: Event pattern to match (e.g., \"code.*\", \"error.network\")\n        when: Optional filter function for conditional activation\n        prompt: Prompt text injected into agent context\n        tools: Tools made available during skill activation\n        context_enricher: Function to add dynamic context\n        priority: Application order (higher = applied first)\n\n    Returns:\n        Skill instance ready for registration\n    \"\"\"\n</code></pre>"},{"location":"skills/#flowregister_skill","title":"<code>Flow.register_skill()</code>","text":"<pre><code>def register_skill(self, skill: Skill) -&gt; None:\n    \"\"\"Register a skill on the flow.\n\n    Args:\n        skill: Skill to register\n    \"\"\"\n</code></pre>"},{"location":"streaming/","title":"Streaming Reactions","text":"<p>Real-time token-by-token streaming from event-driven Flow executions.</p>"},{"location":"streaming/#overview","title":"Overview","text":"<p>Streaming reactions enable Flow to yield output progressively as agents generate tokens, rather than waiting for complete responses. This provides:</p> <ul> <li>Real-time feedback during long-running agent operations</li> <li>Better UX with progressive output display</li> <li>Lower perceived latency by showing immediate progress</li> <li>Agent tracking to see which agent is currently active</li> <li>Cancellation support for in-flight operations</li> </ul>"},{"location":"streaming/#quick-start","title":"Quick Start","text":"<pre><code>from cogent import Agent, Flow, react_to\nfrom cogent.models import ChatModel\n\n# Create agents with streaming-capable models\nresearcher = Agent(\n    name=\"researcher\",\n    model=ChatModel(model=\"gpt-4o\"),\n    system_prompt=\"You research topics thoroughly.\",\n)\n\nwriter = Agent(\n    name=\"writer\",\n    model=ChatModel(model=\"gpt-4o\"),\n    system_prompt=\"You write engaging content.\",\n)\n\n# Create flow\nflow = Flow()\nflow.register(researcher, [react_to(\"task.created\").emits(\"researcher.completed\")])\nflow.register(writer, [react_to(\"researcher.completed\")])\n\n# Stream execution - tokens arrive in real-time\nasync for chunk in flow.run_streaming(\"Research quantum computing\"):\n    print(f\"[{chunk.agent_name}] {chunk.content}\", end=\"\", flush=True)\n\n    if chunk.is_final:\n        print()  # Newline after agent completes\n</code></pre>"},{"location":"streaming/#core-concepts","title":"Core Concepts","text":""},{"location":"streaming/#streamchunk","title":"StreamChunk","text":"<p>Each streaming chunk contains full context about the flow execution:</p> <pre><code>@dataclass\nclass StreamChunk:\n    agent_name: str           # Which agent is streaming\n    event_id: str             # Event that triggered this agent\n    event_name: str           # Type of triggering event\n    content: str              # Token content\n    delta: str                # Incremental text (same as content)\n    is_final: bool           # Last chunk from this agent?\n    finish_reason: str | None # Why stopped (stop, length, error)\n    metadata: dict | None     # Additional context\n</code></pre> <p>Key Properties: - <code>agent_name</code> \u2014 Identifies which agent in the flow is currently streaming - <code>event_id</code> / <code>event_name</code> \u2014 Event context that triggered this agent - <code>is_final</code> \u2014 True when agent completes (useful for UI formatting) - <code>finish_reason</code> \u2014 \"stop\" (complete), \"length\" (max tokens), \"error\" (failed) - <code>metadata</code> \u2014 NEW in v1.14.2: Includes model metadata with token usage, model name, response ID, timestamp</p> <p>Streaming Metadata (v1.14.2+):</p> <p>All chat providers now return complete metadata during streaming:</p> <pre><code>async for chunk in flow.run_streaming(\"Analyze data\"):\n    print(chunk.content, end=\"\")\n\n    # Access LLM metadata from the underlying model\n    # Note: metadata is from the model's response, not the Flow chunk itself\n    if hasattr(chunk, 'raw_response') and chunk.raw_response:\n        msg_metadata = chunk.raw_response.metadata\n        if msg_metadata:\n            print(f\"\\nModel: {msg_metadata.model}\")\n            print(f\"Tokens: {msg_metadata.tokens}\")  # TokenUsage(prompt, completion, total)\n</code></pre> <p>For direct model streaming (without Flow), see Models - Streaming for full metadata details.</p>"},{"location":"streaming/#run_streaming-vs-run","title":"run_streaming() vs run()","text":"Feature <code>run()</code> <code>run_streaming()</code> Returns <code>FlowResult</code> <code>AsyncIterator[FlowStreamChunk]</code> Output Complete final output Progressive tokens Latency Wait for completion Immediate feedback Use Case Batch processing Interactive UX Agent Execution Parallel (up to max_concurrent) Sequential (preserves order) <p>When to use streaming: - Interactive applications (CLIs, web UIs, chatbots) - Long-running multi-agent workflows - Progress tracking and status updates - User experience is priority</p> <p>When to use regular run(): - Batch processing or automation - Final result is all that matters - Parallel agent execution preferred - Simpler code (no async iteration)</p>"},{"location":"streaming/#usage-patterns","title":"Usage Patterns","text":""},{"location":"streaming/#basic-streaming","title":"Basic Streaming","text":"<p>Simple single-agent streaming:</p> <pre><code>from cogent import Flow, react_to\n\nagent = Agent(name=\"assistant\", model=model)\nflow = Flow()\nflow.register(agent, [react_to(\"task.created\")])\n\n# Stream tokens as they arrive\nasync for chunk in flow.run_streaming(\"Explain streaming\"):\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"streaming/#multi-agent-streaming","title":"Multi-Agent Streaming","text":"<p>Track which agent is speaking:</p> <pre><code>current_agent = None\n\nasync for chunk in flow.run_streaming(\"Multi-step task\"):\n    # Detect agent transitions\n    if chunk.agent_name != current_agent:\n        if current_agent is not None:\n            print()  # Newline before new agent\n        print(f\"\\n[{chunk.agent_name}]:\", end=\" \")\n        current_agent = chunk.agent_name\n\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"streaming/#progress-indicators","title":"Progress Indicators","text":"<p>Show pipeline progress:</p> <pre><code>agents_completed = 0\ntotal_agents = 3\ncurrent_agent = None\n\nasync for chunk in flow.run_streaming(\"Build web scraper\"):\n    # Update progress when agent changes\n    if chunk.agent_name != current_agent:\n        if current_agent is not None:\n            agents_completed += 1\n\n        current_agent = chunk.agent_name\n        progress = f\"[{agents_completed + 1}/{total_agents}]\"\n        print(f\"\\n{progress} {chunk.agent_name}:\")\n        print(\"-\" * 50)\n\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"streaming/#conditional-routing","title":"Conditional Routing","text":"<p>Different agents stream based on event data:</p> <pre><code>flow = Flow()\n\n# Python expert handles Python questions\nflow.register(\n    python_expert,\n    [react_to(\"question.asked\").when(lambda e: \"python\" in str(e.data).lower())],\n)\n\n# JavaScript expert handles JS questions\nflow.register(\n    js_expert,\n    [react_to(\"question.asked\").when(lambda e: \"javascript\" in str(e.data).lower())],\n)\n\n# Route based on question content\nasync for chunk in flow.run_streaming(\n    \"How do I use async/await in Python?\",\n    initial_event=\"question.asked\",\n    initial_data={\"language\": \"python\"},\n):\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"streaming/#error-handling","title":"Error Handling","text":"<p>Gracefully handle errors during streaming:</p> <pre><code>try:\n    async for chunk in flow.run_streaming(\"Task with potential errors\"):\n        print(chunk.content, end=\"\", flush=True)\n\n        # Check for error metadata\n        if chunk.metadata and chunk.metadata.get(\"error\"):\n            print(f\"\\n\u26a0\ufe0f Error: {chunk.metadata['error']}\")\n\n        if chunk.is_final:\n            if chunk.finish_reason == \"error\":\n                print(\"\\n\u274c Stream ended with error\")\n            else:\n                print(\"\\n\u2705 Stream completed\")\n\nexcept Exception as e:\n    print(f\"\\n\u274c Exception during streaming: {e}\")\n</code></pre>"},{"location":"streaming/#architecture","title":"Architecture","text":""},{"location":"streaming/#how-it-works","title":"How It Works","text":"<ol> <li>Event Loop: <code>run_streaming()</code> processes events like regular <code>run()</code></li> <li>Agent Triggers: Events trigger matching agents (same matching logic)</li> <li>Streaming Execution: Agents execute with <code>stream=True</code> parameter</li> <li>Chunk Conversion: Agent <code>StreamChunk</code> \u2192 <code>FlowStreamChunk</code> with event context</li> <li>Sequential Flow: Agents execute sequentially to preserve output order</li> <li>Yield to Caller: Each chunk is yielded immediately as it arrives</li> </ol> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Flow.run_streaming(\"task\")                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502\n                  \u25bc\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502 Event Loop     \u2502 Processes events in rounds\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502\n                  \u25bc\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502 Match Agents   \u2502 Find agents triggered by event\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502\n                  \u25bc\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502 Execute Agent (stream=True)     \u2502 Sequential execution\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502\n                  \u25bc\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502 Agent Streams Tokens     \u2502 token1, token2, token3...\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502\n                  \u25bc\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502 Convert to FlowStreamChunk       \u2502 Add event context\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502\n                  \u25bc\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502 Yield to Caller          \u2502 Immediate feedback\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"streaming/#integration-with-agent-streaming","title":"Integration with Agent Streaming","text":"<p>Flow streaming leverages the existing agent streaming infrastructure:</p> <ul> <li>Agent.run(stream=True) \u2014 Returns <code>AsyncIterator[StreamChunk]</code></li> <li>StreamChunk \u2014 Token content from LLM provider</li> <li>FlowStreamChunk \u2014 Wraps StreamChunk with flow event context</li> </ul> <p>This means: - \u2705 No duplicate streaming logic - \u2705 All LLM providers supported (OpenAI, Anthropic, etc.) - \u2705 Agent-level streaming configuration respected - \u2705 Consistent behavior with imperative Flow.stream()</p>"},{"location":"streaming/#performance-considerations","title":"Performance Considerations","text":""},{"location":"streaming/#sequential-vs-parallel-execution","title":"Sequential vs Parallel Execution","text":"<p>Regular <code>run()</code> executes agents in parallel (up to <code>max_concurrent_agents</code>): <pre><code># Agents run in parallel\nresult = await flow.run(\"task\")  \n# \u26a1 Faster completion\n# \u274c No real-time feedback\n</code></pre></p> <p><code>run_streaming()</code> executes agents sequentially: <pre><code># Agents run one at a time\nasync for chunk in flow.run_streaming(\"task\"):\n    print(chunk.content)\n# \u2705 Real-time feedback\n# \u23f1\ufe0f Slower completion (sequential)\n</code></pre></p> <p>Trade-off: Streaming sacrifices parallel speedup for user experience.</p>"},{"location":"streaming/#memory-usage","title":"Memory Usage","text":"<p>Streaming is more memory-efficient than batching: - Regular <code>run()</code> accumulates full output in memory - <code>run_streaming()</code> yields chunks immediately, allowing garbage collection</p> <p>For very long outputs, streaming prevents memory buildup.</p>"},{"location":"streaming/#configuration","title":"Configuration","text":""},{"location":"streaming/#flow-configuration","title":"Flow Configuration","text":"<p>Streaming respects <code>FlowConfig</code> settings:</p> <pre><code>from cogent import FlowConfig\n\nconfig = FlowConfig(\n    max_rounds=100,          # Maximum event processing rounds\n    event_timeout=30.0,      # Timeout for waiting on events\n    stop_on_idle=True,       # Stop when no more events\n    stop_events=frozenset({\"flow.completed\"}),  # Events that end flow\n)\n\nflow = Flow(config=config)\n\n# Streaming obeys these limits\nasync for chunk in flow.run_streaming(\"task\"):\n    print(chunk.content)\n</code></pre>"},{"location":"streaming/#agent-configuration","title":"Agent Configuration","text":"<p>Enable streaming by default for specific agents:</p> <pre><code>agent = Agent(\n    name=\"assistant\",\n    model=model,\n    stream=True,  # Always stream (even in non-streaming flows)\n)\n\n# This agent will stream in both run() and run_streaming()\n</code></pre>"},{"location":"streaming/#examples","title":"Examples","text":"<p>See examples/basics/streaming.py for comprehensive demonstrations:</p> <ol> <li>Basic Streaming \u2014 Simple token-by-token display</li> <li>Multi-Agent Streaming \u2014 Track agent transitions</li> <li>Progress Indicators \u2014 Pipeline progress visualization</li> <li>Conditional Streaming \u2014 Event-driven routing</li> <li>Error Handling \u2014 Graceful failure recovery</li> </ol> <p>Run: <pre><code>uv run python examples/basics/streaming.py\n</code></pre></p>"},{"location":"streaming/#testing","title":"Testing","text":"<pre><code>import pytest\nfrom cogent import Flow\nfrom cogent.flow.streaming import FlowStreamChunk\n\n@pytest.mark.asyncio\nasync def test_streaming():\n    flow = Flow()\n    flow.register(agent, [react_to(\"task.created\")])\n\n    chunks = []\n    async for chunk in flow.run_streaming(\"Test\"):\n        assert isinstance(chunk, FlowStreamChunk)\n        assert chunk.agent_name == \"agent\"\n        chunks.append(chunk)\n\n    assert len(chunks) &gt; 1  # Multiple chunks received\n</code></pre> <p>See tests/test_streaming.py for full test suite.</p>"},{"location":"streaming/#api-reference","title":"API Reference","text":""},{"location":"streaming/#flowrun_streaming","title":"Flow.run_streaming()","text":"<pre><code>async def run_streaming(\n    self,\n    task: str,\n    *,\n    initial_event: str = \"task.created\",\n    initial_data: dict[str, Any] | None = None,\n    context: dict[str, Any] | None = None,\n) -&gt; AsyncIterator[FlowStreamChunk]:\n    \"\"\"\n    Execute event-driven flow with streaming output.\n\n    Args:\n        task: The task/prompt to execute\n        initial_event: Event type to emit at start\n        initial_data: Additional data for initial event\n        context: Shared context available to all agents\n\n    Yields:\n        FlowStreamChunk: Streaming chunks from agent executions\n\n    Example:\n        async for chunk in flow.run_streaming(\"Research topic\"):\n            print(f\"[{chunk.agent_name}] {chunk.content}\", end=\"\")\n    \"\"\"\n</code></pre>"},{"location":"streaming/#flowstreamchunk","title":"FlowStreamChunk","text":"<pre><code>@dataclass\nclass FlowStreamChunk:\n    \"\"\"Streaming chunk from event-driven agent execution.\"\"\"\n\n    agent_name: str\n    \"\"\"Name of the agent generating this chunk.\"\"\"\n\n    event_id: str\n    \"\"\"ID of the event that triggered this agent.\"\"\"\n\n    event_name: str\n    \"\"\"Name/type of the event that triggered this agent.\"\"\"\n\n    content: str\n    \"\"\"The text content of this streaming chunk.\"\"\"\n\n    delta: str\n    \"\"\"Incremental text added (same as content).\"\"\"\n\n    is_final: bool = False\n    \"\"\"True if this is the last chunk from this reaction.\"\"\"\n\n    metadata: dict[str, Any] | None = None\n    \"\"\"Additional context about the streaming execution.\"\"\"\n\n    finish_reason: str | None = None\n    \"\"\"Reason streaming stopped (stop, length, tool_calls, error, etc.).\"\"\"\n</code></pre>"},{"location":"streaming/#comparison-with-imperative-flow-streaming","title":"Comparison with Imperative Flow Streaming","text":"Feature Flow.run_streaming() Flow.stream() Paradigm Event-driven Imperative/Topology-based Chunk Type <code>FlowStreamChunk</code> Status updates Agent Coordination Event triggers Topology structure Event Context Full event metadata Limited Use Case Reactive orchestration Pipeline/Supervisor flows <p>Both support real-time streaming, but <code>run_streaming()</code> provides richer event context.</p>"},{"location":"streaming/#best-practices","title":"Best Practices","text":"<ol> <li>Show agent names \u2014 Help users understand which agent is active</li> <li>Handle transitions \u2014 Add newlines/separators when agents change</li> <li>Progress indicators \u2014 Show completion percentage for pipelines</li> <li>Error recovery \u2014 Check <code>finish_reason</code> and <code>metadata</code> for errors</li> <li>Cancellation \u2014 Use <code>asyncio.timeout()</code> to limit streaming duration</li> <li>Memory cleanup \u2014 Process chunks immediately, don't accumulate all</li> </ol> <pre><code># \u2705 Good - process immediately\nasync for chunk in flow.run_streaming(\"task\"):\n    await send_to_ui(chunk.content)\n\n# \u274c Bad - accumulates memory\nchunks = []\nasync for chunk in flow.run_streaming(\"task\"):\n    chunks.append(chunk)  # Memory grows\n</code></pre>"},{"location":"streaming/#limitations","title":"Limitations","text":"<ol> <li>Sequential execution \u2014 Agents run one at a time in streaming mode</li> <li>No checkpointing \u2014 Streaming flows don't support checkpoint saving (yet)</li> <li>Order dependency \u2014 Agent order determined by event trigger order</li> <li>Token overhead \u2014 More network roundtrips than batch mode</li> </ol>"},{"location":"streaming/#future-enhancements","title":"Future Enhancements","text":"<ul> <li>Parallel streaming with chunk interleaving</li> <li>Checkpoint support during streaming</li> <li>Stream pausing and resumption</li> <li>Custom chunk transformers/filters</li> <li>Streaming metrics and observability</li> </ul>"},{"location":"streaming/#see-also","title":"See Also","text":"<ul> <li>Flow Guide \u2014 Event-driven orchestration</li> <li>Agent Streaming \u2014 Agent-level streaming</li> <li>Transport \u2014 Distributed event transport</li> <li>Examples \u2014 Streaming example</li> </ul>"},{"location":"tool-composition/","title":"Tool Composition Patterns","text":"<p>Research basis: arXiv:2601.11327 - Tools are the primary value driver in agentic systems. Composition makes tools exponentially more powerful than isolated execution.</p> <p>Tool composition is the art of combining multiple tools to solve complex tasks. cogent provides flexible execution strategies that enable powerful composition patterns while maintaining simplicity.</p>"},{"location":"tool-composition/#quick-start","title":"Quick Start","text":"<pre><code>from cogent import Agent\nfrom cogent.executors import NativeExecutor, SequentialExecutor\nfrom cogent.tools import tool\n\n# Pattern 1: Parallel execution (default)\nagent = Agent(model=\"gpt-4o-mini\", tools=[tool1, tool2, tool3])\nresult = await agent.arun(\"Use all three tools\")  # Runs in parallel\n\n# Pattern 2: Sequential execution\nagent = Agent(model=\"gpt-4o-mini\", tools=[step1, step2, step3])\nexecutor = SequentialExecutor(agent)\nresult = await executor.execute(\"Do step1, then step2, then step3\")  # Runs sequentially\n</code></pre>"},{"location":"tool-composition/#execution-strategies","title":"Execution Strategies","text":"<p>cogent provides two built-in execution strategies:</p> Strategy Tool Execution Use Case NativeExecutor (default) Parallel when LLM batches toolsSequential when LLM calls one at a time Most tasks - flexible, performant SequentialExecutor Always sequential, even if LLM batches Strict ordering, debugging, stateful tools <p>Key insight: With NativeExecutor, the LLM decides whether to call tools in parallel (batched in one turn) or sequentially (one per turn). SequentialExecutor forces sequential execution regardless of what the LLM wants.</p>"},{"location":"tool-composition/#nativeexecutor-default","title":"NativeExecutor (Default)","text":"<p>Parallel tool execution when LLM requests multiple tools - Maximum performance.</p> <pre><code>from cogent import Agent\n\nagent = Agent(\n    model=\"gpt-4o-mini\",\n    tools=[fetch_weather, fetch_news, fetch_stock]\n)\n\n# If LLM requests multiple tools in one turn, they run concurrently\nresult = await agent.run(\"Get weather, news, and stock data\")\n</code></pre> <p>Execution behavior: - Parallel: When LLM requests multiple tools in one turn (e.g., <code>fetch_weather</code>, <code>fetch_news</code>, <code>fetch_stock</code> all at once) - Sequential: When LLM naturally calls tools one at a time across multiple turns - Flexible: LLM decides based on task requirements and prompt</p> <p>When to use: - Default choice for most tasks - Tools may or may not be independent - Want maximum performance when tools can run parallel - Trust LLM to determine execution order</p> <p>Performance: - 3 tools in one turn \u00d7 0.5s each = ~0.5s (parallel execution via asyncio.gather) - 3 tools across 3 turns \u00d7 0.5s each = ~1.5s + LLM overhead (sequential, LLM decides)</p> <p>Features: - Direct asyncio loop (no graph overhead) - Configurable concurrency limit (default: 20 concurrent tools) - LLM resilience with automatic retry for rate limits - Tool call batching (up to 50 tools per LLM turn)</p>"},{"location":"tool-composition/#sequentialexecutor","title":"SequentialExecutor","text":"<p>Forces sequential tool execution - One tool at a time, always.</p> <pre><code>from cogent import Agent\nfrom cogent.executors import SequentialExecutor\n\nagent = Agent(\n    model=\"gpt-4o-mini\",\n    tools=[search_company, analyze_sentiment, generate_report]\n)\n\nexecutor = SequentialExecutor(agent)\nresult = await executor.execute(\"\"\"\n    1. Search for company info\n    2. Analyze sentiment of what you found\n    3. Generate report using both pieces\n\"\"\")\n</code></pre> <p>Execution behavior: - Always sequential: Even if LLM requests multiple tools, they execute one at a time - Deterministic order: Tools execute in the order LLM requested them - Forced serialization: No parallelism, even for independent tools</p> <p>When to use: - Strict ordering required: When you MUST guarantee tools execute in a specific sequence - Debugging: To see exactly what order tools run in - Stateful tools: Tools that modify shared state (databases, files, etc.) - Resource constraints: External API has strict rate limits</p> <p>Difference from NativeExecutor: <pre><code># NativeExecutor - LLM requests 3 tools \u2192 all 3 run in parallel\n# SequentialExecutor - LLM requests 3 tools \u2192 they run 1, 2, 3 sequentially\n\n# If LLM naturally calls one tool per turn, both executors behave the same!\n</code></pre></p> <p>Trade-offs: - Slower (no parallelism) - More predictable execution order - Easier to debug - Better for stateful operations</p>"},{"location":"tool-composition/#composition-patterns","title":"Composition Patterns","text":""},{"location":"tool-composition/#pattern-1-parallel-execution","title":"Pattern 1: Parallel Execution","text":"<p>Independent tools that can run concurrently.</p> <pre><code>from cogent import Agent\nfrom cogent.tools import tool\n\n@tool\nasync def fetch_weather(city: str) -&gt; str:\n    \"\"\"Fetch weather for a city.\"\"\"\n    return f\"Weather in {city}: Sunny, 72\u00b0F\"\n\n@tool\nasync def fetch_news(topic: str) -&gt; str:\n    \"\"\"Fetch news about a topic.\"\"\"\n    return f\"Latest {topic} news: AI breakthroughs\"\n\n@tool\nasync def fetch_stock(symbol: str) -&gt; str:\n    \"\"\"Fetch stock price.\"\"\"\n    return f\"Stock {symbol}: $150.25 (+2.3%)\"\n\nagent = Agent(\n    model=\"gpt-4o-mini\",\n    tools=[fetch_weather, fetch_news, fetch_stock]\n)\n\n# NativeExecutor (default) runs these in parallel\nresult = await agent.arun(\"\"\"\n    Get me:\n    1. Weather in San Francisco\n    2. Latest AI news  \n    3. Stock price for AAPL\n\"\"\")\n</code></pre> <p>Result: All three tools execute concurrently (~0.5s total).</p>"},{"location":"tool-composition/#pattern-2-sequential-execution","title":"Pattern 2: Sequential Execution","text":"<p>Dependent tools that need ordered execution.</p> <pre><code>from cogent import Agent\nfrom cogent.executors import SequentialExecutor\nfrom cogent.tools import tool\n\n@tool\nasync def search_company(name: str) -&gt; str:\n    \"\"\"Search for company information.\"\"\"\n    return f\"{name} - Tech company, founded 2020, AI specialization\"\n\n@tool\nasync def analyze_sentiment(text: str) -&gt; str:\n    \"\"\"Analyze sentiment of text.\"\"\"\n    return \"Sentiment: POSITIVE (innovative, strong fundamentals)\"\n\n@tool\nasync def generate_report(company_info: str, sentiment: str) -&gt; str:\n    \"\"\"Generate investment report.\"\"\"\n    return f\"\"\"\nInvestment Report:\n- Company: {company_info}\n- Sentiment: {sentiment}\n- Recommendation: BUY\n\"\"\"\n\nagent = Agent(\n    name=\"sequential_demo\",\n    model=\"gpt-4o-mini\",\n    tools=[search_company, analyze_sentiment, generate_report]\n)\n\n# Option 1: Use SequentialExecutor to FORCE sequential execution\nexecutor = SequentialExecutor(agent)\nresult = await executor.execute(\"\"\"\n    Research ACME Corp:\n    1. Search for company info\n    2. Analyze sentiment\n    3. Generate report\n\n    Execute in strict order.\n\"\"\")\n\n# Option 2: Use NativeExecutor with clear prompting (LLM will likely call sequentially)\nresult = await agent.run(\"\"\"\n    Research ACME Corp step by step:\n    1. First search for company info\n    2. Then analyze the sentiment of what you found\n    3. Finally generate a report using both pieces\n\n    Do these one at a time, in order.\n\"\"\")\n</code></pre> <p>Result:  - SequentialExecutor: Tools execute sequentially: search \u2192 analyze \u2192 report (forced) - NativeExecutor with prompt: LLM likely calls tools one per turn sequentially (natural)</p> <p>When to use each: - SequentialExecutor: Need to guarantee sequential execution regardless of LLM behavior - NativeExecutor + prompt: Trust LLM to decide, get parallel execution when possible</p>"},{"location":"tool-composition/#pattern-3-conditional-logic","title":"Pattern 3: Conditional Logic","text":"<p>If/else branching based on tool results.</p> <pre><code>from cogent import Agent\nfrom cogent.tools import tool\n\n@tool\nasync def check_inventory(product: str) -&gt; str:\n    \"\"\"Check product inventory.\"\"\"\n    if \"laptop\" in product.lower():\n        return f\"{product}: OUT OF STOCK (restock: 2024-03-15)\"\n    return f\"{product}: IN STOCK (23 units available)\"\n\n@tool\nasync def backorder(product: str) -&gt; str:\n    \"\"\"Place item on backorder.\"\"\"\n    return f\"Backorder created for {product}\"\n\n@tool\nasync def complete_purchase(product: str) -&gt; str:\n    \"\"\"Complete purchase.\"\"\"\n    return f\"Purchase complete: {product} - Total: $999\"\n\nagent = Agent(\n    model=\"gpt-4o-mini\",\n    tools=[check_inventory, backorder, complete_purchase]\n)\n\nresult = await agent.arun(\"\"\"\n    Try to purchase a 'Gaming Laptop'.\n\n    First check inventory. Then:\n    - If IN STOCK: complete the purchase\n    - If OUT OF STOCK: place a backorder\n\"\"\")\n</code></pre> <p>Result: Agent uses if/else logic: - Calls <code>check_inventory</code> - Sees \"OUT OF STOCK\" - Calls <code>backorder</code> (not <code>complete_purchase</code>)</p>"},{"location":"tool-composition/#pattern-4-error-recovery","title":"Pattern 4: Error Recovery","text":"<p>Fallback chains for resilient execution.</p> <pre><code>from cogent import Agent\nfrom cogent.tools import tool\n\n@tool\nasync def premium_api(query: str) -&gt; str:\n    \"\"\"Premium API (might fail).\"\"\"\n    raise RuntimeError(\"Quota exceeded\")\n\n@tool\nasync def fallback_api(query: str) -&gt; str:\n    \"\"\"Fallback API (more reliable).\"\"\"\n    return f\"Fallback result for {query}\"\n\n@tool\nasync def cached_data(query: str) -&gt; str:\n    \"\"\"Cached data (always works).\"\"\"\n    return f\"Cached result for {query}\"\n\nagent = Agent(\n    model=\"gpt-4o-mini\",\n    tools=[premium_api, fallback_api, cached_data],\n    system_prompt=\"\"\"\nStrategy: Try premium_api first. If it fails, use fallback_api.\nIf that fails too, use cached_data as last resort.\n\"\"\"\n)\n\nresult = await agent.arun(\"Get data about quantum computing\")\n</code></pre> <p>Result: Agent tries premium_api \u2192 fails \u2192 uses fallback_api.</p>"},{"location":"tool-composition/#pattern-5-mixed-strategy","title":"Pattern 5: Mixed Strategy","text":"<p>Parallel data gathering + sequential processing.</p> <pre><code>from cogent import Agent\n\n# Phase 1: Parallel data gathering\n# Phase 2: Sequential report generation\n\nagent = Agent(\n    model=\"gpt-4o-mini\",\n    tools=[\n        fetch_weather,  # Parallel\n        fetch_news,     # Parallel\n        fetch_stock,    # Parallel\n        generate_report # Sequential (after gathering)\n    ]\n)\n\nresult = await agent.arun(\"\"\"\n    Create a market summary:\n\n    1. Gather data in parallel:\n       - Weather in NYC\n       - Latest tech news\n       - TSLA stock price\n\n    2. Generate summary report using all data\n\"\"\")\n</code></pre> <p>Result: - Phase 1: Three tools run in parallel (~0.5s) - Phase 2: Report generated with all results (~0.5s) - Total: ~1.0s (vs ~2.0s fully sequential)</p>"},{"location":"tool-composition/#pattern-6-real-world-composition","title":"Pattern 6: Real-World Composition","text":"<p>Combining capabilities with custom tools.</p> <pre><code>from cogent import Agent\nfrom cogent.capabilities import WebSearch\nfrom cogent.tools import tool\n\n@tool\nasync def summarize_findings(research: str) -&gt; str:\n    \"\"\"Create executive summary.\"\"\"\n    return f\"\"\"\nEXECUTIVE SUMMARY:\n- Research: {len(research)} chars\n- Key topics: AI, productivity\n- Recommendation: Implement AI tools\n\"\"\"\n\nagent = Agent(\n    model=\"gpt-4o-mini\",\n    capabilities=[WebSearch()],\n    tools=[summarize_findings]\n)\n\nresult = await agent.arun(\"\"\"\n    Research top 3 AI productivity tools of 2024.\n    Then create an executive summary.\n\"\"\")\n</code></pre> <p>Result: - WebSearch performs parallel searches - <code>summarize_findings</code> processes results - Clean separation of concerns</p>"},{"location":"tool-composition/#configuration","title":"Configuration","text":""},{"location":"tool-composition/#nativeexecutor-configuration","title":"NativeExecutor Configuration","text":"<pre><code>from cogent.executors import NativeExecutor\n\nexecutor = NativeExecutor(\n    agent,\n    max_tool_calls_per_turn=50,  # Max tools per LLM response\n    max_concurrent_tools=20,      # Max concurrent tool execution\n    resilience=True               # Enable LLM retry on rate limits\n)\n</code></pre> <p>Parameters: - <code>max_tool_calls_per_turn</code> (default: 50)   - Prevents runaway tool execution   - LLM can request up to 50 tools in one response</p> <ul> <li><code>max_concurrent_tools</code> (default: 20)</li> <li>Controls asyncio.Semaphore for tool execution</li> <li> <p>Prevents overwhelming external APIs</p> </li> <li> <p><code>resilience</code> (default: True)</p> </li> <li>Automatic retry for LLM rate limits</li> <li>Uses RetryPolicy.aggressive() by default</li> <li>Can be configured via agent's resilience_config</li> </ul>"},{"location":"tool-composition/#sequentialexecutor-configuration","title":"SequentialExecutor Configuration","text":"<pre><code>from cogent.executors import SequentialExecutor\n\nexecutor = SequentialExecutor(\n    agent,\n    max_tool_calls_per_turn=50,  # Same as NativeExecutor\n    max_concurrent_tools=1,       # Always 1 (sequential)\n    resilience=True               # LLM retry enabled\n)\n</code></pre> <p>SequentialExecutor inherits from NativeExecutor but forces tools to run one at a time.</p>"},{"location":"tool-composition/#factory-pattern","title":"Factory Pattern","text":"<p>Create executors using the factory:</p> <pre><code>from cogent.executors import create_executor, ExecutionStrategy\n\n# Parallel (default)\nexecutor = create_executor(agent, ExecutionStrategy.NATIVE)\n\n# Sequential\nexecutor = create_executor(agent, ExecutionStrategy.SEQUENTIAL)\n</code></pre>"},{"location":"tool-composition/#standalone-execution","title":"Standalone Execution","text":"<p>For quick tasks without creating an Agent:</p> <pre><code>from cogent.executors import run\nfrom cogent.tools import tool\n\n@tool\ndef search(query: str) -&gt; str:\n    \"\"\"Search the web.\"\"\"\n    return f\"Results for {query}\"\n\n# Standalone execution - no Agent needed\nresult = await run(\n    \"Search for Python tutorials\",\n    tools=[search],\n    model=\"gpt-4o-mini\",\n    max_iterations=25\n)\n</code></pre> <p>Benefits: - No Agent class overhead - Fastest execution path - Great for simple tasks</p>"},{"location":"tool-composition/#performance-tips","title":"Performance Tips","text":""},{"location":"tool-composition/#1-use-parallel-execution-by-default","title":"1. Use Parallel Execution by Default","text":"<pre><code># \u2705 GOOD - Default NativeExecutor\nagent = Agent(model=\"gpt-4o-mini\", tools=[...])\nresult = await agent.arun(task)\n</code></pre> <p>Why: Most tools are independent. Parallel execution is 2-3\u00d7 faster.</p>"},{"location":"tool-composition/#2-only-use-sequential-when-needed","title":"2. Only Use Sequential When Needed","text":"<pre><code># \u274c BAD - Unnecessary sequential\nexecutor = SequentialExecutor(agent)\nresult = await executor.execute(\"Get weather and news\")  # Independent!\n\n# \u2705 GOOD - Sequential only when dependent\nexecutor = SequentialExecutor(agent)\nresult = await executor.execute(\"Search, then analyze, then report\")\n</code></pre> <p>Why: Sequential execution is slower. Only use for dependencies.</p>"},{"location":"tool-composition/#3-configure-concurrency-limits","title":"3. Configure Concurrency Limits","text":"<pre><code># \u2705 GOOD - Tune for your use case\nexecutor = NativeExecutor(\n    agent,\n    max_concurrent_tools=10  # Lower if external API has rate limits\n)\n</code></pre> <p>Why: Prevents overwhelming external services with 20 concurrent calls.</p>"},{"location":"tool-composition/#4-use-resilience-for-production","title":"4. Use Resilience for Production","text":"<pre><code># \u2705 GOOD - Production config\nexecutor = NativeExecutor(\n    agent,\n    resilience=True  # Automatic LLM retry\n)\n</code></pre> <p>Why: Handles transient LLM failures (rate limits, timeouts) automatically.</p>"},{"location":"tool-composition/#error-handling","title":"Error Handling","text":""},{"location":"tool-composition/#llm-errors","title":"LLM Errors","text":"<p>Both executors include automatic retry for LLM failures:</p> <pre><code>from cogent.agent.resilience import RetryPolicy\n\n# Configure via agent's resilience_config\nagent = Agent(\n    model=\"gpt-4o-mini\",\n    tools=[...],\n    # Resilience applied at executor level\n)\n\nexecutor = NativeExecutor(agent, resilience=True)\n</code></pre> <p>Automatic retry for: - Rate limit errors (429) - Timeout errors - Transient API failures</p> <p>Retry policy: - Max 5 attempts - Exponential backoff: 1s, 2s, 4s, 8s, 16s - Total timeout: 120s</p>"},{"location":"tool-composition/#tool-errors","title":"Tool Errors","text":"<p>Tool errors are captured and returned as ToolMessage:</p> <pre><code>@tool\nasync def risky_operation() -&gt; str:\n    \"\"\"Might fail.\"\"\"\n    raise ValueError(\"Something went wrong\")\n\n# Agent receives error message and can retry or use fallback\n</code></pre> <p>Best practice: Let the agent handle tool errors via fallback patterns (see Pattern 4).</p>"},{"location":"tool-composition/#testing","title":"Testing","text":""},{"location":"tool-composition/#testing-parallel-execution","title":"Testing Parallel Execution","text":"<pre><code>import pytest\nfrom cogent import Agent\nfrom cogent.tools import tool\n\n@tool\nasync def mock_tool(value: str) -&gt; str:\n    return f\"Processed: {value}\"\n\n@pytest.mark.asyncio\nasync def test_parallel_execution():\n    agent = Agent(\n        model=\"gpt-4o-mini\",\n        tools=[mock_tool]\n    )\n\n    result = await agent.arun(\"Process A, B, and C in parallel\")\n\n    assert \"Processed: A\" in result\n    assert \"Processed: B\" in result\n    assert \"Processed: C\" in result\n</code></pre>"},{"location":"tool-composition/#testing-sequential-execution","title":"Testing Sequential Execution","text":"<pre><code>@pytest.mark.asyncio\nasync def test_sequential_execution():\n    call_order = []\n\n    @tool\n    async def step1() -&gt; str:\n        call_order.append(1)\n        return \"Step 1 done\"\n\n    @tool\n    async def step2() -&gt; str:\n        call_order.append(2)\n        return \"Step 2 done\"\n\n    agent = Agent(model=\"gpt-4o-mini\", tools=[step1, step2])\n    executor = SequentialExecutor(agent)\n\n    await executor.execute(\"Do step1 then step2\")\n\n    assert call_order == [1, 2]  # Sequential order preserved\n</code></pre>"},{"location":"tool-composition/#best-practices","title":"Best Practices","text":""},{"location":"tool-composition/#do","title":"\u2705 DO","text":"<ol> <li>Use parallel by default - NativeExecutor is faster for most tasks</li> <li>Use prompts to guide execution - Tell LLM when order matters (\"do step by step\")</li> <li>Configure concurrency - Tune <code>max_concurrent_tools</code> for external APIs</li> <li>Enable resilience - Use <code>resilience=True</code> in production</li> <li>Test composition - Unit test your tool chains</li> <li>Use fallbacks - Implement error recovery patterns</li> <li>Let LLM decide - Trust the model to determine parallel vs sequential when possible</li> </ol>"},{"location":"tool-composition/#dont","title":"\u274c DON'T","text":"<ol> <li>Don't use SequentialExecutor unnecessarily - Only when you need guaranteed sequential execution</li> <li>Don't hardcode execution order - Let the LLM decide when possible via prompts</li> <li>Don't ignore tool errors - Implement fallback patterns</li> <li>Don't skip concurrency limits - Can overwhelm external services</li> <li>Don't disable resilience in production - Transient failures are common</li> <li>Don't assume parallel = faster - If LLM calls tools one at a time, both executors are the same speed</li> </ol>"},{"location":"tool-composition/#examples","title":"Examples","text":"<p>See examples/advanced/tool_composition.py for complete runnable examples of all patterns.</p>"},{"location":"tool-composition/#further-reading","title":"Further Reading","text":"<ul> <li>Executors API Reference</li> <li>Tools Guide</li> <li>Error Handling</li> <li>arXiv:2601.11327 - Research on tool composition</li> </ul>"},{"location":"tools/","title":"Tools Module","text":"<p>The <code>cogent.tools</code> module provides tool creation, registration, and deferred execution for agent capabilities.</p>"},{"location":"tools/#overview","title":"Overview","text":"<p>Tools are functions that agents can call to interact with the world:</p> <pre><code>from cogent import Agent, tool\n\n@tool\ndef search(query: str) -&gt; str:\n    \"\"\"Search the web for information.\"\"\"\n    return f\"Results for: {query}\"\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    tools=[search],\n)\n\nresult = await agent.run(\"Search for AI news\")\n</code></pre>"},{"location":"tools/#creating-tools","title":"Creating Tools","text":""},{"location":"tools/#tool-decorator","title":"@tool Decorator","text":"<p>The simplest way to create tools:</p> <pre><code>from cogent import tool\n\n@tool\ndef calculate(expression: str) -&gt; float:\n    \"\"\"Evaluate a mathematical expression.\n\n    Args:\n        expression: Math expression to evaluate (e.g., \"2 + 2\")\n\n    Returns:\n        The result of the calculation\n    \"\"\"\n    return eval(expression)\n\n# Docstring becomes tool description\n# Type hints become parameter schema\n# Return type is included in description for LLM visibility\n</code></pre>"},{"location":"tools/#return-type-information","title":"Return Type Information","text":"<p>The <code>@tool</code> decorator automatically extracts return type information and includes it in the tool description. This helps the LLM understand what output to expect:</p> <pre><code>@tool\ndef get_weather(city: str) -&gt; dict[str, int]:\n    \"\"\"Get weather data for a city.\n\n    Args:\n        city: City name to query.\n\n    Returns:\n        A dictionary with temp, humidity, and wind_speed.\n    \"\"\"\n    return {\"temp\": 75, \"humidity\": 45, \"wind_speed\": 10}\n\n# LLM sees this description:\n# \"Get weather data for a city. Returns: dict[str, int] - A dictionary with temp, humidity, and wind_speed.\"\n\n# Access the return info directly:\nprint(get_weather.return_info)\n# Output: \"dict[str, int] - A dictionary with temp, humidity, and wind_speed.\"\n</code></pre> <p>What gets extracted:</p> Source Example Result Return type annotation <code>-&gt; str</code> <code>\"str\"</code> Generic types <code>-&gt; dict[str, int]</code> <code>\"dict[str, int]\"</code> Optional types <code>-&gt; str \\| None</code> <code>\"str \\| None\"</code> Docstring Returns section <code>Returns: The result.</code> <code>\"The result.\"</code> Both combined Type + docstring <code>\"dict[str, int] - A dictionary with...\"</code> <p>[!TIP] Always include a <code>Returns:</code> section in your docstrings to give the LLM context about the output format.</p>"},{"location":"tools/#with-options","title":"With Options","text":"<pre><code>@tool(\n    name=\"web_search\",           # Override function name\n    description=\"Search the web\",  # Override docstring\n    return_direct=True,          # Return result directly to user\n)\ndef search(query: str, max_results: int = 10) -&gt; str:\n    \"\"\"Search implementation.\"\"\"\n    return f\"Found {max_results} results for: {query}\"\n</code></pre>"},{"location":"tools/#async-tools","title":"Async Tools","text":"<pre><code>@tool\nasync def fetch_url(url: str) -&gt; str:\n    \"\"\"Fetch content from a URL.\"\"\"\n    async with httpx.AsyncClient() as client:\n        response = await client.get(url)\n        return response.text\n</code></pre>"},{"location":"tools/#tools-with-context","title":"Tools with Context","text":"<p>Access run context in tools:</p> <pre><code>from cogent import tool, RunContext\n\n@tool\ndef get_user_data(ctx: RunContext) -&gt; str:\n    \"\"\"Get data for the current user.\"\"\"\n    user_id = ctx.metadata.get(\"user_id\")\n    return f\"Data for user: {user_id}\"\n\n# Pass context when running agent\nresult = await agent.run(\n    \"Get my data\",\n    context=RunContext(metadata={\"user_id\": \"123\"}),\n)\n</code></pre>"},{"location":"tools/#tool-registry","title":"Tool Registry","text":"<p>Manage collections of tools:</p> <pre><code>from cogent.tools import ToolRegistry\n\nregistry = ToolRegistry()\n\n# Register tools\nregistry.register(search_tool)\nregistry.register(calculate_tool)\nregistry.register(fetch_tool)\n\n# Get all tools\nall_tools = registry.get_all()\n\n# Get by name\nsearch = registry.get(\"search\")\n\n# Check existence\nhas_search = registry.has(\"search\")\n\n# List names\nnames = registry.list_names()  # [\"search\", \"calculate\", \"fetch\"]\n</code></pre>"},{"location":"tools/#from-functions","title":"From Functions","text":"<pre><code>from cogent.tools import create_tool_from_function\n\ndef my_function(x: int, y: int) -&gt; int:\n    \"\"\"Add two numbers.\"\"\"\n    return x + y\n\ntool = create_tool_from_function(my_function)\nregistry.register(tool)\n</code></pre>"},{"location":"tools/#categories","title":"Categories","text":"<p>Organize tools by category:</p> <pre><code># Register with category\nregistry.register(search_tool, category=\"web\")\nregistry.register(fetch_tool, category=\"web\")\nregistry.register(calculate_tool, category=\"math\")\n\n# Get by category\nweb_tools = registry.get_by_category(\"web\")\n</code></pre>"},{"location":"tools/#deferred-tools","title":"Deferred Tools","text":"<p>For operations requiring human approval or async completion:</p>"},{"location":"tools/#deferredresult","title":"DeferredResult","text":"<pre><code>from cogent.tools import DeferredResult, DeferredStatus\n\n@tool\ndef send_email(to: str, subject: str, body: str) -&gt; DeferredResult:\n    \"\"\"Send an email (requires approval).\"\"\"\n    return DeferredResult(\n        status=DeferredStatus.PENDING,\n        message=\"Email pending approval\",\n        data={\"to\": to, \"subject\": subject},\n    )\n</code></pre>"},{"location":"tools/#deferredmanager","title":"DeferredManager","text":"<p>Manage deferred operations:</p> <pre><code>from cogent.tools import DeferredManager\n\nmanager = DeferredManager()\n\n# Register deferred result\nresult_id = await manager.register(deferred_result)\n\n# Check status\nstatus = await manager.status(result_id)\n\n# Approve/reject\nawait manager.approve(result_id, approver=\"admin\")\nawait manager.reject(result_id, reason=\"Not allowed\")\n\n# Get result after approval\nfinal_result = await manager.get_result(result_id)\n</code></pre>"},{"location":"tools/#deferredstatus","title":"DeferredStatus","text":"<pre><code>from cogent.tools import DeferredStatus\n\nDeferredStatus.PENDING    # Waiting for action\nDeferredStatus.APPROVED   # Approved, ready to execute\nDeferredStatus.REJECTED   # Rejected\nDeferredStatus.COMPLETED  # Execution completed\nDeferredStatus.FAILED     # Execution failed\n</code></pre>"},{"location":"tools/#deferredwaiter","title":"DeferredWaiter","text":"<p>Wait for deferred completion:</p> <pre><code>from cogent.tools import DeferredWaiter\n\nwaiter = DeferredWaiter(manager)\n\n# Wait for result (with timeout)\nresult = await waiter.wait(result_id, timeout=300)\n\n# Wait for multiple\nresults = await waiter.wait_all([id1, id2, id3])\n</code></pre>"},{"location":"tools/#deferredretry","title":"DeferredRetry","text":"<p>Auto-retry failed deferred operations:</p> <pre><code>from cogent.tools import DeferredRetry\n\nretry = DeferredRetry(\n    manager=manager,\n    max_attempts=3,\n    backoff=\"exponential\",\n)\n\nresult = await retry.execute(result_id)\n</code></pre>"},{"location":"tools/#is_deferred-helper","title":"is_deferred Helper","text":"<pre><code>from cogent.tools import is_deferred\n\nresult = tool_call()\n\nif is_deferred(result):\n    # Handle deferred result\n    result_id = await manager.register(result)\nelse:\n    # Handle immediate result\n    print(result)\n</code></pre>"},{"location":"tools/#tool-schemas","title":"Tool Schemas","text":"<p>Tools automatically generate JSON schemas from type hints:</p> <pre><code>@tool\ndef create_event(\n    title: str,\n    date: str,\n    attendees: list[str] | None = None,\n    priority: int = 1,\n) -&gt; str:\n    \"\"\"Create a calendar event.\"\"\"\n    ...\n\n# Generated schema:\n# {\n#     \"name\": \"create_event\",\n#     \"description\": \"Create a calendar event.\",\n#     \"parameters\": {\n#         \"type\": \"object\",\n#         \"properties\": {\n#             \"title\": {\"type\": \"string\"},\n#             \"date\": {\"type\": \"string\"},\n#             \"attendees\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n#             \"priority\": {\"type\": \"integer\", \"default\": 1}\n#         },\n#         \"required\": [\"title\", \"date\"]\n#     }\n# }\n</code></pre>"},{"location":"tools/#complex-types","title":"Complex Types","text":""},{"location":"tools/#pydantic-models","title":"Pydantic Models","text":"<pre><code>from pydantic import BaseModel\n\nclass EmailRequest(BaseModel):\n    to: str\n    subject: str\n    body: str\n    cc: list[str] = []\n\n@tool\ndef send_email(request: EmailRequest) -&gt; str:\n    \"\"\"Send an email.\"\"\"\n    return f\"Sent to {request.to}\"\n</code></pre>"},{"location":"tools/#enum-parameters","title":"Enum Parameters","text":"<pre><code>from enum import Enum\n\nclass Priority(Enum):\n    LOW = \"low\"\n    MEDIUM = \"medium\"\n    HIGH = \"high\"\n\n@tool\ndef create_task(title: str, priority: Priority = Priority.MEDIUM) -&gt; str:\n    \"\"\"Create a task with priority.\"\"\"\n    return f\"Created: {title} ({priority.value})\"\n</code></pre>"},{"location":"tools/#agent-tool-integration","title":"Agent Tool Integration","text":""},{"location":"tools/#adding-tools-to-agents","title":"Adding Tools to Agents","text":"<pre><code>from cogent import Agent\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    tools=[search, calculate, fetch],  # List of tools\n)\n</code></pre>"},{"location":"tools/#dynamic-tool-access","title":"Dynamic Tool Access","text":"<pre><code># Agent can see which tools are available\nfor tool in agent.tools:\n    print(f\"{tool.name}: {tool.description}\")\n</code></pre>"},{"location":"tools/#tool-results-in-responses","title":"Tool Results in Responses","text":"<p>When agents use tools, results are available in the response:</p> <pre><code>result = await agent.run(\"Calculate 2 + 2\")\n\n# Access tool calls made\nfor call in result.tool_calls:\n    print(f\"Tool: {call.name}\")\n    print(f\"Args: {call.args}\")\n    print(f\"Result: {call.result}\")\n</code></pre>"},{"location":"tools/#error-handling","title":"Error Handling","text":""},{"location":"tools/#tool-errors","title":"Tool Errors","text":"<pre><code>@tool\ndef risky_operation(data: str) -&gt; str:\n    \"\"\"Perform risky operation.\"\"\"\n    if not data:\n        raise ValueError(\"Data required\")\n    return process(data)\n\n# Errors are caught and returned to agent for handling\n</code></pre>"},{"location":"tools/#graceful-degradation","title":"Graceful Degradation","text":"<pre><code>@tool\ndef search_with_fallback(query: str) -&gt; str:\n    \"\"\"Search with fallback.\"\"\"\n    try:\n        return primary_search(query)\n    except Exception:\n        return fallback_search(query)\n</code></pre>"},{"location":"tools/#api-reference","title":"API Reference","text":""},{"location":"tools/#decorators","title":"Decorators","text":"Decorator Description <code>@tool</code> Create a tool from a function"},{"location":"tools/#core-classes","title":"Core Classes","text":"Class Description <code>ToolRegistry</code> Manage tool collections <code>BaseTool</code> Base class for tools"},{"location":"tools/#deferred-execution","title":"Deferred Execution","text":"Class Description <code>DeferredResult</code> Result requiring async completion <code>DeferredStatus</code> Status of deferred operation <code>DeferredManager</code> Manage deferred results <code>DeferredWaiter</code> Wait for deferred completion <code>DeferredRetry</code> Retry failed operations"},{"location":"tools/#utility-functions","title":"Utility Functions","text":"Function Description <code>create_tool_from_function(fn)</code> Create tool from function <code>is_deferred(result)</code> Check if result is deferred"},{"location":"transport/","title":"Distributed Transport","text":"<p>Enable cross-process agent communication with pluggable transport backends.</p>"},{"location":"transport/#overview","title":"Overview","text":"<p>The Transport system allows agents to communicate across process boundaries using message brokers like Redis. This enables:</p> <ul> <li>Multi-process deployments \u2014 Scale agents horizontally across multiple servers</li> <li>Distributed workflows \u2014 Coordinate agents running in different processes</li> <li>Loose coupling \u2014 Agents communicate via events, not direct calls</li> <li>Flexibility \u2014 Swap transport backends (local, Redis, custom) without changing agent code</li> </ul>"},{"location":"transport/#quick-start","title":"Quick Start","text":""},{"location":"transport/#local-transport-in-memory","title":"Local Transport (In-Memory)","text":"<p>For single-process applications or testing:</p> <pre><code>from cogent.events import Event, EventBus\nfrom cogent.events.transport import LocalTransport\n\n# Create local transport\ntransport = LocalTransport()\nawait transport.connect()\n\n# Use with EventBus\nbus = EventBus(transport=transport)\n\n# Subscribe to events\nasync def handler(event: Event):\n    print(f\"Received: {event.name} - {event.data}\")\n\nawait transport.subscribe(\"task.*\", handler)\n\n# Publish events\nawait transport.publish(Event(name=\"task.created\", data={\"id\": \"123\"}))\n</code></pre>"},{"location":"transport/#redis-transport-distributed","title":"Redis Transport (Distributed)","text":"<p>For production multi-process deployments:</p> <p>Install Redis support: <pre><code>uv add cogent[redis]\n# or\nuv add redis&gt;=5.0.0\n</code></pre></p> <p>Use RedisTransport: <pre><code>from cogent.events.transport import RedisTransport\n\ntransport = RedisTransport(redis_url=\"redis://localhost:6379/0\")\nawait transport.connect()\n\n# Same API as LocalTransport\nawait transport.subscribe(\"agent.*.response\", handler)\nawait transport.publish(Event(name=\"agent.task.response\", data={\"result\": \"done\"}))\n\nawait transport.disconnect()\n</code></pre></p>"},{"location":"transport/#transport-protocol","title":"Transport Protocol","text":"<p>All transports implement the <code>Transport</code> protocol:</p> <pre><code>from typing import Protocol\n\nclass Transport(Protocol):\n    \"\"\"Abstract event transport for distributed systems.\"\"\"\n\n    async def connect(self) -&gt; None:\n        \"\"\"Establish connection to transport backend.\"\"\"\n        ...\n\n    async def disconnect(self) -&gt; None:\n        \"\"\"Close connection and cleanup resources.\"\"\"\n        ...\n\n    async def publish(self, event: Event) -&gt; None:\n        \"\"\"Publish event to all matching subscribers.\"\"\"\n        ...\n\n    async def subscribe(self, pattern: str, handler: EventHandler) -&gt; str:\n        \"\"\"Subscribe to events matching pattern. Returns subscription ID.\"\"\"\n        ...\n\n    async def unsubscribe(self, subscription_id: str) -&gt; bool:\n        \"\"\"Unsubscribe from events. Returns True if successful.\"\"\"\n        ...\n</code></pre>"},{"location":"transport/#pattern-matching","title":"Pattern Matching","text":"<p>Transports support wildcard patterns for flexible subscriptions:</p> Pattern Matches Examples <code>task.created</code> Exact match <code>task.created</code> only <code>task.*</code> Single level wildcard <code>task.created</code>, <code>task.updated</code>, <code>task.deleted</code> <code>agent.**</code> Multi-level wildcard <code>agent.task.created</code>, <code>agent.task.subtask.done</code> <code>**</code> Match all events Any event <p>Examples:</p> <pre><code># Exact match\nawait transport.subscribe(\"user.registered\", handle_registration)\n\n# Single level wildcard\nawait transport.subscribe(\"task.*\", handle_task_events)  # task.created, task.updated\n\n# Multi-level wildcard\nawait transport.subscribe(\"agent.**\", handle_all_agent_events)  # agent.*, agent.task.*, etc.\n\n# Match all\nawait transport.subscribe(\"**\", log_all_events)\n</code></pre>"},{"location":"transport/#built-in-transports","title":"Built-in Transports","text":""},{"location":"transport/#localtransport","title":"LocalTransport","text":"<p>Use case: Single-process applications, testing, development</p> <p>Features: - In-memory <code>asyncio.Queue</code>-based - No external dependencies - Fast, low latency - Automatic cleanup on disconnect</p> <p>Configuration: <pre><code>transport = LocalTransport()\n</code></pre></p> <p>Limitations: - \u274c Events don't cross process boundaries - \u274c No persistence (events lost on restart)</p>"},{"location":"transport/#redistransport","title":"RedisTransport","text":"<p>Use case: Production distributed systems, multi-process deployments</p> <p>Features: - Redis Pub/Sub for event distribution - Supports multiple processes/servers - Built-in pattern matching via Redis channels - Connection pooling</p> <p>Configuration: <pre><code>transport = RedisTransport(\n    redis_url=\"redis://localhost:6379/0\",\n    channel_prefix=\"cogent\"  # Optional namespace\n)\n</code></pre></p> <p>Requirements: - Redis server 5.0+ running - <code>redis</code> Python package installed</p> <p>Limitations: - \u26a0\ufe0f Redis Pub/Sub is at-most-once delivery (no persistence) - \u26a0\ufe0f Requires Redis server availability - \u26a0\ufe0f Network latency vs LocalTransport</p> <p>Production Setup:</p> <pre><code># Install Redis\nbrew install redis  # macOS\nsudo apt-get install redis-server  # Ubuntu\n\n# Start Redis\nredis-server\n\n# Install Python package\nuv add cogent[redis]\n</code></pre>"},{"location":"transport/#integration-with-eventbus","title":"Integration with EventBus","text":"<p>Use transport with <code>EventBus</code> for automatic event routing:</p> <pre><code>from cogent.events.transport import RedisTransport\nfrom cogent.events import EventBus\n\n# Create transport\ntransport = RedisTransport(redis_url=\"redis://localhost:6379\")\nawait transport.connect()\n\n# Pass to EventBus\nbus = EventBus(transport=transport)\n\n# Subscribe via bus\nbus.subscribe(\"task.*\", handle_task_events)\n\n# Publish via bus (delegates to transport)\nawait bus.publish(Event(name=\"task.created\", data={\"id\": \"123\"}))\n</code></pre> <p>Benefits: - Events automatically routed across processes - Same API as local-only EventBus - Easy to swap transports (local \u2192 Redis) without code changes</p>"},{"location":"transport/#multi-process-coordination","title":"Multi-Process Coordination","text":"<p>Example: Coordinating agents across 2 processes</p> <p>Process 1 (Worker Agent): <pre><code># worker.py\nfrom cogent import Agent\nfrom cogent.events.transport import RedisTransport\nfrom cogent.events import Event\n\ntransport = RedisTransport(redis_url=\"redis://localhost:6379\")\nawait transport.connect()\n\nworker = Agent(\n    name=\"worker\",\n    role=\"Process tasks from queue\",\n    model=\"gpt-4\",\n)\n\n# Subscribe to task events\nasync def handle_task(event: Event):\n    result = await worker.run(f\"Process task: {event.data}\")\n    await transport.publish(Event(\n        name=\"task.completed\",\n        data={\"task_id\": event.data[\"id\"], \"result\": result.response}\n    ))\n\nawait transport.subscribe(\"task.created\", handle_task)\n</code></pre></p> <p>Process 2 (Coordinator): <pre><code># coordinator.py\nfrom cogent.events.transport import RedisTransport\nfrom cogent.events import Event\n\ntransport = RedisTransport(redis_url=\"redis://localhost:6379\")\nawait transport.connect()\n\n# Create task\nawait transport.publish(Event(\n    name=\"task.created\",\n    data={\"id\": \"123\", \"description\": \"Analyze data\"}\n))\n\n# Wait for completion\nasync def handle_completion(event: Event):\n    print(f\"Task {event.data['task_id']} done: {event.data['result']}\")\n\nawait transport.subscribe(\"task.completed\", handle_completion)\n</code></pre></p> <p>Run both processes: <pre><code># Terminal 1\npython worker.py\n\n# Terminal 2\npython coordinator.py\n</code></pre></p>"},{"location":"transport/#error-handling","title":"Error Handling","text":"<p>Transports raise specific exceptions:</p> <pre><code>from cogent.events.transport import (\n    TransportError,\n    ConnectionError,\n    PublishError,\n)\n\ntry:\n    await transport.connect()\nexcept ConnectionError as e:\n    logger.error(f\"Failed to connect: {e}\")\n\ntry:\n    await transport.publish(event)\nexcept PublishError as e:\n    logger.error(f\"Failed to publish: {e}\")\nexcept TransportError as e:\n    logger.error(f\"Transport error: {e}\")\n</code></pre> <p>Best Practices: - Always wrap <code>connect()</code> in try/except - Handle <code>ConnectionError</code> on startup - Retry publish on <code>PublishError</code> with exponential backoff - Call <code>disconnect()</code> in finally blocks or use context managers</p>"},{"location":"transport/#testing","title":"Testing","text":"<p>Use LocalTransport for tests (fast, no external dependencies):</p> <pre><code>import pytest\nfrom cogent.events import Event\nfrom cogent.events.transport import LocalTransport\n\n@pytest.mark.asyncio\nasync def test_event_delivery():\n    transport = LocalTransport()\n    await transport.connect()\n\n    received = []\n\n    async def handler(event: Event):\n        received.append(event)\n\n    await transport.subscribe(\"test.*\", handler)\n    await transport.publish(Event(name=\"test.event\", data={\"value\": 123}))\n\n    await asyncio.sleep(0.1)  # Let dispatcher process\n\n    assert len(received) == 1\n    assert received[0].name == \"test.event\"\n    assert received[0].data[\"value\"] == 123\n\n    await transport.disconnect()\n</code></pre> <p>Integration tests with Redis (requires Redis server):</p> <pre><code>@pytest.mark.skipif(not REDIS_AVAILABLE, reason=\"Redis not available\")\n@pytest.mark.asyncio\nasync def test_redis_transport():\n    transport = RedisTransport(redis_url=\"redis://localhost:6379/15\")\n    await transport.connect()\n\n    # Test event flow\n    ...\n\n    await transport.disconnect()\n</code></pre>"},{"location":"transport/#advanced-usage","title":"Advanced Usage","text":""},{"location":"transport/#multiple-subscribers","title":"Multiple Subscribers","text":"<p>Multiple handlers can subscribe to the same pattern:</p> <pre><code>async def logger(event: Event):\n    print(f\"LOG: {event.name}\")\n\nasync def metrics(event: Event):\n    print(f\"METRIC: {event.name}\")\n\nawait transport.subscribe(\"task.*\", logger)\nawait transport.subscribe(\"task.*\", metrics)\n\n# Both handlers receive the event\nawait transport.publish(Event(name=\"task.created\", data={}))\n</code></pre>"},{"location":"transport/#subscription-management","title":"Subscription Management","text":"<p>Unsubscribe when no longer needed:</p> <pre><code># Subscribe\nsub_id = await transport.subscribe(\"task.*\", handler)\n\n# Later, unsubscribe\nsuccess = await transport.unsubscribe(sub_id)\nassert success is True\n</code></pre>"},{"location":"transport/#context-managers-future-enhancement","title":"Context Managers (Future Enhancement)","text":"<pre><code># Planned for future release\nasync with RedisTransport(redis_url=\"...\") as transport:\n    await transport.subscribe(\"task.*\", handler)\n    # Automatically disconnects on exit\n</code></pre>"},{"location":"transport/#custom-transport-backends","title":"Custom Transport Backends","text":"<p>Implement the <code>Transport</code> protocol for custom backends:</p> <pre><code>class NATSTransport:\n    \"\"\"NATS JetStream transport (example).\"\"\"\n\n    async def connect(self) -&gt; None:\n        self._client = await nats.connect(self._url)\n        self._js = self._client.jetstream()\n\n    async def publish(self, event: Event) -&gt; None:\n        await self._js.publish(\n            subject=event.name,\n            payload=event.model_dump_json().encode(),\n        )\n\n    async def subscribe(self, pattern: str, handler: EventHandler) -&gt; str:\n        # Convert pattern to NATS subject\n        subject = pattern.replace(\"**\", \"&gt;\").replace(\"*\", \"*\")\n\n        async def nats_handler(msg):\n            event = Event.model_validate_json(msg.data)\n            await handler(event)\n\n        sub = await self._js.subscribe(subject, cb=nats_handler)\n        return str(id(sub))\n\n    # ... implement rest of protocol\n</code></pre>"},{"location":"transport/#comparison","title":"Comparison","text":"Feature LocalTransport RedisTransport Custom Single Process \u2705 Yes \u26a0\ufe0f Works but overkill Depends Multi Process \u274c No \u2705 Yes Depends Persistence \u274c No \u274c No (Pub/Sub) Depends Dependencies \u2705 None \u26a0\ufe0f Redis server + package Depends Latency \u26a0\ufe0f Lowest \u26a0\ufe0f Network overhead Depends Pattern Matching \u2705 Yes \u2705 Yes Implement Best For Development, testing Production distributed Special needs"},{"location":"transport/#next-steps","title":"Next Steps","text":"<ul> <li>See examples/reactive/distributed_transport_demo.py for runnable demos</li> <li>Explore Flow for using transport in event-driven workflows</li> <li>Check EventBus documentation for event routing patterns</li> <li>Consider Checkpointing for fault-tolerant distributed flows</li> </ul>"},{"location":"transport/#future-enhancements","title":"Future Enhancements","text":"<p>Planned features: - NATS transport \u2014 JetStream for persistent event streams - Context managers \u2014 <code>async with transport:</code> syntax - Backpressure \u2014 Rate limiting and flow control - Dead Letter Queue \u2014 Handle failed event delivery - Metrics \u2014 Built-in observability for transport health - Encryption \u2014 TLS/mTLS for secure event transmission</p>"},{"location":"vectorstore/","title":"VectorStore Module","text":"<p>The <code>cogent.vectorstore</code> module provides semantic search and vector storage for RAG applications.</p>"},{"location":"vectorstore/#overview","title":"Overview","text":"<p>VectorStore provides: - Document storage with embeddings - Similarity search - Multiple backend support (InMemory, FAISS, Chroma, Qdrant, pgvector) - Multiple embedding providers (OpenAI, Ollama, etc.)</p> <pre><code>from cogent.vectorstore import VectorStore\n\n# Simple: in-memory with OpenAI embeddings\nstore = VectorStore()\nawait store.add_texts([\"Python is great\", \"JavaScript is popular\"])\nresults = await store.search(\"programming language\", k=2)\n</code></pre>"},{"location":"vectorstore/#quick-start","title":"Quick Start","text":""},{"location":"vectorstore/#basic-usage","title":"Basic Usage","text":"<pre><code>from cogent.vectorstore import VectorStore\n\n# Create store (uses OpenAI embeddings by default)\nstore = VectorStore()\n\n# Add texts\nawait store.add_texts([\n    \"Python is a programming language\",\n    \"Machine learning uses algorithms\",\n    \"Neural networks learn from data\",\n])\n\n# Search\nresults = await store.search(\"AI and deep learning\", k=2)\nfor r in results:\n    print(f\"{r.score:.3f}: {r.text[:50]}\")\n</code></pre>"},{"location":"vectorstore/#with-documents","title":"With Documents","text":"<pre><code>from cogent.vectorstore import VectorStore, Document, DocumentMetadata\n\n# Create documents with structured metadata\ndocs = [\n    Document(\n        text=\"Python guide\", \n        metadata=DocumentMetadata(\n            source=\"tutorial.md\",\n            source_type=\"markdown\",\n            custom={\"type\": \"tutorial\", \"lang\": \"python\"}\n        )\n    ),\n    Document(\n        text=\"JavaScript intro\", \n        metadata=DocumentMetadata(\n            source=\"intro.md\",\n            custom={\"type\": \"tutorial\", \"lang\": \"js\"}\n        )\n    ),\n    Document(\n        text=\"ML basics\", \n        metadata=DocumentMetadata(\n            source=\"guide.pdf\",\n            source_type=\"pdf\",\n            page=1,\n            custom={\"type\": \"guide\", \"topic\": \"ml\"}\n        )\n    ),\n]\n\nstore = VectorStore()\nawait store.add_documents(docs)\n\n# Access structured metadata in results\nresults = await store.search(\"programming tutorial\", k=5)\nfor r in results:\n    print(f\"Source: {r.document.source}\")  # Convenience property\n    print(f\"Type: {r.document.metadata.source_type}\")\n    print(f\"Custom: {r.document.metadata.custom.get('type')}\")\n\n# Search with metadata filter (use custom dict for app-specific filters)\nresults = await store.search(\n    \"programming tutorial\",\n    k=5,\n    filter={\"custom\": {\"type\": \"tutorial\"}},\n)\n</code></pre> <p>Note on Metadata Filtering:</p> <p>VectorStore backends have different filtering capabilities:</p> <ul> <li>InMemory/FAISS: Full dict matching, nested filtering supported</li> <li>Chroma: Only supports primitive types (str, int, float, bool) - nested dicts like <code>custom</code> are stringified</li> <li>Qdrant/PGVector: Full JSON filtering supported</li> </ul> <p>For maximum compatibility, use standard DocumentMetadata fields (source, page, etc.) or keep custom dict values simple.</p>"},{"location":"vectorstore/#embedding-providers","title":"Embedding Providers","text":"<p>All embedding providers support a standardized API:</p> <p>Primary Methods (with metadata): - <code>embed(texts)</code> / <code>aembed(texts)</code> \u2192 Returns <code>EmbeddingResult</code> with metadata - <code>embed_one(text)</code> / <code>aembed_one(text)</code> \u2192 Returns vector only</p> <p>VectorStore Protocol Methods (async, no metadata): - <code>embed_texts(texts)</code> \u2192 Returns <code>list[list[float]]</code>  - <code>embed_query(text)</code> \u2192 Returns <code>list[float]</code></p>"},{"location":"vectorstore/#openai-default","title":"OpenAI (Default)","text":"<pre><code>from cogent.vectorstore import VectorStore\nfrom cogent.models import OpenAIEmbedding\n\n# Uses text-embedding-3-small by default\nstore = VectorStore()\n\n# Or specify model\nembeddings = OpenAIEmbedding(model=\"text-embedding-3-large\")\nstore = VectorStore(embeddings=embeddings)\n\n# Use embeddings directly\nresult = await embeddings.embed([\"Hello\", \"World\"])\nprint(result.metadata.tokens)  # Track token usage\n</code></pre>"},{"location":"vectorstore/#ollama-local","title":"Ollama (Local)","text":"<pre><code>from cogent.models import OllamaEmbedding\n\nembeddings = OllamaEmbedding(\n    model=\"nomic-embed-text\",\n    host=\"http://localhost:11434\",\n)\nstore = VectorStore(embeddings=embeddings)\n</code></pre>"},{"location":"vectorstore/#other-providers","title":"Other Providers","text":"<pre><code>from cogent.models import (\n    GeminiEmbedding,\n    CohereEmbedding,\n    MistralEmbedding,\n    AzureOpenAIEmbedding,\n    CloudflareEmbedding,\n    CustomEmbedding,\n)\n\n# Gemini\nembeddings = GeminiEmbedding(model=\"text-embedding-004\")\n\n# Cohere\nembeddings = CohereEmbedding(model=\"embed-english-v3.0\")\n\n# Mistral\nembeddings = MistralEmbedding(model=\"mistral-embed\")\n\n# Azure OpenAI\nfrom cogent.models.azure import AzureEntraAuth\nembeddings = AzureOpenAIEmbedding(\n    azure_endpoint=\"https://your-resource.openai.azure.com\",\n    deployment=\"text-embedding-ada-002\",\n)\n\n# Cloudflare Workers AI\nembeddings = CloudflareEmbedding(\n    model=\"@cf/baai/bge-base-en-v1.5\",\n    account_id=\"your-account-id\",\n)\n\n# Custom OpenAI-compatible\nembeddings = CustomEmbedding(\n    base_url=\"http://localhost:8000/v1\",\n    model=\"custom-model\",\n)\n</code></pre>"},{"location":"vectorstore/#mock-testing","title":"Mock (Testing)","text":"<pre><code>from cogent.models import MockEmbedding\n\nembeddings = MockEmbedding(dimensions=384)\nstore = VectorStore(embeddings=embeddings)\n\n# Fast, deterministic embeddings for tests\n</code></pre>"},{"location":"vectorstore/#storage-backends","title":"Storage Backends","text":""},{"location":"vectorstore/#inmemory-default","title":"InMemory (Default)","text":"<p>NumPy-based, no external dependencies:</p> <pre><code>from cogent.vectorstore import VectorStore\nfrom cogent.vectorstore.backends.inmemory import SimilarityMetric\n\nstore = VectorStore(\n    metric=SimilarityMetric.COSINE,  # or DOT_PRODUCT, EUCLIDEAN\n)\n</code></pre>"},{"location":"vectorstore/#faiss","title":"FAISS","text":"<p>Large-scale similarity search:</p> <pre><code># pip install faiss-cpu (or faiss-gpu)\nfrom cogent.vectorstore.backends import FAISSBackend\n\nbackend = FAISSBackend(\n    index_type=\"IVF\",      # or \"Flat\", \"HNSW\"\n    nlist=100,             # Number of clusters\n    nprobe=10,             # Search clusters\n)\nstore = VectorStore(backend=backend)\n</code></pre>"},{"location":"vectorstore/#chroma","title":"Chroma","text":"<p>Persistent vector database:</p> <pre><code># pip install chromadb\nfrom cogent.vectorstore.backends import ChromaBackend\n\nbackend = ChromaBackend(\n    collection_name=\"my_docs\",\n    persist_directory=\"./chroma_db\",\n)\nstore = VectorStore(backend=backend)\n</code></pre>"},{"location":"vectorstore/#qdrant","title":"Qdrant","text":"<p>Production-ready vector database:</p> <pre><code># pip install qdrant-client\nfrom cogent.vectorstore.backends import QdrantBackend\n\nbackend = QdrantBackend(\n    collection_name=\"my_docs\",\n    url=\"http://localhost:6333\",\n    # Or cloud:\n    # url=\"https://xxx.qdrant.io\",\n    # api_key=\"...\",\n)\nstore = VectorStore(backend=backend)\n</code></pre>"},{"location":"vectorstore/#pgvector","title":"pgvector","text":"<p>PostgreSQL with vector extension:</p> <pre><code># pip install psycopg[pool]\nfrom cogent.vectorstore.backends import PgVectorBackend\n\nbackend = PgVectorBackend(\n    connection_string=\"postgresql://user:pass@localhost/db\",\n    table_name=\"embeddings\",\n    dimension=1536,\n)\nstore = VectorStore(backend=backend)\n</code></pre>"},{"location":"vectorstore/#document-management","title":"Document Management","text":""},{"location":"vectorstore/#adding-documents","title":"Adding Documents","text":"<pre><code>from cogent.vectorstore import VectorStore, Document\n\nstore = VectorStore()\n\n# Add texts (simple)\nids = await store.add_texts([\n    \"First document\",\n    \"Second document\",\n])\n\n# Add texts with metadata\nids = await store.add_texts(\n    texts=[\"Doc 1\", \"Doc 2\"],\n    metadatas=[{\"type\": \"a\"}, {\"type\": \"b\"}],\n)\n\n# Add Document objects\ndocs = [\n    Document(text=\"Content\", metadata={\"source\": \"file.txt\"}),\n]\nids = await store.add_documents(docs)\n</code></pre>"},{"location":"vectorstore/#document-utilities","title":"Document Utilities","text":"<pre><code>from cogent.vectorstore import (\n    create_documents,\n    split_text,\n    split_documents,\n)\n\n# Create documents from texts\ndocs = create_documents(\n    texts=[\"Text 1\", \"Text 2\"],\n    metadatas=[{\"id\": 1}, {\"id\": 2}],\n)\n\n# Split text into chunks\nchunks = split_text(\n    text=long_text,\n    chunk_size=1000,\n    chunk_overlap=200,\n)\n\n# Split documents into chunks\nchunks = split_documents(\n    documents=docs,\n    chunk_size=1000,\n    chunk_overlap=200,\n)\n</code></pre>"},{"location":"vectorstore/#searching","title":"Searching","text":""},{"location":"vectorstore/#basic-search","title":"Basic Search","text":"<pre><code>results = await store.search(\"query\", k=5)\n\nfor r in results:\n    print(f\"Score: {r.score}\")\n    print(f\"Text: {r.text}\")\n    print(f\"Metadata: {r.metadata}\")\n</code></pre>"},{"location":"vectorstore/#filtered-search","title":"Filtered Search","text":"<pre><code># Filter by metadata\nresults = await store.search(\n    \"query\",\n    k=10,\n    filter={\"type\": \"tutorial\"},\n)\n\n# Multiple filter conditions\nresults = await store.search(\n    \"query\",\n    k=10,\n    filter={\n        \"type\": \"tutorial\",\n        \"language\": \"python\",\n    },\n)\n</code></pre>"},{"location":"vectorstore/#search-with-threshold","title":"Search with Threshold","text":"<pre><code># Only return results above score threshold\nresults = await store.search(\n    \"query\",\n    k=10,\n    score_threshold=0.7,\n)\n</code></pre>"},{"location":"vectorstore/#searchresult","title":"SearchResult","text":"<p>Search results contain:</p> <pre><code>from cogent.vectorstore import SearchResult\n\n@dataclass\nclass SearchResult:\n    text: str              # Document text\n    score: float           # Similarity score\n    metadata: dict         # Document metadata\n    id: str | None         # Document ID\n    embedding: list | None # Vector (if requested)\n</code></pre>"},{"location":"vectorstore/#factory-function","title":"Factory Function","text":"<p>Create vectorstore with specific configuration:</p> <pre><code>from cogent.vectorstore import create_vectorstore\n\n# Simple\nstore = create_vectorstore()\n\n# With backend\nstore = create_vectorstore(\n    backend=\"chroma\",\n    collection_name=\"docs\",\n    persist_directory=\"./data\",\n)\n\n# With embeddings\nstore = create_vectorstore(\n    embeddings=\"ollama\",\n    model=\"nomic-embed-text\",\n)\n</code></pre>"},{"location":"vectorstore/#similarity-metrics","title":"Similarity Metrics","text":"<pre><code>from cogent.vectorstore.backends.inmemory import SimilarityMetric\n\nSimilarityMetric.COSINE       # Normalized dot product (default)\nSimilarityMetric.DOT_PRODUCT  # Raw dot product\nSimilarityMetric.EUCLIDEAN    # L2 distance\n</code></pre>"},{"location":"vectorstore/#integration-with-retrievers","title":"Integration with Retrievers","text":"<pre><code>from cogent.retriever import DenseRetriever\nfrom cogent.vectorstore import VectorStore\n\n# Create vectorstore and add documents\nstore = VectorStore()\nawait store.add_texts(docs)\n\n# Create retriever\nretriever = DenseRetriever(store)\n\n# Search directly\nresults = await retriever.retrieve(\"What is X?\", k=5)\nfor r in results:\n    print(r.document.text)\n</code></pre>"},{"location":"vectorstore/#persistence","title":"Persistence","text":""},{"location":"vectorstore/#saveload-inmemory","title":"Save/Load (InMemory)","text":"<pre><code># Save to disk\nawait store.save(\"vectorstore.pkl\")\n\n# Load from disk\nstore = await VectorStore.load(\"vectorstore.pkl\")\n</code></pre>"},{"location":"vectorstore/#persistent-backends","title":"Persistent Backends","text":"<p>Chroma, Qdrant, and pgvector automatically persist:</p> <pre><code># Chroma persists to disk\nbackend = ChromaBackend(persist_directory=\"./data\")\nstore = VectorStore(backend=backend)\n\n# Data persists across sessions\n</code></pre>"},{"location":"vectorstore/#batch-operations","title":"Batch Operations","text":"<pre><code># Add in batches for large datasets\nbatch_size = 100\nfor i in range(0, len(texts), batch_size):\n    batch = texts[i:i + batch_size]\n    await store.add_texts(batch)\n\n# Parallel embedding\nawait store.add_texts(\n    texts=large_list,\n    batch_size=50,\n    parallel=True,\n)\n</code></pre>"},{"location":"vectorstore/#api-reference","title":"API Reference","text":""},{"location":"vectorstore/#vectorstore","title":"VectorStore","text":"Method Description <code>add_texts(texts, metadatas?)</code> Add texts to store <code>add_documents(docs)</code> Add Document objects <code>search(query, k?, filter?)</code> Similarity search <code>delete(ids)</code> Delete by IDs <code>clear()</code> Remove all documents <code>save(path)</code> Save to disk <code>load(path)</code> Load from disk"},{"location":"vectorstore/#embedding-providers_1","title":"Embedding Providers","text":"<p>All embedding providers implement the <code>EmbeddingProvider</code> protocol:</p> <pre><code>from cogent.vectorstore.base import EmbeddingProvider\n\nclass EmbeddingProvider(Protocol):\n    \"\"\"Protocol for embedding providers used by VectorStore.\"\"\"\n\n    @property\n    def dimension(self) -&gt; int:\n        \"\"\"Return embedding dimension.\"\"\"\n\n    async def embed_texts(self, texts: list[str]) -&gt; list[list[float]]:\n        \"\"\"Generate embeddings for multiple texts.\"\"\"\n\n    async def embed_query(self, text: str) -&gt; list[float]:\n        \"\"\"Generate embedding for a query text.\"\"\"\n</code></pre> <p>Available providers: - <code>OpenAIEmbedding</code> - OpenAI API (default) - <code>AzureOpenAIEmbedding</code> - Azure OpenAI - <code>OllamaEmbedding</code> - Local Ollama - <code>GeminiEmbedding</code> - Google Gemini - <code>CohereEmbedding</code> - Cohere - <code>MistralEmbedding</code> - Mistral AI - <code>CloudflareEmbedding</code> - Cloudflare Workers AI - <code>CustomEmbedding</code> - Custom OpenAI-compatible APIs - <code>MockEmbedding</code> - Testing</p> <p>All providers are imported from <code>cogent.models</code>.</p>"},{"location":"vectorstore/#backends","title":"Backends","text":"Backend Use Case <code>InMemoryBackend</code> Development, small datasets <code>FAISSBackend</code> Large-scale, local <code>ChromaBackend</code> Persistent, easy setup <code>QdrantBackend</code> Production, distributed <code>PgVectorBackend</code> PostgreSQL integration"},{"location":"vectorstore/#utilities","title":"Utilities","text":"Function Description <code>create_documents(texts, metadatas)</code> Create Document list <code>split_text(text, chunk_size)</code> Split text into chunks <code>split_documents(docs, chunk_size)</code> Split documents into chunks"}]}