{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Cogent","text":"<p> Build AI agents that actually work. </p> <p>Cogent is a production-grade AI agent framework designed for performance, simplicity, and real-world deployment. Unlike frameworks that wrap LangChain or add unnecessary abstractions, Cogent uses native SDK integrations and a zero-overhead executor to deliver the fastest possible agent execution.</p>"},{"location":"#why-cogent","title":"Why Cogent?","text":"<ul> <li>\ud83d\ude80 Fast \u2014 Parallel tool execution, cached model binding, direct SDK calls</li> <li>\ud83d\udd27 Simple \u2014 Define tools with <code>@tool</code>, create agents in 3 lines, no boilerplate</li> <li>\ud83c\udfed Production-ready \u2014 Built-in resilience, observability, and security interceptors</li> <li>\ud83e\udd1d Multi-agent \u2014 Supervisor, Pipeline, Mesh, and Hierarchical coordination patterns</li> <li>\ud83d\udce6 Batteries included \u2014 File system, web search, code sandbox, browser, PDF, knowledge graphs, and more</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from cogent import Agent, tool\n\n@tool\ndef search(query: str) -&gt; str:\n    \"\"\"Search the web.\"\"\"\n    return web_search(query)\n\n# v1.14.1: Simple string models!\nagent = Agent(name=\"Assistant\", model=\"gpt4\", tools=[search])\nresult = await agent.run(\"Find the latest news on AI agents\")\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<pre><code># Install from GitHub\npip install git+https://github.com/milad-o/cogent.git\n\n# Or with uv (recommended)\nuv add git+https://github.com/milad-o/cogent.git\n</code></pre> <p>Get Started \u2192</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Native Executor \u2014 High-performance parallel tool execution with zero framework overhead</li> <li>Native Model Support \u2014 OpenAI, Azure, Anthropic, Gemini, Groq, Ollama, Custom endpoints</li> <li>Multi-Agent Patterns \u2014 Supervisor, Pipeline, Mesh, Hierarchical</li> <li>Capabilities \u2014 Filesystem, Web Search, Code Sandbox, Browser, PDF, Shell, MCP, Spreadsheet, and more</li> <li>RAG Pipeline \u2014 Document loading, per-file-type splitting, embeddings, vector stores, retrievers</li> <li>Memory &amp; Persistence \u2014 Conversation history, long-term memory with fuzzy matching</li> <li>Graph Visualization \u2014 Mermaid, Graphviz, ASCII diagrams for agents and patterns</li> <li>Observability \u2014 Tracing, metrics, progress tracking, structured logging</li> <li>Interceptors \u2014 Budget guards, rate limiting, PII protection, tool gates</li> <li>Resilience \u2014 Retry policies, circuit breakers, fallbacks</li> <li>Human-in-the-Loop \u2014 Tool approval, guidance, interruption handling</li> <li>Streaming \u2014 Real-time token streaming with callbacks</li> <li>Structured Output \u2014 Type-safe responses with Pydantic schemas</li> <li>Reasoning \u2014 Extended thinking mode with chain-of-thought</li> </ul>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Getting Started \u2014 Get started in 5 minutes</li> <li>Agent Documentation \u2014 Learn about the core Agent class</li> <li>Multi-Agent \u2014 Build coordinated multi-agent systems</li> <li>Capabilities \u2014 Explore built-in capabilities</li> <li>Examples \u2014 See working examples</li> </ul>"},{"location":"#latest-release-v1141","title":"Latest Release (v1.14.1)","text":"<p>3-Tier Model API - String Models</p> <ul> <li>\ud83c\udfaf Simple String Models \u2014 <code>Agent(model=\"gpt4\")</code> auto-resolves to gpt-4o</li> <li>\ud83c\udff7\ufe0f 50+ Model Aliases \u2014 <code>gpt5</code>, <code>gpt4</code>, <code>claude</code>, <code>gemini3</code>, <code>mistral</code>, <code>command-a</code>, etc.</li> <li>\ud83d\udd17 Provider Prefix \u2014 <code>\"anthropic:claude\"</code>, <code>\"groq:llama-70b\"</code></li> <li>\u2699\ufe0f Auto-Configuration \u2014 Loads API keys and model overrides from <code>.env</code>, TOML/YAML, or env vars</li> <li>\ud83d\udd04 Backward Compatible \u2014 Existing code works unchanged</li> <li>\ud83e\udde0 3 API Tiers \u2014 String (simple), Factory (4 patterns), Direct (full control)</li> <li>\ud83d\udd0d Auto Provider Detection \u2014 Supports GPT-5, Gemini 3, Mistral Large 3, Command A, and all mainstream models</li> <li>\u2705 74 New Tests \u2014 Comprehensive test coverage for all new features</li> </ul> <p>See CHANGELOG for full version history.</p>"},{"location":"acc/","title":"Agentive Context Control (ACC)","text":"<p>Bounded memory for long conversations with drift prevention.</p>"},{"location":"acc/#overview","title":"Overview","text":"<p>ACC (Agentic Context Compression) maintains bounded internal state instead of unbounded transcript replay. Based on arXiv:2601.11653, it prevents:</p> <ul> <li>Context drift \u2014 Maintains constraints and entities across turns</li> <li>Memory poisoning \u2014 Verifies artifacts before committing</li> <li>Context overflow \u2014 Bounded state regardless of conversation length</li> </ul>"},{"location":"acc/#quick-start","title":"Quick Start","text":"<p>Enable ACC with <code>acc=True</code> on Agent or Memory:</p> <pre><code>from cogent import Agent\nfrom cogent.memory import Memory\n\n# Option 1: Enable on Agent\nagent = Agent(name=\"Assistant\", model=\"gpt-4o\", acc=True)\n\n# Option 2: Enable on Memory\nmemory = Memory(acc=True)\nagent = Agent(name=\"Assistant\", model=\"gpt-4o\", memory=memory)\n\n# Use thread_id to persist context across turns\nawait agent.run(\"My name is Alice\", thread_id=\"session-1\")\nawait agent.run(\"I prefer dark mode\", thread_id=\"session-1\")\nawait agent.run(\"What's my name?\", thread_id=\"session-1\")  # Remembers!\n</code></pre>"},{"location":"acc/#custom-bounds","title":"Custom Bounds","text":"<p>For fine-grained control, pass custom bounds directly to <code>AgentCognitiveCompressor</code>:</p> <pre><code>from cogent import Agent\nfrom cogent.memory import Memory\nfrom cogent.memory.acc import AgentCognitiveCompressor\n\n# Create ACC with custom bounds\nacc = AgentCognitiveCompressor(\n    max_constraints=10,  # Rules, guidelines (default: 10)\n    max_entities=30,     # Facts, knowledge (default: 50)\n    max_actions=20,      # Past actions (default: 30)\n    max_context=15,      # Relevant context (default: 20)\n)\n\n# Pass to Agent or Memory\nagent = Agent(name=\"Assistant\", model=\"gpt-4o\", acc=acc)\n# OR\nmemory = Memory(acc=acc)\nagent = Agent(name=\"Assistant\", model=\"gpt-4o\", memory=memory)\n\n# Access state for monitoring\nprint(f\"Entities: {len(acc.state.entities)}/{acc.state.max_entities}\")\nprint(f\"Actions: {len(acc.state.actions)}/{acc.state.max_actions}\")\n</code></pre>"},{"location":"acc/#extraction-modes","title":"Extraction Modes","text":"<p>ACC supports two extraction modes:</p> Mode Description Speed Quality <code>heuristic</code> Rule-based extraction (default) \u26a1 Fast Good <code>model</code> LLM-based semantic extraction Slower Better"},{"location":"acc/#heuristic-mode-default","title":"Heuristic Mode (Default)","text":"<p>Fast, rule-based extraction using keyword matching and simple heuristics:</p> <pre><code>acc = AgentCognitiveCompressor(\n    extraction_mode=\"heuristic\",  # Default\n)\n</code></pre>"},{"location":"acc/#model-mode","title":"Model Mode","text":"<p>Uses an LLM to semantically extract constraints, entities, and actions:</p> <pre><code># Option 1: Specify a dedicated efficient model\nacc = AgentCognitiveCompressor(\n    extraction_mode=\"model\",\n    model=\"gpt-4o-mini\",  # Efficient model for extraction\n)\n\n# Option 2: Use agent's model (no model specified)\nacc = AgentCognitiveCompressor(\n    extraction_mode=\"model\",\n    # model=None \u2192 Uses agent's model automatically\n)\n\n# Option 3: Pass any BaseChatModel\nfrom cogent.models import AnthropicChat\nacc = AgentCognitiveCompressor(\n    extraction_mode=\"model\",\n    model=AnthropicChat(model=\"claude-3-haiku-20240307\"),\n)\n</code></pre> <p>Recommendation: Use <code>extraction_mode=\"model\"</code> with <code>model=\"gpt-4o-mini\"</code> or similar efficient model for best quality/cost balance.</p>"},{"location":"acc/#when-to-use-acc","title":"When to Use ACC","text":"Use ACC When Don't Use When Long conversations (&gt;10 turns) Short, stateless queries Need to prevent drift Simple Q&amp;A Bounded memory is critical Need full transcript replay Multi-turn workflows One-off operations"},{"location":"acc/#how-acc-works","title":"How ACC Works","text":"<p>ACC maintains bounded internal state with four categories:</p> Category Purpose Default Max Constraints Rules, guidelines, requirements 10 Entities Facts, knowledge, data 50 Actions What worked/failed 30 Context Relevant snippets 20 <p>Total: ~110 items regardless of conversation length.</p> <pre><code>from cogent.memory.acc import BoundedMemoryState\n\n# View state contents\nstate = BoundedMemoryState()\nprint(state.constraints)  # List of constraints\nprint(state.entities)     # List of entities\nprint(state.actions)      # List of actions\nprint(state.context)      # List of context items\n</code></pre>"},{"location":"acc/#acc-vs-semanticcache","title":"ACC vs SemanticCache","text":"Feature ACC SemanticCache Purpose Bounded conversation context Cache tool outputs Matching Structured memory extraction Semantic similarity Use Case Long conversations Expensive tool calls Thread-aware Yes (thread_id) No <p>Use together: ACC for conversation context, SemanticCache for tool output caching.</p>"},{"location":"acc/#best-practices","title":"Best Practices","text":"<ol> <li>Always use thread_id \u2014 Required for context persistence across turns</li> <li>Set appropriate bounds \u2014 Smaller bounds = less context but faster</li> <li>Scope per user/session \u2014 Use unique thread_id per conversation</li> <li>Monitor state \u2014 Check entity/action counts for debugging</li> </ol>"},{"location":"acc/#examples","title":"Examples","text":"<p>See working examples: - examples/advanced/acc.py \u2014 ACC usage patterns - examples/advanced/content_review.py \u2014 ACC with Memory integration</p>"},{"location":"acc/#api-reference","title":"API Reference","text":""},{"location":"acc/#boundedmemorystate","title":"BoundedMemoryState","text":"<pre><code>class BoundedMemoryState:\n    def __init__(\n        self,\n        max_constraints: int = 10,\n        max_entities: int = 50,\n        max_actions: int = 30,\n        max_context: int = 20,\n    ):\n        \"\"\"Initialize bounded state with category limits.\"\"\"\n\n    @property\n    def constraints(self) -&gt; list[str]: ...\n    @property\n    def entities(self) -&gt; list[str]: ...\n    @property\n    def actions(self) -&gt; list[str]: ...\n    @property\n    def context(self) -&gt; list[str]: ...\n</code></pre>"},{"location":"acc/#agentcognitivecompressor","title":"AgentCognitiveCompressor","text":"<pre><code>class AgentCognitiveCompressor:\n    def __init__(\n        self,\n        state: BoundedMemoryState,\n        forget_gate: SemanticForgetGate | None = None,\n    ):\n        \"\"\"Initialize ACC with bounded state.\"\"\"\n\n    async def update_from_turn(\n        self,\n        user_message: str,\n        assistant_message: str,\n        tool_calls: list[dict],\n        current_task: str,\n    ) -&gt; None:\n        \"\"\"Update memory state from a conversation turn.\"\"\"\n</code></pre>"},{"location":"acc/#further-reading","title":"Further Reading","text":"<ul> <li>Memory System \u2014 Overview of all memory components</li> <li>Semantic Cache \u2014 Similarity-based caching</li> <li>Agent Configuration \u2014 Configuring agents with ACC</li> </ul>"},{"location":"agent/","title":"Agent Module","text":"<p>The <code>cogent.agent</code> module defines the core agent abstraction - autonomous entities that can think, act, and communicate.</p>"},{"location":"agent/#overview","title":"Overview","text":"<p>Agents are the primary actors in the system. Each agent has: - A unique identity and role - Configuration defining its capabilities - Runtime state tracking its activity - Access to tools and the event bus</p> <pre><code>from cogent import Agent\n\n# Simple string model (recommended for v1.14.1+)\nagent = Agent(\n    name=\"Researcher\",\n    model=\"gpt4\",  # Auto-resolves to gpt-4o\n    tools=[search_tool],\n    instructions=\"You are a research assistant.\",\n)\n\n# With provider prefix\nagent = Agent(\n    name=\"Researcher\",\n    model=\"anthropic:claude\",  # Explicit provider\n    tools=[search_tool],\n)\n\n# Medium-level: Factory function\nfrom cogent.models import create_chat\nagent = Agent(\n    name=\"Researcher\",\n    model=create_chat(\"gpt4\"),\n    tools=[search_tool],\n)\n\n# Low-level: Full control\nfrom cogent.models import OpenAIChat\nmodel = OpenAIChat(model=\"gpt-4o\", temperature=0.7)\nagent = Agent(\n    name=\"Researcher\",\n    model=model,\n    tools=[search_tool],\n)\n\nresult = await agent.run(\"Find information about quantum computing\")\n</code></pre>"},{"location":"agent/#core-classes","title":"Core Classes","text":""},{"location":"agent/#agent","title":"Agent","text":"<p>The main agent class with multiple construction patterns:</p> <pre><code>from cogent import Agent\n\n# Simplified API (recommended)\nagent = Agent(\n    name=\"Writer\",\n    model=\"gpt4\",  # String model - auto-resolves to gpt-4o\n    role=\"worker\",  # String: \"worker\", \"supervisor\", \"autonomous\", \"reviewer\"\n    tools=[write_tool],\n    instructions=\"You write compelling content.\",\n)\n\n# With provider prefix for other providers\nagent = Agent(\n    name=\"Writer\",\n    model=\"anthropic:claude-sonnet-4\",\n    role=\"worker\",\n)\n\n# Advanced API with AgentConfig\nfrom cogent.agent import AgentConfig\nfrom cogent.core.enums import AgentRole\nfrom cogent.models import create_chat\n\nconfig = AgentConfig(\n    name=\"Writer\",\n    role=AgentRole.WORKER,\n    model=create_chat(\"gpt4\"),\n    tools=[\"write_poem\", \"write_story\"],\n    resilience_config=ResilienceConfig.aggressive(),\n)\nagent = Agent(config=config)\n</code></pre>"},{"location":"agent/#roleconfig-objects-recommended","title":"RoleConfig Objects (Recommended)","text":"<p>Use role configuration objects for type-safe, immutable role definitions:</p> <pre><code>from cogent import (\n    SupervisorRole,\n    WorkerRole,\n    ReviewerRole,\n    AutonomousRole,\n    CustomRole,\n)\n\n# Supervisor - coordinates workers\nsupervisor = Agent(\n    name=\"Manager\",\n    model=\"gpt4\",  # String model\n    role=SupervisorRole(workers=[\"Analyst\", \"Writer\"]),\n)\n\n# Worker - executes tasks with tools\nworker = Agent(\n    name=\"Analyst\",\n    model=\"claude\",  # Alias for claude-sonnet-4\n    role=WorkerRole(specialty=\"data analysis and visualization\"),\n    tools=[search, analyze],\n)\n\n# Reviewer - evaluates and approves work\nreviewer = Agent(\n    name=\"QA\",\n    model=\"gemini-pro\",  # Alias for gemini-2.5-pro\n    role=ReviewerRole(criteria=[\"accuracy\", \"clarity\", \"completeness\"]),\n)\n\n# Autonomous - independent agent with full capabilities\nautonomous = Agent(\n    name=\"Assistant\",\n    model=\"anthropic:claude-opus-4\",  # Provider prefix\n    role=AutonomousRole(),\n)    tools=[search, write],\n)\n\n# Custom - hybrid role with explicit capability overrides\ncustom = Agent(\n    name=\"TechnicalReviewer\",\n    model=model,\n    role=CustomRole(\n        base_role=AgentRole.REVIEWER,\n        can_use_tools=True,  # Reviewer that can use tools!\n    ),\n    tools=[code_analyzer, linter],\n)\n</code></pre> <p>Benefits of RoleConfig objects: - Type-safe configuration - Immutable (frozen dataclasses) - Built-in prompt enhancement - Clear, explicit role definitions - IDE autocomplete and type checking</p>"},{"location":"agent/#role-specific-parameters-backward-compatible","title":"Role-Specific Parameters (Backward Compatible)","text":"<p>You can also use string/enum roles with parameters:</p> <pre><code># Supervisor - with team members\nsupervisor = Agent(\n    name=\"Manager\",\n    model=model,\n    role=AgentRole.SUPERVISOR,  # or \"supervisor\"\n    workers=[\"analyst\", \"writer\"],  # Adds team members to prompt\n)\n\n# Worker - with specialty description\nworker = Agent(\n    name=\"Analyst\", \n    model=model,\n    role=\"worker\",\n    specialty=\"data analysis and visualization\",  # Adds specialty to prompt\n    tools=[search, analyze],\n)\n\n# Reviewer - with evaluation criteria\nreviewer = Agent(\n    name=\"QA\",\n    model=model,\n    role=\"reviewer\",\n    criteria=[\"accuracy\", \"clarity\", \"completeness\"],  # Adds criteria to prompt\n)\n\n# Autonomous - works independently, can finish\nautonomous = Agent(\n    name=\"Assistant\",\n    model=model,\n    role=\"autonomous\",\n    tools=[search, write],\n)\n</code></pre> <p>Note: While backward compatible, RoleConfig objects are recommended for new code.</p>"},{"location":"agent/#custom-roles-capability-overrides","title":"Custom Roles (Capability Overrides)","text":"<p>Recommended: Use <code>CustomRole</code> for hybrid capabilities:</p> <pre><code>from cogent import CustomRole\nfrom cogent.core import AgentRole\n\n# Reviewer that can use tools\nhybrid_reviewer = Agent(\n    name=\"TechnicalReviewer\",\n    model=model,\n    role=CustomRole(\n        base_role=AgentRole.REVIEWER,\n        can_use_tools=True,  # Override!\n    ),\n    tools=[code_analyzer, linter],\n)\n\n# Worker that can finish and delegate\norchestrator = Agent(\n    name=\"Orchestrator\",\n    model=model,\n    role=CustomRole(\n        base_role=AgentRole.WORKER,\n        can_finish=True,      # Override!\n        can_delegate=True,    # Override!\n    ),\n    tools=[deployment_tool],\n)\n</code></pre> <p>Backward compatible: You can also use capability overrides with string/enum roles:</p> <pre><code># Hybrid: Reviewer that can use tools\nhybrid_reviewer = Agent(\n    name=\"TechnicalReviewer\",\n    model=model,\n    role=\"reviewer\",\n    can_use_tools=True,  # Override! Reviewer normally can't use tools\n    tools=[code_analyzer, linter],\n)\n\n# Custom orchestrator: Worker that can finish and delegate\norchestrator = Agent(\n    name=\"Orchestrator\",\n    model=model,\n    role=\"worker\",\n    can_finish=True,      # Override! Worker normally can't finish\n    can_delegate=True,   # Override! Worker normally can't delegate\n    tools=[deployment_tool],\n)\n</code></pre>"},{"location":"agent/#role-system","title":"Role System","text":"<p>Roles define capabilities (what an agent CAN do) and inject system prompts that guide LLM behavior. They don't define personalities - that comes from your <code>instructions</code>.</p>"},{"location":"agent/#role-capabilities","title":"Role Capabilities","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Role        \u2502 can_finish \u2502 can_delegate \u2502 can_use_tools \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 WORKER      \u2502     \u274c     \u2502      \u274c      \u2502      \u2705       \u2502\n\u2502 SUPERVISOR  \u2502     \u2705     \u2502      \u2705      \u2502      \u274c       \u2502\n\u2502 AUTONOMOUS  \u2502     \u2705     \u2502      \u274c      \u2502      \u2705       \u2502\n\u2502 REVIEWER    \u2502     \u2705     \u2502      \u274c      \u2502      \u274c       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>When to Use: - WORKER: Executes tasks with tools, reports back - SUPERVISOR: Coordinates workers, makes final decisions - AUTONOMOUS: Independent operation, full lifecycle - REVIEWER: Evaluates work, approves/rejects</p>"},{"location":"agent/#how-roles-work","title":"How Roles Work","text":"<p>Roles affect agent behavior in two ways:</p> <p>1. Capability Controls - What the agent is allowed to do: <pre><code># WORKER can use tools but cannot finish\nworker = Agent(name=\"Analyst\", model=model, role=\"worker\", tools=[analyze_tool])\nassert worker.can_use_tools == True\nassert worker.can_finish == False  # Must report to supervisor\n\n# AUTONOMOUS can use tools AND finish\nautonomous = Agent(name=\"Assistant\", model=model, role=\"autonomous\", tools=[search_tool])\nassert autonomous.can_use_tools == True\nassert autonomous.can_finish == True  # Can conclude independently\n</code></pre></p> <p>2. System Prompt Injection - How the LLM thinks:</p> <p>Each role gets a specialized system prompt that guides its behavior:</p> <ul> <li>WORKER: \"Execute tasks using tools... You cannot finish the workflow yourself\"</li> <li>SUPERVISOR: \"Delegate tasks to workers... Provide FINAL ANSWER when complete\"</li> <li>AUTONOMOUS: \"Work independently... Finish when the task is complete\"</li> <li>REVIEWER: \"Evaluate work quality... Approve or request revisions\"</li> </ul> <p>Example - See the difference: <pre><code># WORKER won't conclude\nworker = Agent(name=\"Worker\", model=model, role=\"worker\")\nresult = await worker.run(\"What is Python?\")\n# Response: \"Python is a programming language...\" (no conclusion)\n\n# AUTONOMOUS will conclude\nautonomous = Agent(name=\"Assistant\", model=model, role=\"autonomous\")\nresult = await autonomous.run(\"What is Python?\")\n# Response: \"FINAL ANSWER: Python is a high-level programming language...\"\n</code></pre></p>"},{"location":"agent/#when-to-use-each-role","title":"When to Use Each Role","text":"<p>WORKER - Task execution: <pre><code># \u2705 Good: Has tools, reports results\ndata_analyst = Agent(\n    name=\"DataAnalyst\",\n    model=model,\n    role=\"worker\",\n    tools=[load_data, analyze, plot],\n    instructions=\"Analyze datasets and create visualizations\",\n)\n\n# In multi-agent setup, supervisor coordinates workers\n</code></pre></p> <p>SUPERVISOR - Team coordination: <pre><code># \u2705 Good: Delegates to workers, makes final decisions\nmanager = Agent(\n    name=\"Manager\",\n    model=model,\n    role=\"supervisor\",\n    instructions=\"Coordinate the research team to deliver comprehensive reports\",\n)\n\n# LLM will try to delegate: \"DELEGATE TO researcher: Find information about...\"\n</code></pre></p> <p>AUTONOMOUS - Independent agents: <pre><code># \u2705 Good: Standalone assistant, full capability\nassistant = Agent(\n    name=\"Assistant\",\n    model=model,\n    role=\"autonomous\",\n    tools=[search, calculator, send_email],\n    instructions=\"Help users with their requests\",\n)\n\n# Can use tools AND provide final answers independently\n</code></pre></p> <p>REVIEWER - Quality control: <pre><code># \u2705 Good: Evaluates quality, no tool execution\nqa = Agent(\n    name=\"QualityAssurance\",\n    model=model,\n    role=\"reviewer\",\n    instructions=\"Review code for quality, security, and best practices\",\n)\n\n# LLM focuses on judgment: \"FINAL ANSWER: Approved\" or \"REVISION NEEDED: ...\"\n</code></pre></p>"},{"location":"agent/#capability-overrides","title":"Capability Overrides","text":"<p>Override role capabilities when needed: <pre><code># Hybrid: Reviewer that can use tools\ntech_reviewer = Agent(\n    name=\"TechnicalReviewer\",\n    model=model,\n    role=\"reviewer\",\n    can_use_tools=True,  # Override! Run automated checks\n    tools=[lint_code, run_tests],\n)\n</code></pre></p> <p>See <code>examples/basics/role_behavior.py</code> for real LLM behavior examples.</p>"},{"location":"agent/#taskboard","title":"TaskBoard","text":"<p>Enable task tracking for complex, multi-step work:</p> <pre><code>agent = Agent(\n    name=\"ProjectManager\",\n    model=\"gpt-4o-mini\",\n    instructions=\"You are a helpful project manager.\",\n    taskboard=True,  # Enables task tracking tools\n)\n\nresult = await agent.run(\"Plan a REST API for a todo app\")\n\n# Check taskboard after execution\nprint(agent.taskboard.summary())\n</code></pre>"},{"location":"agent/#taskboard-tools","title":"TaskBoard Tools","text":"<p>When <code>taskboard=True</code>, the agent gets these tools:</p> Tool Description <code>add_task</code> Create a new task to track <code>update_task</code> Update task status (pending, in_progress, completed, failed, blocked) <code>add_note</code> Record observations and findings <code>verify_task</code> Verify a task was completed correctly <code>get_taskboard_status</code> See overall progress"},{"location":"agent/#how-it-works","title":"How It Works","text":"<ol> <li>Instructions injected \u2014 Agent receives guidance on when/how to use taskboard</li> <li>LLM decides \u2014 For complex tasks, the agent breaks them into subtasks</li> <li>Progress tracked \u2014 Tasks have status, notes, and verification</li> <li>Summary available \u2014 <code>agent.taskboard.summary()</code> shows progress</li> </ol>"},{"location":"agent/#taskboard-configuration","title":"TaskBoard Configuration","text":"<pre><code>from cogent.agent.taskboard import TaskBoardConfig\n\nagent = Agent(\n    name=\"Worker\",\n    model=\"gpt4\",\n    taskboard=TaskBoardConfig(\n        include_instructions=True,  # Inject usage instructions (default: True)\n        max_tasks=50,               # Maximum tasks to track\n        track_time=True,            # Track task timing\n    ),\n)\n</code></pre> <p>See <code>examples/advanced/taskboard.py</code> for a complete example.</p>"},{"location":"agent/#memory-4-layer-architecture","title":"Memory (4-Layer Architecture)","text":"<p>Cogent provides a 4-layer memory architecture:</p> Layer Parameter Purpose 1 <code>conversation=True</code> Thread-based message history (default ON) 2 <code>acc=True</code> Agentic Context Compression - prevents drift 3 <code>memory=True</code> Long-term memory with remember/recall tools 4 <code>cache=True</code> Semantic cache for tool outputs"},{"location":"agent/#basic-usage","title":"Basic Usage","text":"<pre><code># Layer 1: Conversation memory (default ON)\nagent = Agent(name=\"Assistant\", model=\"gpt4\")\nawait agent.run(\"Hi, I'm Alice\", thread_id=\"conv-1\")\nawait agent.run(\"What's my name?\", thread_id=\"conv-1\")  # Remembers!\n\n# Layer 2: ACC for long conversations (prevents drift)\nagent = Agent(name=\"Assistant\", model=\"gpt4\", acc=True)\n\n# Layer 3: Long-term memory with tools\nagent = Agent(name=\"Assistant\", model=\"gpt4\", memory=True)\n# Agent gets remember(), recall(), forget() tools\n\n# Layer 4: Semantic cache for expensive tools\nagent = Agent(name=\"Assistant\", model=\"gpt4\", cache=True)\n\n# All layers together\nagent = Agent(\n    name=\"SuperAgent\",\n    model=\"gpt4\",\n    acc=True,     # Prevents context drift\n    memory=True,  # Long-term facts\n    cache=True,   # Cache tool outputs\n)\n</code></pre>"},{"location":"agent/#acc-agentic-context-compression","title":"ACC (Agentic Context Compression)","text":"<p>For long conversations (&gt;10 turns), enable ACC to prevent memory drift:</p> <pre><code>from cogent.memory.acc import AgentCognitiveCompressor\n\n# Simple: Enable with defaults\nagent = Agent(name=\"Assistant\", model=\"gpt4\", acc=True)\n\n# Advanced: Custom ACC with specific bounds\nacc = AgentCognitiveCompressor(max_constraints=5, max_entities=20)\nagent = Agent(name=\"Assistant\", model=\"gpt4\", acc=acc)\n</code></pre> <p>See docs/acc.md for detailed ACC documentation.</p>"},{"location":"agent/#semantic-cache","title":"Semantic Cache","text":"<p>For expensive tools, enable semantic caching to avoid redundant calls:</p> <pre><code>from cogent.memory import SemanticCache\nfrom cogent.models import create_embedding\n\n# Simple: Enable with defaults\nagent = Agent(name=\"Assistant\", model=\"gpt4\", cache=True)\n\n# Advanced: Custom SemanticCache instance\nembed = create_embedding(\"openai\", \"text-embedding-3-small\")\ncache = SemanticCache(\n    embedding=embed,\n    similarity_threshold=0.90,  # Stricter matching\n    max_entries=5000,\n    default_ttl=3600,  # 1 hour\n)\nagent = Agent(name=\"Assistant\", model=\"gpt4\", cache=cache)\n</code></pre> <p>See docs/memory.md#semantic-cache for detailed cache documentation.</p>"},{"location":"agent/#resilience","title":"Resilience","text":"<p>Built-in fault tolerance with retries, circuit breakers, and fallbacks:</p> <pre><code>from cogent.agent import ResilienceConfig, RetryPolicy\n\nagent = Agent(\n    name=\"Worker\",\n    model=model,\n    resilience=ResilienceConfig(\n        retry_policy=RetryPolicy(\n            max_retries=3,\n            base_delay=1.0,\n            strategy=\"exponential\",\n        ),\n    ),\n)\n</code></pre>"},{"location":"agent/#resilience-components","title":"Resilience Components","text":"<ul> <li>RetryPolicy: Configure retry behavior with exponential/linear backoff</li> <li>CircuitBreaker: Prevent cascading failures</li> <li>FallbackRegistry: Define fallback behaviors for failures</li> </ul>"},{"location":"agent/#human-in-the-loop-hitl","title":"Human-in-the-Loop (HITL)","text":"<p>Enable human oversight for sensitive operations:</p> <pre><code>agent = Agent(\n    name=\"Executor\",\n    model=model,\n    tools=[delete_file, send_email],\n    interrupt_on={\n        \"tools\": [\"delete_file\", \"send_email\"],  # Require approval\n    },\n)\n\ntry:\n    result = await agent.run(\"Delete temp files\")\nexcept InterruptedException as e:\n    # Human reviews pending action\n    decision = HumanDecision(approved=True)\n    result = await agent.resume(e.state, decision)\n</code></pre>"},{"location":"agent/#reasoning","title":"Reasoning","text":"<p>Enable extended thinking for complex problems with AI-controlled reasoning rounds.</p>"},{"location":"agent/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from cogent import Agent\nfrom cogent.agent.reasoning import ReasoningConfig\n\n# Simple: Enable with defaults\nagent = Agent(\n    name=\"Analyst\",\n    model=model,\n    reasoning=True,  # Default config\n)\n\nresult = await agent.run(\"Analyze this complex problem...\")\n</code></pre>"},{"location":"agent/#custom-configuration","title":"Custom Configuration","text":"<pre><code># Full control with ReasoningConfig\nagent = Agent(\n    name=\"DeepThinker\",\n    model=model,\n    reasoning=ReasoningConfig(\n        max_thinking_rounds=15,         # AI decides when ready (up to 15)\n        style=ReasoningStyle.CRITICAL,  # Critical reasoning style\n        show_thinking=True,             # Include thoughts in output\n    ),\n)\n</code></pre>"},{"location":"agent/#per-call-overrides","title":"Per-Call Overrides","text":"<p>Enable or customize reasoning for specific calls:</p> <pre><code># Agent without reasoning by default\nagent = Agent(name=\"Helper\", model=model, reasoning=False)\n\n# Simple task - no reasoning\nresult = await agent.run(\"What time is it?\")\n\n# Complex task - enable reasoning\nresult = await agent.run(\n    \"Analyze this codebase architecture\",\n    reasoning=True,  # Enable for this call\n)\n\n# Very complex - custom config\nresult = await agent.run(\n    \"Debug this complex issue\",\n    reasoning=ReasoningConfig(\n        max_thinking_rounds=10,\n        style=ReasoningStyle.ANALYTICAL,\n    ),\n)\n</code></pre>"},{"location":"agent/#reasoning-styles","title":"Reasoning Styles","text":"<ul> <li><code>ANALYTICAL</code>: Step-by-step logical breakdown (default)</li> <li><code>EXPLORATORY</code>: Consider multiple approaches</li> <li><code>CRITICAL</code>: Question assumptions, find flaws</li> <li><code>CREATIVE</code>: Generate novel solutions</li> </ul>"},{"location":"agent/#ai-controlled-rounds","title":"AI-Controlled Rounds","text":"<p>The AI signals when reasoning is complete via <code>&lt;ready&gt;true&lt;/ready&gt;</code> tags. The <code>max_thinking_rounds</code> is a safety limit, not a fixed count:</p> <pre><code>ReasoningConfig.standard()  # max 10 rounds (safety net)\nReasoningConfig.deep()      # max 15 rounds (complex problems)\n</code></pre>"},{"location":"agent/#structured-output","title":"Structured Output","text":"<p>Enforce response schemas with validation:</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import Literal\n\n# Structured models\nclass ContactInfo(BaseModel):\n    name: str = Field(description=\"Full name\")\n    email: str = Field(description=\"Email address\")\n    phone: str | None = Field(None, description=\"Phone number\")\n\nagent = Agent(\n    name=\"Extractor\",\n    model=model,\n    output=ContactInfo,  # Enforce schema\n)\n\nresult = await agent.run(\"Extract: John Doe, john@acme.com\")\nprint(result.content.data)  # ContactInfo(name=\"John Doe\", ...)\n\n# Bare types - return primitive values directly\nagent = Agent(name=\"Reviewer\", model=model, output=Literal[\"PROCEED\", \"REVISE\"])\nresult = await agent.run(\"Review this code\")\nprint(result.content.data)  # \"PROCEED\" (bare string, not wrapped)\n\n# Other bare types: str, int, bool, float\nagent = Agent(name=\"Counter\", model=model, output=int)\nresult = await agent.run(\"How many items?\")\nprint(result.content.data)  # 42 (bare int)\n</code></pre> <p>Supported schema types: - Pydantic models - Full validation with <code>BaseModel</code> - Dataclasses - Standard Python dataclasses - TypedDict - Typed dictionaries - Bare primitives - <code>str</code>, <code>int</code>, <code>bool</code>, <code>float</code> - Bare Literal - <code>Literal[\"A\", \"B\", ...]</code> for constrained choices - JSON Schema - Raw JSON Schema dicts</p>"},{"location":"agent/#taskboard_1","title":"TaskBoard","text":"<p>Human-like task tracking for complex workflows:</p> <pre><code>agent = Agent(\n    name=\"Researcher\",\n    model=model,\n    tools=[search, summarize],\n    taskboard=True,  # Adds task management tools\n)\n\nresult = await agent.run(\"Research Python async patterns\")\nprint(agent.taskboard.summary())\n</code></pre>"},{"location":"agent/#streaming","title":"Streaming","text":"<p>Enable token-by-token streaming:</p> <pre><code>agent = Agent(\n    name=\"Writer\",\n    model=model,\n    stream=True,\n)\n\nasync for chunk in agent.run(\"Write a story\", stream=True):\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"agent/#spawning","title":"Spawning","text":"<p>Dynamic agent creation at runtime:</p> <pre><code>from cogent.agent import SpawningConfig, AgentSpec\n\nagent = Agent(\n    name=\"Coordinator\",\n    model=model,\n    spawning=SpawningConfig(\n        allowed_specs=[\n            AgentSpec(name=\"researcher\", tools=[\"search\"]),\n            AgentSpec(name=\"writer\", tools=[\"write\"]),\n        ],\n    ),\n)\n\n# Agent can spawn sub-agents during execution\nresult = await agent.run(\"Research and write about AI\")\n</code></pre>"},{"location":"agent/#observability","title":"Observability","text":"<p>Built-in observability for standalone usage:</p> <pre><code>from cogent import Agent\nfrom cogent.observability import ObservabilityLevel\n\n# Boolean shorthand\nagent = Agent(name=\"Worker\", model=model, verbosity=True)  # Progress level\n\n# String levels\nagent = Agent(name=\"Worker\", model=model, verbosity=\"debug\")\n\n# Enum (explicit)\nagent = Agent(name=\"Worker\", model=model, verbosity=ObservabilityLevel.DEBUG)\n\n# Integer (0-5)\nagent = Agent(name=\"Worker\", model=model, verbosity=4)  # DEBUG\n\n# Advanced: Full control with observer\nfrom cogent.observability import Observer\n\nobserver = Observer(level=\"debug\")\nagent = Agent(name=\"Worker\", model=model, observer=observer)\n</code></pre> <p>Verbosity levels:</p> Level Int String Description <code>OFF</code> 0 <code>\"off\"</code> No output <code>RESULT</code> 1 <code>\"result\"</code>, <code>\"minimal\"</code> Only final results <code>PROGRESS</code> 2 <code>\"progress\"</code>, <code>\"normal\"</code> Key milestones (default for <code>True</code>) <code>DETAILED</code> 3 <code>\"detailed\"</code>, <code>\"verbose\"</code> Tool calls, timing <code>DEBUG</code> 4 <code>\"debug\"</code> Everything including internal events <code>TRACE</code> 5 <code>\"trace\"</code> Maximum detail + execution graph <p>Priority: <code>observer</code> parameter takes precedence over <code>verbosity</code>.</p>"},{"location":"agent/#api-reference","title":"API Reference","text":""},{"location":"agent/#agent-methods","title":"Agent Methods","text":"Method Description <code>run(task)</code> Execute a task and return result <code>chat(message, thread_id)</code> Chat with memory support <code>think(prompt)</code> Single reasoning step <code>stream_chat(message)</code> Streaming chat response <code>resume(state, decision)</code> Resume after HITL interrupt"},{"location":"agent/#agentconfig-fields","title":"AgentConfig Fields","text":"Field Type Description <code>name</code> <code>str</code> Agent name <code>role</code> <code>AgentRole</code> Agent role <code>model</code> <code>BaseChatModel</code> Chat model <code>tools</code> <code>list[str]</code> Tool names <code>system_prompt</code> <code>str</code> System instructions <code>resilience_config</code> <code>ResilienceConfig</code> Fault tolerance <code>interrupt_on</code> <code>dict</code> HITL triggers <code>stream</code> <code>bool</code> Enable streaming"},{"location":"agent/#exports","title":"Exports","text":"<pre><code>from cogent.agent import (\n    # Core\n    Agent,\n    AgentConfig,\n    AgentState,\n    # Memory\n    AgentMemory,\n    MemorySnapshot,\n    InMemorySaver,\n    ThreadConfig,\n    # Roles\n    RoleBehavior,\n    get_role_prompt,\n    get_role_behavior,\n    # Resilience\n    RetryStrategy,\n    RetryPolicy,\n    CircuitBreaker,\n    ResilienceConfig,\n    ToolResilience,\n    # HITL\n    InterruptReason,\n    HumanDecision,\n    InterruptedException,\n    # TaskBoard\n    TaskBoard,\n    TaskBoardConfig,\n    Task,\n    TaskStatus,\n    # Reasoning\n    ReasoningConfig,\n    ReasoningStyle,\n    ThinkingStep,\n    # Output\n    ResponseSchema,\n    StructuredResult,\n    # Spawning\n    AgentSpec,\n    SpawningConfig,\n    SpawnManager,\n)\n</code></pre>"},{"location":"capabilities/","title":"Capabilities Module","text":"<p>The <code>cogent.capabilities</code> module provides composable tools that plug into any agent. Capabilities are reusable building blocks that add domain-specific functionality.</p>"},{"location":"capabilities/#overview","title":"Overview","text":"<p>Capabilities encapsulate related functionality and expose it through native tools. Each capability: - Provides tools for the agent to use - May maintain internal state (graphs, caches, connections) - Can be initialized/shutdown with the agent lifecycle</p> <pre><code>from cogent import Agent\nfrom cogent.capabilities import (\n    KnowledgeGraph, FileSystem, WebSearch, CodeSandbox,\n    MCP, Spreadsheet, PDF, Browser, Shell, Summarizer,\n)\n\nagent = Agent(\n    name=\"Assistant\",\n    model=model,\n    capabilities=[\n        KnowledgeGraph(),                     # Entity/relationship memory\n        FileSystem(allowed_paths=[\"./data\"]), # Sandboxed file operations\n        WebSearch(),                          # Web search and fetching\n        CodeSandbox(),                        # Safe Python execution\n    ],\n)\n</code></pre>"},{"location":"capabilities/#available-capabilities","title":"Available Capabilities","text":""},{"location":"capabilities/#knowledgegraph","title":"KnowledgeGraph","text":"<p>Entity/relationship memory with multi-hop reasoning and multiple storage backends.</p> <pre><code>from cogent.capabilities import KnowledgeGraph\n\n# In-memory (default, no persistence)\nkg = KnowledgeGraph()\n\n# In-memory with auto-save to file\nkg = KnowledgeGraph(backend=\"memory\", path=\"memory.json\", auto_save=True)\n\n# SQLite for persistence\nkg = KnowledgeGraph(backend=\"sqlite\", path=\"knowledge.db\")\n\n# JSON file with auto-save\nkg = KnowledgeGraph(backend=\"json\", path=\"knowledge.json\")\n\n# Custom backend instance\nfrom cogent.capabilities.knowledge_graph.backends import GraphBackend\ncustom_backend = MyCustomBackend()  # Your implementation\nkg = KnowledgeGraph(backend=custom_backend)\n\nagent = Agent(\n    name=\"Researcher\",\n    model=model,\n    capabilities=[kg],\n)\n</code></pre> <p>Backend Switching:</p> <p>Switch backends dynamically with optional data migration:</p> <pre><code># Start with in-memory\nkg = KnowledgeGraph()\nkg.remember(\"Alice\", \"Person\", {\"role\": \"Engineer\"})\n\n# Switch to SQLite with migration\nkg.set_backend(\"sqlite\", path=\"knowledge.db\", migrate=True)\n\n# All data is now persisted, continue using same instance\nkg.remember(\"Bob\", \"Person\", {\"role\": \"Manager\"})\n\n# Switch to JSON\nkg.set_backend(\"json\", path=\"knowledge.json\", migrate=True)\n</code></pre> <p>Tools provided: | Tool | Description | |------|-------------| | <code>remember</code> | Store entities with attributes (dict or JSON string) | | <code>recall</code> | Retrieve information about entities | | <code>connect</code> | Create relationships between entities | | <code>query_knowledge</code> | Query relationships (source/relation/target params) | | <code>forget</code> | Remove entities and their relationships | | <code>list_knowledge</code> | List all entities, optionally filtered by type |</p> <p>Backends: - <code>memory</code>: Fast in-memory (uses networkx if available) - optionally auto-save to file - <code>sqlite</code>: Persistent SQLite for large graphs - always saves - <code>json</code>: Simple JSON file with auto-save - always saves - <code>neo4j</code>: Production graph database (requires neo4j package) - always saves - Custom: Extend <code>GraphBackend</code> for your own implementation</p> <p>Visualization:</p> <p>KnowledgeGraph provides a three-level API for visualization:</p> <pre><code>kg = KnowledgeGraph()\n# ... add entities and relationships ...\n\n# 1. LOW-LEVEL: kg.mermaid() - raw Mermaid code\ncode = kg.mermaid(direction=\"LR\")\nprint(code)  # Raw Mermaid diagram code\n\n# 2. MEDIUM-LEVEL: kg.render(format) - multiple formats\nascii_art = kg.render(\"ascii\")     # Terminal-friendly\nhtml = kg.render(\"html\")           # Interactive HTML\npng_bytes = kg.render(\"png\")       # PNG image bytes\nsvg_bytes = kg.render(\"svg\")       # SVG vector bytes\n\n# 3. HIGH-LEVEL: kg.display() - Jupyter inline rendering\nkg.display()  # Renders inline in Jupyter notebook\nkg.display(direction=\"TB\", show_attributes=True)\n\n# GRAPHVIEW: kg.visualize() - full control\nview = kg.visualize(direction=\"LR\", group_by_type=True)\nview.mermaid()   # Mermaid code\nview.ascii()     # ASCII art  \nview.url()       # Shareable mermaid.ink URL\nview.save(\"graph.mmd\")   # Mermaid source\nview.save(\"graph.html\")  # Interactive HTML\nview.save(\"graph.png\")   # PNG image\nview.save(\"graph.svg\")   # SVG vector\nview.save(\"graph.dot\")   # Graphviz DOT\n</code></pre> <p>Visualization options: - <code>direction</code>: Layout direction - <code>\"LR\"</code> (left-right), <code>\"TB\"</code> (top-bottom), <code>\"BT\"</code>, <code>\"RL\"</code> - <code>group_by_type</code>: Group entities by type in subgraphs (default: True) - <code>show_attributes</code>: Display entity attributes in labels (default: False)</p> <p>Entity colors: - Person: Blue (<code>#60a5fa</code>) - Company/Organization: Green (<code>#7eb36a</code>) - Location: Orange (<code>#f59e0b</code>) - Event: Purple (<code>#9b59b6</code>) - Generic: Gray (<code>#94a3b8</code>)</p> <p>Tool API (improved in v1.8.3): <pre><code># Query with structured parameters (NEW)\nquery_knowledge(source=None, relation=\"works_at\", target=\"TechCorp\")\n# Returns: Who works at TechCorp?\n\n# Remember with dict attributes (NEW - preferred)\nremember(entity=\"Alice\", entity_type=\"Person\", attributes={\"role\": \"CEO\", \"age\": 35})\n\n# Also accepts JSON string for backward compatibility\nremember(entity=\"Alice\", entity_type=\"Person\", attributes='{\"role\": \"CEO\"}')\n</code></pre></p>"},{"location":"capabilities/#filesystem","title":"FileSystem","text":"<p>Sandboxed file operations with security controls.</p> <pre><code>from cogent.capabilities import FileSystem\n\n# Read-only access to docs\nfs = FileSystem(\n    allowed_paths=[\"./docs\"],\n    allow_write=False,\n)\n\n# Full access with security\nfs = FileSystem(\n    allowed_paths=[\"./workspace\"],\n    deny_patterns=[\"*.env\", \"*.key\", \".git/*\"],\n    allow_delete=True,\n    max_file_size=10 * 1024 * 1024,  # 10MB\n)\n\nagent = Agent(name=\"Worker\", model=model, capabilities=[fs])\n</code></pre> <p>Tools provided: | Tool | Description | |------|-------------| | <code>read_file</code> | Read file contents | | <code>write_file</code> | Write/update files | | <code>list_directory</code> | List directory contents | | <code>search_files</code> | Search by pattern/content | | <code>copy_file</code> | Copy files | | <code>move_file</code> | Move/rename files | | <code>delete_file</code> | Delete files (if enabled) |</p> <p>Parameters: - <code>allowed_paths</code>: Directories the agent can access - <code>deny_patterns</code>: Glob patterns to block (e.g., <code>[\"*.env\", \".git/*\"]</code>) - <code>max_file_size</code>: Maximum file size in bytes (default: 10MB) - <code>allow_write</code>: Enable write operations (default: True) - <code>allow_delete</code>: Enable delete operations (default: False)</p>"},{"location":"capabilities/#websearch","title":"WebSearch","text":"<p>Web search and page fetching using DuckDuckGo (free, no API key).</p> <pre><code>from cogent.capabilities import WebSearch\n\n# Default configuration\nws = WebSearch()\n\n# Custom settings\nws = WebSearch(\n    max_results=10,\n    timeout=30,\n    include_raw_html=False,\n)\n\nagent = Agent(name=\"Researcher\", model=model, capabilities=[ws])\n</code></pre> <p>Tools provided: | Tool | Description | |------|-------------| | <code>web_search</code> | Search the web | | <code>news_search</code> | Search news articles | | <code>fetch_page</code> | Fetch and extract page content | | <code>fetch_multiple</code> | Fetch multiple URLs concurrently |</p> <p>Requires: <code>uv add ddgs</code></p>"},{"location":"capabilities/#codesandbox","title":"CodeSandbox","text":"<p>Safe Python code execution with security controls.</p> <pre><code>from cogent.capabilities import CodeSandbox\n\nsandbox = CodeSandbox(\n    timeout=30,           # Execution timeout in seconds\n    max_output_size=10000, # Max output characters\n    allow_imports=[\"math\", \"json\", \"re\"],  # Allowed imports\n)\n\nagent = Agent(name=\"Coder\", model=model, capabilities=[sandbox])\n</code></pre> <p>Tools provided: | Tool | Description | |------|-------------| | <code>execute_python</code> | Run Python code | | <code>run_function</code> | Execute a specific function | | <code>eval_expression</code> | Evaluate an expression |</p> <p>Security features: - Blocked dangerous imports (os, subprocess, socket, etc.) - Resource limits (time, memory) - AST-based code analysis - Sandboxed execution environment</p>"},{"location":"capabilities/#mcp-model-context-protocol","title":"MCP (Model Context Protocol)","text":"<p>Connect to local and remote MCP servers.</p> <pre><code>from cogent.capabilities import MCP\n\n# Local server (stdio)\nmcp_local = MCP.stdio(\n    command=\"uv\",\n    args=[\"run\", \"my-mcp-server\"],\n)\n\n# Remote server (HTTP/SSE)\nmcp_http = MCP.http(\"https://api.example.com/mcp\")\n\n# WebSocket\nmcp_ws = MCP.websocket(\"ws://localhost:8766\")\n\n# Multiple servers\nagent = Agent(\n    name=\"Assistant\",\n    model=model,\n    capabilities=[\n        MCP.stdio(command=\"npx\", args=[\"-y\", \"@modelcontextprotocol/server-filesystem\", \".\"]),\n        MCP.http(\"https://weather-api.example.com/mcp\"),\n    ],\n)\n</code></pre> <p>Transport types: - <code>STDIO</code>: Local process communication - <code>HTTP</code>: HTTP/Streamable HTTP - <code>SSE</code>: Server-Sent Events - <code>WEBSOCKET</code>: WebSocket connection</p>"},{"location":"capabilities/#spreadsheet","title":"Spreadsheet","text":"<p>Excel and CSV file operations.</p> <pre><code>from cogent.capabilities import Spreadsheet\n\nss = Spreadsheet(\n    default_format=\"xlsx\",\n    max_rows=100000,\n)\n\nagent = Agent(name=\"Analyst\", model=model, capabilities=[ss])\n</code></pre> <p>Tools provided: | Tool | Description | |------|-------------| | <code>read_spreadsheet</code> | Read Excel/CSV files | | <code>write_spreadsheet</code> | Create/update spreadsheets | | <code>query_spreadsheet</code> | Query with filters | | <code>spreadsheet_stats</code> | Get statistics |</p> <p>Requires: <code>uv add openpyxl pandas</code></p>"},{"location":"capabilities/#browser","title":"Browser","text":"<p>Headless browser automation with Playwright.</p> <pre><code>from cogent.capabilities import Browser\n\nbrowser = Browser(\n    headless=True,\n    timeout=30000,\n)\n\nagent = Agent(name=\"WebAgent\", model=model, capabilities=[browser])\n</code></pre> <p>Tools provided: | Tool | Description | |------|-------------| | <code>browse_url</code> | Navigate to URL | | <code>click_element</code> | Click on elements | | <code>fill_form</code> | Fill form fields | | <code>screenshot</code> | Take screenshots | | <code>extract_content</code> | Extract page content |</p> <p>Requires: <code>uv add playwright &amp;&amp; playwright install</code></p>"},{"location":"capabilities/#shell","title":"Shell","text":"<p>Sandboxed terminal command execution.</p> <pre><code>from cogent.capabilities import Shell\n\nshell = Shell(\n    allowed_commands=[\"ls\", \"cat\", \"grep\", \"find\"],\n    working_dir=\"./workspace\",\n    timeout=30,\n)\n\nagent = Agent(name=\"SysAdmin\", model=model, capabilities=[shell])\n</code></pre> <p>Tools provided: | Tool | Description | |------|-------------| | <code>run_command</code> | Execute shell command | | <code>run_script</code> | Execute shell script |</p> <p>Security features: - Command allowlist/blocklist - Working directory sandboxing - Timeout enforcement - Output size limits</p>"},{"location":"capabilities/#summarizer","title":"Summarizer","text":"<p>Document summarization with multiple strategies.</p> <pre><code>from cogent.capabilities import Summarizer, SummarizerConfig\n\nsummarizer = Summarizer(\n    config=SummarizerConfig(\n        strategy=\"map_reduce\",  # or \"refine\", \"hierarchical\"\n        chunk_size=4000,\n        max_concurrency=5,\n    ),\n)\n\nagent = Agent(name=\"Summarizer\", model=model, capabilities=[summarizer])\n</code></pre> <p>Tools provided: | Tool | Description | |------|-------------| | <code>summarize_text</code> | Summarize text content | | <code>summarize_file</code> | Summarize file contents | | <code>summarize_url</code> | Fetch and summarize URL |</p> <p>Strategies: - <code>map_reduce</code>: Parallel chunk summarization + combine - <code>refine</code>: Iterative refinement through chunks - <code>hierarchical</code>: Multi-level summarization tree</p>"},{"location":"capabilities/#codebaseanalyzer","title":"CodebaseAnalyzer","text":"<p>Python codebase analysis with AST parsing.</p> <pre><code>from cogent.capabilities import CodebaseAnalyzer\n\nanalyzer = CodebaseAnalyzer(\n    root_path=\"./src\",\n    include_patterns=[\"*.py\"],\n)\n\nagent = Agent(name=\"CodeReviewer\", model=model, capabilities=[analyzer])\n</code></pre> <p>Tools provided: | Tool | Description | |------|-------------| | <code>analyze_file</code> | Analyze Python file structure | | <code>find_definitions</code> | Find classes/functions | | <code>find_usages</code> | Find symbol usages | | <code>get_dependencies</code> | Analyze imports |</p>"},{"location":"capabilities/#creating-custom-capabilities","title":"Creating Custom Capabilities","text":"<p>Extend <code>BaseCapability</code> to create your own:</p> <pre><code>from cogent.capabilities.base import BaseCapability\nfrom cogent.tools import tool, BaseTool\n\nclass MyCapability(BaseCapability):\n    @property\n    def name(self) -&gt; str:\n        return \"my_capability\"\n\n    @property\n    def description(self) -&gt; str:\n        return \"Does something useful\"\n\n    @property\n    def tools(self) -&gt; list[BaseTool]:\n        return [self._my_tool()]\n\n    def _my_tool(self):\n        @tool\n        def do_something(x: str) -&gt; str:\n            '''Do something useful.'''\n            return f\"Did: {x}\"\n        return do_something\n\n    async def initialize(self, agent) -&gt; None:\n        \"\"\"Called when attached to agent.\"\"\"\n        pass\n\n    async def shutdown(self) -&gt; None:\n        \"\"\"Called when agent shuts down.\"\"\"\n        pass\n</code></pre>"},{"location":"capabilities/#exports","title":"Exports","text":"<pre><code>from cogent.capabilities import (\n    # Base class\n    BaseCapability,\n    # Capabilities\n    Browser,\n    CodebaseAnalyzer,\n    CodeSandbox,\n    FileSystem,\n    KnowledgeGraph,\n    MCP,\n    MCPServerConfig,\n    MCPTransport,\n    PDF,\n    Shell,\n    Spreadsheet,\n    Summarizer,\n    SummarizerConfig,\n    WebSearch,\n)\n</code></pre>"},{"location":"context/","title":"RunContext Module","text":"<p>The <code>cogent.context</code> module provides invocation-scoped context for dependency injection into tools and interceptors.</p>"},{"location":"context/#overview","title":"Overview","text":"<p>RunContext enables passing typed data to tools and interceptors at invocation time without global state:</p> <pre><code>from dataclasses import dataclass\nfrom cogent import Agent, RunContext, tool\n\n@dataclass\nclass AppContext(RunContext):\n    user_id: str\n    db: Database\n    api_key: str\n\n@tool\ndef get_user_data(ctx: RunContext) -&gt; str:\n    \"\"\"Get data for the current user.\"\"\"\n    user = ctx.db.get_user(ctx.user_id)\n    return f\"User: {user.name}\"\n\nagent = Agent(name=\"assistant\", model=model, tools=[get_user_data])\n\nresult = await agent.run(\n    \"Get my profile data\",\n    context=AppContext(user_id=\"123\", db=db, api_key=key),\n)\n</code></pre>"},{"location":"context/#creating-custom-contexts","title":"Creating Custom Contexts","text":""},{"location":"context/#basic-context","title":"Basic Context","text":"<pre><code>from dataclasses import dataclass\nfrom cogent import RunContext\n\n@dataclass\nclass MyContext(RunContext):\n    user_id: str\n    session_id: str\n    permissions: list[str] = field(default_factory=list)\n\n    def has_permission(self, perm: str) -&gt; bool:\n        return perm in self.permissions\n</code></pre>"},{"location":"context/#with-services","title":"With Services","text":"<pre><code>@dataclass\nclass ServiceContext(RunContext):\n    db: Database\n    cache: Redis\n    api_client: APIClient\n    logger: Logger\n\n    async def get_user(self, user_id: str) -&gt; User:\n        \"\"\"Get user with caching.\"\"\"\n        cached = await self.cache.get(f\"user:{user_id}\")\n        if cached:\n            return User.from_json(cached)\n        user = await self.db.get_user(user_id)\n        await self.cache.set(f\"user:{user_id}\", user.to_json())\n        return user\n</code></pre>"},{"location":"context/#using-context-in-tools","title":"Using Context in Tools","text":""},{"location":"context/#accessing-context","title":"Accessing Context","text":"<pre><code>from cogent import tool, RunContext\n\n@tool\ndef get_user_orders(ctx: RunContext) -&gt; str:\n    \"\"\"Get orders for the current user.\"\"\"\n    # Access context properties\n    user_id = ctx.user_id\n    orders = ctx.db.get_orders(user_id)\n    return f\"Found {len(orders)} orders\"\n\n@tool\ndef admin_action(action: str, ctx: RunContext) -&gt; str:\n    \"\"\"Perform admin action (requires admin permission).\"\"\"\n    if not ctx.has_permission(\"admin\"):\n        return \"Permission denied\"\n    return f\"Performed: {action}\"\n</code></pre>"},{"location":"context/#context-with-other-parameters","title":"Context with Other Parameters","text":"<pre><code>@tool\ndef search_with_context(\n    query: str,\n    limit: int = 10,\n    ctx: RunContext = None,  # Context is optional\n) -&gt; str:\n    \"\"\"Search with user context.\"\"\"\n    if ctx and ctx.user_id:\n        # Personalized search\n        return personalized_search(query, ctx.user_id, limit)\n    return generic_search(query, limit)\n</code></pre>"},{"location":"context/#using-context-in-interceptors","title":"Using Context in Interceptors","text":"<pre><code>from cogent.interceptors import Interceptor, InterceptContext, InterceptResult\n\nclass PermissionInterceptor(Interceptor):\n    async def intercept(\n        self,\n        phase: Phase,\n        context: InterceptContext,\n    ) -&gt; InterceptResult:\n        run_ctx = context.run_context\n\n        # Check permissions\n        if not run_ctx.has_permission(\"use_agent\"):\n            return InterceptResult.stop(\"Permission denied\")\n\n        return InterceptResult.continue_()\n</code></pre>"},{"location":"context/#context-metadata","title":"Context Metadata","text":"<p>The base <code>RunContext</code> includes a metadata dict for extension:</p> <pre><code>from cogent import RunContext\n\nctx = RunContext(metadata={\n    \"request_id\": \"req-123\",\n    \"correlation_id\": \"corr-456\",\n    \"trace_id\": \"trace-789\",\n})\n\n# Access metadata\nrequest_id = ctx.get(\"request_id\")\nrequest_id = ctx.metadata.get(\"request_id\")\n\n# Create new context with additional metadata\nnew_ctx = ctx.with_metadata(\n    timestamp=datetime.now(),\n    version=\"1.0\",\n)\n</code></pre>"},{"location":"context/#passing-context-to-agents","title":"Passing Context to Agents","text":"<pre><code>from cogent import Agent\n\nagent = Agent(name=\"assistant\", model=model, tools=[...])\n\n# Pass context at run time\nresult = await agent.run(\n    \"Perform action\",\n    context=MyContext(\n        user_id=\"user-123\",\n        session_id=\"sess-456\",\n        permissions=[\"read\", \"write\"],\n    ),\n)\n</code></pre>"},{"location":"context/#context-immutability","title":"Context Immutability","text":"<p>Context should be treated as immutable during execution:</p> <pre><code>@dataclass(frozen=True)  # Enforce immutability\nclass ImmutableContext(RunContext):\n    user_id: str\n    tenant_id: str\n\n# To \"modify\", create a new instance\nnew_ctx = ctx.with_metadata(extra=\"value\")\n</code></pre>"},{"location":"context/#request-scoped-context","title":"Request-Scoped Context","text":"<p>Common pattern for web applications:</p> <pre><code>from fastapi import FastAPI, Depends\nfrom cogent import Agent, RunContext\n\napp = FastAPI()\n\n@dataclass\nclass RequestContext(RunContext):\n    user_id: str\n    tenant_id: str\n    trace_id: str\n    db: AsyncSession\n\nasync def get_context(\n    request: Request,\n    db: AsyncSession = Depends(get_db),\n) -&gt; RequestContext:\n    return RequestContext(\n        user_id=request.state.user_id,\n        tenant_id=request.state.tenant_id,\n        trace_id=request.headers.get(\"X-Trace-ID\"),\n        db=db,\n    )\n\n@app.post(\"/chat\")\nasync def chat(\n    message: str,\n    context: RequestContext = Depends(get_context),\n):\n    agent = get_agent()\n    result = await agent.run(message, context=context)\n    return {\"response\": result.output}\n</code></pre>"},{"location":"context/#multi-tenant-context","title":"Multi-Tenant Context","text":"<pre><code>@dataclass\nclass TenantContext(RunContext):\n    tenant_id: str\n    tenant_config: dict\n    allowed_tools: list[str]\n    rate_limit: int\n\n    def can_use_tool(self, tool_name: str) -&gt; bool:\n        return tool_name in self.allowed_tools\n\n# Use in agent\nresult = await agent.run(\n    \"Query\",\n    context=TenantContext(\n        tenant_id=\"acme-corp\",\n        tenant_config={\"max_tokens\": 4000},\n        allowed_tools=[\"search\", \"calculate\"],\n        rate_limit=100,\n    ),\n)\n</code></pre>"},{"location":"context/#testing-with-context","title":"Testing with Context","text":"<pre><code>import pytest\nfrom unittest.mock import Mock\n\n@pytest.fixture\ndef test_context():\n    return MyContext(\n        user_id=\"test-user\",\n        db=Mock(),\n        cache=Mock(),\n    )\n\nasync def test_tool_with_context(test_context):\n    result = await get_user_orders.__wrapped__(ctx=test_context)\n    assert \"orders\" in result\n</code></pre>"},{"location":"context/#api-reference","title":"API Reference","text":""},{"location":"context/#runcontext","title":"RunContext","text":"<pre><code>@dataclass\nclass RunContext:\n    metadata: dict[str, Any] = field(default_factory=dict)\n\n    def get(self, key: str, default: Any = None) -&gt; Any:\n        \"\"\"Get metadata value by key.\"\"\"\n\n    def with_metadata(self, **kwargs) -&gt; RunContext:\n        \"\"\"Create new context with additional metadata.\"\"\"\n</code></pre>"},{"location":"context/#usage-patterns","title":"Usage Patterns","text":"Pattern Description Tool injection <code>def my_tool(ctx: RunContext)</code> Optional context <code>def my_tool(arg: str, ctx: RunContext = None)</code> Interceptor access <code>context.run_context</code> Metadata access <code>ctx.get(\"key\")</code> or <code>ctx.metadata[\"key\"]</code> Extend context <code>ctx.with_metadata(key=\"value\")</code>"},{"location":"core/","title":"Core Module","text":"<p>The <code>cogent.core</code> module provides foundational types, enums, utilities, and dependency injection used throughout the framework.</p>"},{"location":"core/#overview","title":"Overview","text":"<p>The core module defines: - Enums for status types, event types, and roles - Native message types compatible with all LLM providers - Utility functions for IDs, timestamps, etc. - RunContext for dependency injection (tools and interceptors) - Reactive utilities for event-driven systems (idempotency, retries, delays)</p> <pre><code>from cogent.core import (\n    # Enums\n    TaskStatus,\n    AgentStatus,\n    EventType,\n    Priority,\n    AgentRole,\n    # Context\n    RunContext,\n    EMPTY_CONTEXT,\n    # Utilities\n    generate_id,\n    now_utc,\n    # Reactive utilities\n    IdempotencyGuard,\n    RetryBudget,\n    emit_later,\n    jittered_delay,\n    Stopwatch,\n)\n</code></pre>"},{"location":"core/#enums","title":"Enums","text":""},{"location":"core/#taskstatus","title":"TaskStatus","text":"<p>Task lifecycle states:</p> <pre><code>from cogent.core import TaskStatus\n\nstatus = TaskStatus.RUNNING\n\n# Check state categories\nstatus.is_terminal()  # COMPLETED, FAILED, CANCELLED\nstatus.is_active()    # RUNNING, SPAWNING\n</code></pre> Status Description <code>PENDING</code> Task created, not yet scheduled <code>SCHEDULED</code> Task scheduled for execution <code>BLOCKED</code> Task waiting on dependencies <code>RUNNING</code> Task actively executing <code>SPAWNING</code> Task creating subtasks <code>COMPLETED</code> Task finished successfully <code>FAILED</code> Task failed with error <code>CANCELLED</code> Task was cancelled"},{"location":"core/#agentstatus","title":"AgentStatus","text":"<p>Agent lifecycle states:</p> <pre><code>from cogent.core import AgentStatus\n\nstatus = AgentStatus.THINKING\n\n# Check state categories\nstatus.is_available()  # Can accept new work (IDLE)\nstatus.is_working()    # Currently working (THINKING, ACTING)\n</code></pre> Status Description <code>IDLE</code> Agent ready for new work <code>THINKING</code> Agent reasoning/planning <code>ACTING</code> Agent executing tools <code>WAITING</code> Agent waiting for response <code>ERROR</code> Agent in error state <code>OFFLINE</code> Agent unavailable"},{"location":"core/#eventtype","title":"EventType","text":"<p>System events for observability and coordination:</p> <pre><code>from cogent.core import EventType\n\nevent = EventType.TASK_COMPLETED\nevent.category  # \"task\"\n</code></pre> <p>Categories:</p> Category Examples <code>system</code> SYSTEM_STARTED, SYSTEM_STOPPED, SYSTEM_ERROR <code>task</code> TASK_CREATED, TASK_STARTED, TASK_COMPLETED, TASK_FAILED <code>subtask</code> SUBTASK_SPAWNED, SUBTASK_COMPLETED <code>agent</code> AGENT_THINKING, AGENT_ACTING, AGENT_RESPONDED <code>tool</code> TOOL_CALLED, TOOL_RESULT, TOOL_ERROR <code>llm</code> LLM_REQUEST, LLM_RESPONSE, LLM_TOOL_DECISION <code>stream</code> STREAM_START, TOKEN_STREAMED, STREAM_END <code>plan</code> PLAN_CREATED, PLAN_STEP_STARTED, PLAN_STEP_COMPLETED <code>message</code> MESSAGE_SENT, MESSAGE_RECEIVED"},{"location":"core/#priority","title":"Priority","text":"<p>Task priority levels (comparable):</p> <pre><code>from cogent.core import Priority\n\n# Priorities are comparable\nPriority.HIGH &gt; Priority.NORMAL  # True\nPriority.LOW &lt; Priority.CRITICAL  # True\n</code></pre> Priority Value Description <code>LOW</code> 1 Background tasks <code>NORMAL</code> 2 Standard priority <code>HIGH</code> 3 Important tasks <code>CRITICAL</code> 4 Urgent tasks"},{"location":"core/#agentrole","title":"AgentRole","text":"<p>Agent roles define capabilities:</p> <pre><code>from cogent.core import AgentRole\n\nrole = AgentRole.SUPERVISOR\nrole.can_finish     # True\nrole.can_delegate   # True\nrole.can_use_tools  # False\n</code></pre> Role can_finish can_delegate can_use_tools <code>WORKER</code> \u274c \u274c \u2705 <code>SUPERVISOR</code> \u2705 \u2705 \u274c <code>AUTONOMOUS</code> \u2705 \u274c \u2705 <code>REVIEWER</code> \u2705 \u274c \u274c"},{"location":"core/#messages","title":"Messages","text":"<p>Native message types compatible with OpenAI/Anthropic/etc. APIs:</p>"},{"location":"core/#systemmessage","title":"SystemMessage","text":"<pre><code>from cogent.core.messages import SystemMessage\n\nmsg = SystemMessage(\"You are a helpful assistant.\")\nmsg.to_dict()  # {\"role\": \"system\", \"content\": \"...\"}\n</code></pre>"},{"location":"core/#humanmessage","title":"HumanMessage","text":"<pre><code>from cogent.core.messages import HumanMessage\n\nmsg = HumanMessage(\"Hello, how are you?\")\nmsg.to_dict()  # {\"role\": \"user\", \"content\": \"...\"}\n</code></pre>"},{"location":"core/#aimessage","title":"AIMessage","text":"<pre><code>from cogent.core.messages import AIMessage\n\n# Text response\nmsg = AIMessage(content=\"I'm doing well!\")\n\n# Response with tool calls\nmsg = AIMessage(\n    content=\"\",\n    tool_calls=[\n        {\"id\": \"call_1\", \"name\": \"search\", \"args\": {\"query\": \"weather\"}}\n    ],\n)\nmsg.to_dict()  # Includes properly formatted tool_calls\n</code></pre>"},{"location":"core/#toolmessage","title":"ToolMessage","text":"<pre><code>from cogent.core.messages import ToolMessage\n\nmsg = ToolMessage(\n    content='{\"temperature\": 72}',\n    tool_call_id=\"call_1\",\n)\n</code></pre>"},{"location":"core/#helper-functions","title":"Helper Functions","text":"<pre><code>from cogent.core.messages import (\n    messages_to_dict,\n    parse_openai_response,\n)\n\n# Convert message list for API calls\nmessages = [SystemMessage(\"...\"), HumanMessage(\"...\")]\napi_messages = messages_to_dict(messages)\n\n# Parse OpenAI response into AIMessage\nai_msg = parse_openai_response(openai_response)\n</code></pre>"},{"location":"core/#context-dependency-injection","title":"Context (Dependency Injection)","text":""},{"location":"core/#runcontext","title":"RunContext","text":"<p>Base class for invocation-scoped context that provides dependency injection for tools and interceptors.</p> <pre><code>from dataclasses import dataclass\nfrom cogent import Agent, tool\nfrom cogent.core import RunContext\n\n@dataclass\nclass AppContext(RunContext):\n    user_id: str\n    db: Any  # Your database connection\n    api_key: str\n\n@tool\ndef get_user_data(ctx: RunContext) -&gt; str:\n    \"\"\"Get data for the current user.\"\"\"\n    user = ctx.db.get_user(ctx.user_id)\n    return f\"User: {user.name}\"\n\nagent = Agent(name=\"assistant\", model=model, tools=[get_user_data])\n\n# Pass context at invocation time\nresult = await agent.run(\n    \"Get my profile data\",\n    context=AppContext(user_id=\"123\", db=db, api_key=key),\n)\n</code></pre> <p>Key Features: - Type-safe context data passed to tools and interceptors - No global state \u2014 context scoped to single invocation - Access via <code>ctx: RunContext</code> parameter in tools - Available in interceptors via <code>InterceptContext.run_context</code></p> <p>Methods: - <code>get(key, default)</code> \u2014 Get metadata value by key - <code>with_metadata(**kwargs)</code> \u2014 Create new context with additional metadata</p>"},{"location":"core/#reactive-utilities","title":"Reactive Utilities","text":"<p>Event-driven utilities for building robust reactive systems.</p>"},{"location":"core/#idempotencyguard","title":"IdempotencyGuard","text":"<p>In-memory idempotency guard to ensure side-effects execute only once per key:</p> <pre><code>from cogent.core import IdempotencyGuard\n\nguard = IdempotencyGuard()\n\nasync def process_event(event_id: str, data: dict):\n    if not await guard.claim(event_id):\n        return  # Already processed\n\n    # Process event exactly once\n    await do_work(data)\n</code></pre> <p>Methods: - <code>claim(key: str) -&gt; bool</code> \u2014 Atomically claim a key (returns True if first time) - <code>run_once(key: str, fn: Callable) -&gt; tuple[bool, Any]</code> \u2014 Run coroutine exactly once per key</p> <p>Note: Process-local memory. For distributed systems, back with Redis/database.</p>"},{"location":"core/#retrybudget","title":"RetryBudget","text":"<p>Bounded retry tracker for exponential backoff and retry policies:</p> <pre><code>from cogent.core import RetryBudget\n\nbudget = RetryBudget.in_memory(max_attempts=3)\n\nasync def handle_with_retries(task_id: str):\n    attempt = budget.next_attempt(task_id)\n\n    if attempt &gt;= 3:\n        # Escalate to error handler\n        await escalate_error(task_id)\n        return\n\n    # Try again\n    await retry_task(task_id)\n</code></pre> <p>Methods: - <code>in_memory(max_attempts: int) -&gt; RetryBudget</code> \u2014 Create in-memory tracker - <code>next_attempt(key: str) -&gt; int</code> \u2014 Increment and return attempt count (0-based) - <code>can_retry(key: str) -&gt; bool</code> \u2014 Check if more retries available</p>"},{"location":"core/#jittered_delay","title":"jittered_delay","text":"<p>Calculate exponential backoff with jitter:</p> <pre><code>from cogent.core import jittered_delay\nimport random\n\nattempt = 2\nbase_delay = 2 ** attempt  # 4 seconds\njitter = random.uniform(-1, 1)\n\ndelay = jittered_delay(\n    base_seconds=base_delay,\n    jitter_seconds=jitter,\n    min_seconds=1.0,\n    max_seconds=60.0,\n)\n\nawait asyncio.sleep(delay)\n</code></pre>"},{"location":"core/#stopwatch","title":"Stopwatch","text":"<p>Performance timing helper:</p> <pre><code>from cogent.core import Stopwatch\n\nstopwatch = Stopwatch()\n\nawait do_work()\n\nelapsed = stopwatch.elapsed_s\nprint(f\"Work completed in {elapsed:.2f}s\")\n</code></pre>"},{"location":"core/#utilities","title":"Utilities","text":""},{"location":"core/#generate_id","title":"generate_id","text":"<p>Generate unique identifiers:</p> <pre><code>from cogent.core import generate_id\n\ntask_id = generate_id()  # \"task_a1b2c3d4\"\nagent_id = generate_id(prefix=\"agent\")  # \"agent_e5f6g7h8\"\n</code></pre>"},{"location":"core/#timestamps","title":"Timestamps","text":"<pre><code>from cogent.core import now_utc, now_local, to_local, format_timestamp\n\n# Get current time\nutc_now = now_utc()      # datetime in UTC\nlocal_now = now_local()  # datetime in local timezone\n\n# Convert UTC to local\nlocal_time = to_local(utc_now)\n\n# Format for display\nformatted = format_timestamp(utc_now)  # \"2024-12-04 10:30:45 UTC\"\n</code></pre>"},{"location":"core/#response-protocol","title":"Response Protocol","text":"<p>New in v1.13.0 \u2014 Unified response protocol for all agent operations with consistent metadata, observability, and error handling.</p>"},{"location":"core/#responset","title":"Response[T]","text":"<p>Generic container for agent responses with full metadata:</p> <pre><code>from cogent import Agent\nfrom cogent.core.response import Response\n\nagent = Agent(name=\"analyst\", model=model)\n\n# Agent.run() and Agent.think() return Response[T]\nresponse = await agent.think(\"Analyze sales data\")\n\n# Access response data\nresult: str = response.content\nsuccess: bool = response.success\n\n# Access metadata\ntokens = response.metadata.tokens.total_tokens\nduration = response.metadata.duration\nmodel = response.metadata.model\ncorrelation_id = response.metadata.correlation_id\n\n# Access tool calls with timing\nfor tool_call in response.tool_calls:\n    print(f\"{tool_call.tool_name}: {tool_call.duration}s\")\n\n# Access conversation history\nfor message in response.messages:\n    print(f\"{message.role}: {message.content}\")\n\n# Unwrap or handle errors\ntry:\n    result = response.unwrap()  # Returns content or raises\nexcept ResponseError as e:\n    print(f\"Error: {e.response.error.message}\")\n</code></pre>"},{"location":"core/#response-types","title":"Response Types","text":""},{"location":"core/#tokenusage","title":"TokenUsage","text":"<p>Token consumption tracking:</p> <pre><code>from cogent.core.response import TokenUsage\n\ntokens = response.metadata.tokens\nprint(f\"Prompt: {tokens.prompt_tokens}\")\nprint(f\"Completion: {tokens.completion_tokens}\")\nprint(f\"Total: {tokens.total_tokens}\")\n</code></pre>"},{"location":"core/#toolcall","title":"ToolCall","text":"<p>Tool invocation tracking with timing:</p> <pre><code>from cogent.core.response import ToolCall\n\nfor call in response.tool_calls:\n    print(f\"Tool: {call.tool_name}\")\n    print(f\"Duration: {call.duration}s\")\n    print(f\"Success: {call.success}\")\n    if call.error:\n        print(f\"Error: {call.error}\")\n</code></pre>"},{"location":"core/#responsemetadata","title":"ResponseMetadata","text":"<p>Consistent metadata across all responses:</p> <pre><code>from cogent.core.response import ResponseMetadata\n\nmetadata = response.metadata\n# agent: Agent that generated response\n# model: Model used (e.g., \"gpt-4\")\n# tokens: Token usage (if available)\n# duration: Execution time in seconds\n# timestamp: Unix timestamp\n# correlation_id: For distributed tracing\n# trace_id: Trace identifier\n</code></pre>"},{"location":"core/#errorinfo","title":"ErrorInfo","text":"<p>Structured error information:</p> <pre><code>from cogent.core.response import ErrorInfo\n\nif not response.success:\n    error = response.error\n    print(f\"Type: {error.type}\")\n    print(f\"Message: {error.message}\")\n    if error.traceback:\n        print(f\"Traceback: {error.traceback}\")\n</code></pre>"},{"location":"core/#response-features","title":"Response Features","text":""},{"location":"core/#serialization","title":"Serialization","text":"<pre><code># Convert to dictionary\ndata = response.to_dict()\n# {\n#     \"content\": \"...\",\n#     \"success\": true,\n#     \"metadata\": {...},\n#     \"tool_calls\": [...],\n#     \"messages\": [...],\n#     ...\n# }\n</code></pre>"},{"location":"core/#event-conversion","title":"Event Conversion","text":"<pre><code>from cogent.events import Event\n\n# Create event from response\nevent = Event.from_response(\n    response,\n    name=\"analysis.done\",\n    source=\"analyst\",\n)\n\n# Event includes response metadata\nassert event.data[\"content\"] == response.content\nassert event.metadata[\"tokens\"][\"total\"] == response.metadata.tokens.total_tokens\n</code></pre>"},{"location":"core/#benefits","title":"Benefits","text":"<p>Observability: - Full conversation history with <code>response.messages</code> - Token usage tracking across all operations - Tool call timing and success/failure tracking - Correlation IDs for distributed tracing</p> <p>Debugging: - Inspect exact prompts sent to LLM - See all tool invocations with results - Track execution timing per operation - Access full error context with tracebacks</p> <p>Consistency: - Same response format for <code>Agent.run()</code>, <code>Agent.think()</code>, and A2A - Predictable error handling across all agent operations - Unified metadata structure</p> <p>Integration: - Seamless conversion to Events for orchestration - Works with all executor types (Native, ReAct, ChainOfThought)</p>"},{"location":"core/#exports","title":"Exports","text":"<pre><code>from cogent.core import (\n    # Enums\n    TaskStatus,\n    AgentStatus,\n    EventType,\n    Priority,\n    AgentRole,\n    # Context\n    RunContext,\n    EMPTY_CONTEXT,\n    # Utilities\n    generate_id,\n    now_utc,\n    now_local,\n    to_local,\n    format_timestamp,\n    # Reactive utilities\n    IdempotencyGuard,\n    RetryBudget,\n    emit_later,\n    jittered_delay,\n    Stopwatch,\n)\n\nfrom cogent.core.messages import (\n    BaseMessage,\n    SystemMessage,\n    HumanMessage,\n    AIMessage,\n    ToolMessage,\n    messages_to_dict,\n    parse_openai_response,\n)\n\nfrom cogent.core.response import (\n    Response,\n    ResponseMetadata,\n    TokenUsage,\n    ToolCall,\n    ErrorInfo,\n    ResponseError,\n)\n</code></pre>"},{"location":"document/","title":"Document Module","text":"<p>The <code>cogent.documents</code> module provides comprehensive document loading, text splitting, and summarization capabilities for RAG (Retrieval Augmented Generation) systems.</p>"},{"location":"document/#overview","title":"Overview","text":"<p>The document module includes: - Document &amp; DocumentMetadata: Type-safe document structure with rich metadata - Loaders: Load documents from various file formats with automatic metadata population - Splitters: Chunk text for embedding and retrieval while preserving metadata - Summarizers: Handle documents exceeding LLM context limits</p> <pre><code>from cogent.documents import (\n    Document,\n    DocumentMetadata,\n    DocumentLoader,\n    RecursiveCharacterSplitter,\n)\n\n# Load documents - metadata automatically populated\nloader = DocumentLoader()\ndocs = await loader.load_directory(\"./documents\")\n\n# Access structured metadata\nfor doc in docs:\n    print(f\"Source: {doc.source}\")\n    print(f\"Type: {doc.metadata.source_type}\")\n    print(f\"Page: {doc.metadata.page}\")\n\n# Split into chunks - metadata preserved\nsplitter = RecursiveCharacterSplitter(chunk_size=1000, chunk_overlap=200)\nchunks = splitter.split_documents(docs)\n</code></pre>"},{"location":"document/#document-metadata","title":"Document &amp; Metadata","text":""},{"location":"document/#document","title":"Document","text":"<p>Core document type used throughout cogent:</p> <pre><code>from cogent.documents import Document, DocumentMetadata\n\n# Create with structured metadata\ndoc = Document(\n    text=\"Document content...\",\n    metadata=DocumentMetadata(\n        source=\"report.pdf\",\n        source_type=\"pdf\",\n        page=5,\n        loader=\"PDFMarkdownLoader\"\n    )\n)\n\n# Convenience properties\nprint(doc.id)        # doc_a1b2c3d4e5f6g7h8\nprint(doc.source)    # report.pdf\nprint(doc.page)      # 5\n\n# Access full metadata\nprint(doc.metadata.char_count)  # Auto-populated\nprint(doc.metadata.timestamp)   # Creation time\n</code></pre>"},{"location":"document/#documentmetadata","title":"DocumentMetadata","text":"<p>Structured metadata with type safety and IDE autocomplete:</p> <pre><code>@dataclass\nclass DocumentMetadata:\n    \"\"\"Structured metadata for documents with provenance and tracking.\"\"\"\n\n    # Identification &amp; timing\n    id: str                        # Auto-generated: doc_a1b2c3d4\n    timestamp: float               # Unix timestamp\n\n    # Source information\n    source: str                    # File path, URL, etc.\n    source_type: str | None        # \"pdf\", \"markdown\", \"web\", \"api\"\n\n    # Content positioning (for chunked documents)\n    page: int | None               # Page number\n    chunk_index: int | None        # Chunk position (0-based)\n    chunk_total: int | None        # Total chunks from parent\n    start_char: int | None         # Start position in parent\n    end_char: int | None           # End position in parent\n\n    # Content metrics\n    token_count: int | None        # Token count (if computed)\n    char_count: int | None         # Character count (auto-populated)\n\n    # Provenance &amp; relationships\n    loader: str | None             # Loader that created document\n    created_by: str | None         # Agent/tool name\n    parent_id: str | None          # Parent document ID (for chunks)\n\n    # Custom fields (extensibility)\n    custom: dict[str, Any]         # User-defined metadata\n</code></pre> <p>Key Features:</p> <ol> <li>Type Safety: Properties with proper types, no dict key errors</li> <li>Auto-population: <code>char_count</code>, <code>id</code>, <code>timestamp</code> set automatically</li> <li>Provenance: Track <code>loader</code>, <code>created_by</code>, <code>parent_id</code> for observability</li> <li>Chunking-aware: <code>chunk_index</code>, <code>chunk_total</code>, <code>parent_id</code> for chunk relationships</li> <li>Extensible: Use <code>custom</code> dict for application-specific fields</li> </ol> <p>Example - Chunk Metadata:</p> <pre><code># Loaders automatically populate metadata\ndocs = await PDFMarkdownLoader().load(\"report.pdf\")\ndoc = docs[0]\n\nprint(doc.metadata.loader)       # \"PDFMarkdownLoader\"\nprint(doc.metadata.source_type)  # \"pdf\"\nprint(doc.metadata.page)         # 1\n\n# Splitters preserve and extend metadata\nsplitter = RecursiveCharacterSplitter(chunk_size=500)\nchunks = splitter.split_documents(docs)\n\nchunk = chunks[0]\nprint(chunk.metadata.parent_id)    # ID of original document\nprint(chunk.metadata.chunk_index)  # 0\nprint(chunk.metadata.chunk_total)  # 10\nprint(chunk.metadata.start_char)   # 0\nprint(chunk.metadata.end_char)     # 500\n</code></pre> <p>Serialization:</p> <pre><code># To dict (for storage/API)\ndata = doc.to_dict()\n# {\n#   \"text\": \"...\",\n#   \"metadata\": {\n#     \"id\": \"doc_...\",\n#     \"source\": \"file.txt\",\n#     \"char_count\": 1234,\n#     ...\n#   }\n# }\n\n# From dict (backward compatible - collects unknown fields into custom)\ndoc = Document.from_dict({\n    \"text\": \"...\",\n    \"metadata\": {\n        \"source\": \"file.txt\",\n        \"custom_field\": \"value\"  # Goes into metadata.custom\n    }\n})\nprint(doc.metadata.custom[\"custom_field\"])  # \"value\"\n</code></pre>"},{"location":"document/#document-loaders","title":"Document Loaders","text":""},{"location":"document/#documentloader","title":"DocumentLoader","text":"<p>The main loader class that auto-detects file types:</p> <pre><code>from cogent.documents.loaders import DocumentLoader\n\nloader = DocumentLoader()\n\n# Load single file\ndocs = await loader.load(\"document.pdf\")\n\n# Load directory\ndocs = await loader.load_directory(\"./docs\", glob=\"**/*.md\")\n\n# Load with options\ndocs = await loader.load(\"data.csv\", encoding=\"utf-8\")\n</code></pre>"},{"location":"document/#supported-formats","title":"Supported Formats","text":"Format Extensions Loader Text .txt, .rst, .log <code>TextLoader</code> Markdown .md <code>MarkdownLoader</code> HTML .html, .htm <code>HTMLLoader</code> PDF .pdf <code>PDFLoader</code> Word .docx <code>WordLoader</code> CSV .csv <code>CSVLoader</code> JSON .json, .jsonl <code>JSONLoader</code> Excel .xlsx <code>XLSXLoader</code> Code .py, .js, .ts, etc. <code>CodeLoader</code>"},{"location":"document/#pdfmarkdownloader","title":"PDFMarkdownLoader","text":"<p>High-performance PDF loader optimized for LLM/RAG with parallel processing:</p> <pre><code>from cogent.documents.loaders import PDFMarkdownLoader\n\nloader = PDFMarkdownLoader(\n    max_workers=4,      # CPU workers for parallel processing\n    batch_size=10,      # Pages per batch\n)\n\n# Standard API - returns list[Document] (consistent with other loaders)\ndocs = await loader.load(\"large_document.pdf\")\nprint(f\"Loaded {len(docs)} pages\")\n\n# With tracking - returns PDFProcessingResult with metrics\nresult = await loader.load(\"large_document.pdf\", tracking=True)\nprint(f\"Success rate: {result.success_rate:.0%}\")\nprint(f\"Time: {result.total_time_ms:.0f}ms\")\ndocs = result.documents\n</code></pre> <p>PDFProcessingResult (from <code>load(tracking=True)</code>): <pre><code>@dataclass\nclass PDFProcessingResult:\n    file_path: Path\n    status: PDFProcessingStatus\n    total_pages: int\n    successful_pages: int\n    failed_pages: int\n    empty_pages: int\n    page_results: list[PageResult]\n    total_time_ms: float\n\n    @property\n    def success_rate(self) -&gt; float:\n        \"\"\"Returns ratio 0.0-1.0 for percentage formatting.\"\"\"\n        ...\n\n    @property\n    def documents(self) -&gt; list[Document]:\n        \"\"\"Convert page results to Document list.\"\"\"\n        ...\n</code></pre></p>"},{"location":"document/#convenience-functions","title":"Convenience Functions","text":"<pre><code>from cogent.documents.loaders import load_documents, load_documents_sync\n\n# Async loading\ndocs = await load_documents(\"./data\")\n\n# Sync loading (for scripts)\ndocs = load_documents_sync(\"./data\")\n</code></pre>"},{"location":"document/#custom-loaders","title":"Custom Loaders","text":"<p>Register custom file handlers:</p> <pre><code>from cogent.documents.loaders import BaseLoader, register_loader\n\nclass XMLLoader(BaseLoader):\n    EXTENSIONS = [\".xml\"]\n\n    async def load(self, path, **kwargs) -&gt; list[Document]:\n        # Custom loading logic\n        ...\n\nregister_loader(XMLLoader)\n</code></pre>"},{"location":"document/#text-splitters","title":"Text Splitters","text":""},{"location":"document/#recursivecharactersplitter","title":"RecursiveCharacterSplitter","text":"<p>The recommended splitter for most use cases:</p> <pre><code>from cogent.documents.splitters import RecursiveCharacterSplitter\n\nsplitter = RecursiveCharacterSplitter(\n    chunk_size=1000,      # Target chunk size\n    chunk_overlap=200,    # Overlap between chunks\n    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],  # Hierarchy\n)\n\nchunks = splitter.split_text(text)\nchunks = splitter.split_documents(docs)\n</code></pre>"},{"location":"document/#available-splitters","title":"Available Splitters","text":"Splitter Use Case <code>RecursiveCharacterSplitter</code> General text, preserves structure <code>CharacterSplitter</code> Simple single-separator splitting <code>SentenceSplitter</code> Sentence boundary detection <code>MarkdownSplitter</code> Markdown structure-aware <code>HTMLSplitter</code> HTML tag-based splitting <code>CodeSplitter</code> Language-aware code splitting <code>SemanticSplitter</code> Embedding-based semantic chunking <code>TokenSplitter</code> Token count-based (for LLMs)"},{"location":"document/#sentencesplitter","title":"SentenceSplitter","text":"<p>Split by sentence boundaries:</p> <pre><code>from cogent.documents.splitters import SentenceSplitter\n\nsplitter = SentenceSplitter(\n    chunk_size=1000,\n    chunk_overlap=100,\n)\n\nchunks = splitter.split_text(text)\n</code></pre>"},{"location":"document/#markdownsplitter","title":"MarkdownSplitter","text":"<p>Preserve Markdown structure:</p> <pre><code>from cogent.documents.splitters import MarkdownSplitter\n\nsplitter = MarkdownSplitter(\n    chunk_size=1000,\n    headers_to_split_on=[\n        (\"#\", \"h1\"),\n        (\"##\", \"h2\"),\n        (\"###\", \"h3\"),\n    ],\n)\n\nchunks = splitter.split_text(markdown_text)\n</code></pre>"},{"location":"document/#codesplitter","title":"CodeSplitter","text":"<p>Language-aware code splitting:</p> <pre><code>from cogent.documents.splitters import CodeSplitter\n\nsplitter = CodeSplitter(\n    language=\"python\",\n    chunk_size=1000,\n)\n\nchunks = splitter.split_text(python_code)\n</code></pre> <p>Supported languages: Python, JavaScript, TypeScript, Java, C++, Go, Rust, Ruby, and more.</p>"},{"location":"document/#semanticsplitter","title":"SemanticSplitter","text":"<p>Split based on semantic similarity:</p> <pre><code>from cogent.documents.splitters import SemanticSplitter\n\nsplitter = SemanticSplitter(\n    embedding_model=my_embeddings,\n    breakpoint_threshold=0.5,  # Similarity threshold\n)\n\nchunks = await splitter.split_text(text)\n</code></pre>"},{"location":"document/#tokensplitter","title":"TokenSplitter","text":"<p>Split by token count (for LLM context limits):</p> <pre><code>from cogent.documents.splitters import TokenSplitter\n\nsplitter = TokenSplitter(\n    chunk_size=512,       # Max tokens per chunk\n    chunk_overlap=50,     # Token overlap\n    encoding=\"cl100k_base\",  # Tokenizer encoding\n)\n\nchunks = splitter.split_text(text)\n</code></pre>"},{"location":"document/#convenience-function","title":"Convenience Function","text":"<pre><code>from cogent.documents.splitters import split_text\n\nchunks = split_text(\n    text,\n    chunk_size=1000,\n    chunk_overlap=200,\n    splitter_type=\"recursive\",  # or \"sentence\", \"markdown\", etc.\n)\n</code></pre>"},{"location":"document/#document-type","title":"Document Type","text":"<p>The standard document container:</p> <pre><code>from cogent.documents import Document\n\ndoc = Document(\n    text=\"Document content...\",\n    metadata={\n        \"source\": \"file.pdf\",\n        \"page\": 1,\n        \"author\": \"John Doe\",\n    },\n)\n\n# Access\nprint(doc.text)\nprint(doc.metadata[\"source\"])\n</code></pre>"},{"location":"document/#textchunk","title":"TextChunk","text":"<p>Chunk with position information:</p> <pre><code>from cogent.documents import TextChunk\n\nchunk = TextChunk(\n    text=\"Chunk content...\",\n    start=0,\n    end=1000,\n    metadata={\"chunk_index\": 0},\n)\n</code></pre>"},{"location":"document/#document-summarization","title":"Document Summarization","text":"<p>For documents exceeding LLM context limits, use summarization strategies:</p>"},{"location":"document/#mapreducesummarizer","title":"MapReduceSummarizer","text":"<p>Parallel chunk summarization, then combine:</p> <pre><code>from cogent.documents.summarizer import MapReduceSummarizer\n\nsummarizer = MapReduceSummarizer(model=my_model)\nresult = await summarizer.summarize(long_text)\n</code></pre>"},{"location":"document/#refinesummarizer","title":"RefineSummarizer","text":"<p>Sequential refinement through chunks:</p> <pre><code>from cogent.documents.summarizer import RefineSummarizer\n\nsummarizer = RefineSummarizer(model=my_model)\nresult = await summarizer.summarize(long_text)\n</code></pre>"},{"location":"document/#hierarchicalsummarizer","title":"HierarchicalSummarizer","text":"<p>Tree-based recursive summarization:</p> <pre><code>from cogent.documents.summarizer import HierarchicalSummarizer\n\nsummarizer = HierarchicalSummarizer(\n    model=my_model,\n    levels=3,\n)\nresult = await summarizer.summarize(very_long_text)\n</code></pre>"},{"location":"document/#exports","title":"Exports","text":"<pre><code>from cogent.documents import (\n    # Types\n    Document,\n    TextChunk,\n    FileType,\n    SplitterType,\n    # Loaders\n    BaseLoader,\n    DocumentLoader,\n    TextLoader,\n    MarkdownLoader,\n    HTMLLoader,\n    PDFLoader,\n    PDFMarkdownLoader,\n    WordLoader,\n    CSVLoader,\n    JSONLoader,\n    XLSXLoader,\n    CodeLoader,\n    load_documents,\n    load_documents_sync,\n    register_loader,\n    # Splitters\n    BaseSplitter,\n    RecursiveCharacterSplitter,\n    CharacterSplitter,\n    SentenceSplitter,\n    MarkdownSplitter,\n    HTMLSplitter,\n    CodeSplitter,\n    SemanticSplitter,\n    TokenSplitter,\n    split_text,\n)\n\nfrom cogent.documents.loaders import (\n    # PDF LLM types\n    PDFProcessingResult,\n    PDFProcessingStatus,\n    PageResult,\n    PageStatus,\n    PDFConfig,\n    ProcessingMetrics,\n    OutputFormat,\n)\n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Get up and running with Cogent in minutes.</p>"},{"location":"getting-started/#installation","title":"Installation","text":"<pre><code># Minimal installation (core only)\npip install git+https://github.com/milad-o/cogent.git\n\n# Or with uv (recommended)\nuv add git+https://github.com/milad-o/cogent.git\n</code></pre> <p>Optional dependency groups:</p> <p>Choose what you need:</p> <pre><code># Vector stores (FAISS, Qdrant)\nuv add \"cogent[vector-stores] @ git+https://github.com/milad-o/cogent.git\"\n\n# Retrieval (BM25, rerankers)\nuv add \"cogent[retrieval] @ git+https://github.com/milad-o/cogent.git\"\n\n# Database backends (SQLAlchemy + drivers)\nuv add \"cogent[database] @ git+https://github.com/milad-o/cogent.git\"\n\n# Infrastructure (Redis)\nuv add \"cogent[infrastructure] @ git+https://github.com/milad-o/cogent.git\"\n\n# Web tools (search, scraping)\nuv add \"cogent[web] @ git+https://github.com/milad-o/cogent.git\"\n\n# LLM providers (Anthropic, Azure, Cohere, Groq)\nuv add \"cogent[all-providers] @ git+https://github.com/milad-o/cogent.git\"\n\n# All backends\nuv add \"cogent[all-backend] @ git+https://github.com/milad-o/cogent.git\"\n\n# Everything\nuv add \"cogent[all] @ git+https://github.com/milad-o/cogent.git\"\n</code></pre> <p>Development installation:</p> <pre><code># Development + testing\nuv add --dev cogent[dev,test,test-backends,docs]\n</code></pre>"},{"location":"getting-started/#quick-start","title":"Quick Start","text":""},{"location":"getting-started/#your-first-agent","title":"Your First Agent","text":"<pre><code>import asyncio\nfrom cogent import Agent, tool\n\n@tool\ndef get_weather(city: str) -&gt; str:\n    \"\"\"Get current weather for a city.\"\"\"\n    return f\"Weather in {city}: 72\u00b0F, sunny\"\n\nasync def main():\n    # Simple string model (recommended)\n    agent = Agent(\n        name=\"Assistant\",\n        model=\"gpt4\",  # Auto-resolves to gpt-4o\n        tools=[get_weather],\n    )\n\n    result = await agent.run(\"What's the weather in Tokyo?\")\n    print(result.output)\n\nasyncio.run(main())\n</code></pre> <p>Other model options: <pre><code># With provider prefix\nagent = Agent(name=\"Assistant\", model=\"anthropic:claude\")\n\n# Medium-level: Factory function\nfrom cogent.models import create_chat\nagent = Agent(name=\"Assistant\", model=create_chat(\"gpt4\"))\n\n# Low-level: Full control\nfrom cogent.models import OpenAIChat\nagent = Agent(name=\"Assistant\", model=OpenAIChat(model=\"gpt-4o\", temperature=0.7))\n</code></pre></p>"},{"location":"getting-started/#with-capabilities","title":"With Capabilities","text":"<p>Instead of defining individual tools, use pre-built capabilities:</p> <pre><code>from cogent import Agent\nfrom cogent.capabilities import FileSystem, WebSearch, CodeSandbox\n\nagent = Agent(\n    name=\"Assistant\",\n    model=\"gpt4\",  # Simple string model\n    capabilities=[\n        FileSystem(allowed_paths=[\"./project\"]),\n        WebSearch(),\n        CodeSandbox(timeout=30),\n    ],\n)\n\nresult = await agent.run(\"Search for Python tutorials and create a summary file\")\n</code></pre>"},{"location":"getting-started/#core-concepts","title":"Core Concepts","text":""},{"location":"getting-started/#agents","title":"Agents","text":"<p>Autonomous entities that think, act, and communicate:</p> <pre><code>agent = Agent(\n    name=\"Researcher\",\n    model=\"gpt4\",  # String model - auto-resolves to gpt-4o\n    instructions=\"You are a thorough researcher.\",\n    tools=[search, summarize],\n    verbose=True,\n)\n</code></pre>"},{"location":"getting-started/#tools","title":"Tools","text":"<p>Define tools with the <code>@tool</code> decorator:</p> <pre><code>from cogent import tool\n\n@tool\ndef search(query: str, max_results: int = 10) -&gt; str:\n    \"\"\"Search the web for information.\n\n    Args:\n        query: Search query string.\n        max_results: Maximum results to return.\n    \"\"\"\n    return f\"Found {max_results} results for: {query}\"\n\n# Async tools supported\n@tool\nasync def fetch_data(url: str) -&gt; str:\n    \"\"\"Fetch data from URL.\"\"\"\n    async with httpx.AsyncClient() as client:\n        response = await client.get(url)\n        return response.text\n</code></pre> <p>Tool features: - Automatic schema extraction from type hints - Docstring \u2192 description - Sync and async support - Context injection via <code>ctx: RunContext</code></p>"},{"location":"getting-started/#model-providers","title":"Model Providers","text":"<p>Cogent supports all major LLM providers with native SDK integrations:</p> <pre><code>from cogent.models import (\n    OpenAIChat,\n    AnthropicChat,\n    GeminiChat,\n    GroqChat,\n    OllamaChat,\n)\n\n# OpenAI\nmodel = OpenAIChat(model=\"gpt-4o\")\n\n# Anthropic Claude\nmodel = AnthropicChat(model=\"claude-sonnet-4-20250514\")\n\n# Google Gemini\nmodel = GeminiChat(model=\"gemini-2.0-flash-exp\")\n\n# Groq (fast inference)\nmodel = GroqChat(model=\"llama-3.3-70b-versatile\")\n\n# Ollama (local models)\nmodel = OllamaChat(model=\"llama3.2\")\n\n# Or use factory function\nfrom cogent.models import create_chat\n\nmodel = create_chat(\"anthropic\", model=\"claude-sonnet-4-20250514\")\n</code></pre>"},{"location":"getting-started/#streaming","title":"Streaming","text":"<p>Stream responses token-by-token:</p> <pre><code>agent = Agent(\n    name=\"Writer\",\n    model=model,\n    stream=True,\n)\n\nasync for chunk in agent.run_stream(\"Write a poem about AI\"):\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"getting-started/#memory-context","title":"Memory &amp; Context","text":"<p>Agents maintain conversation history automatically:</p> <pre><code>agent = Agent(\n    name=\"Assistant\",\n    model=model,\n    memory_enabled=True,  # Default: True\n)\n\n# First interaction\nawait agent.run(\"My name is Alice\")\n\n# Later interaction - agent remembers\nawait agent.run(\"What's my name?\")  # \"Your name is Alice\"\n\n# Clear memory\nagent.clear_memory()\n</code></pre>"},{"location":"getting-started/#observability","title":"Observability","text":"<p>Track execution with built-in observability:</p> <pre><code>from cogent.observability import Observer\n\n# Pre-configured observers\nobserver = Observer(level=\"trace\")      # Maximum detail\nobserver = Observer(level=\"verbose\")    # Key events\nobserver = Observer(level=\"minimal\")    # Errors only\n\n# Use observer with agent\nresult = await agent.run(\"Query\", observer=observer)\n\n# Access event history\nfor event in observer.history():\n    print(f\"{event.type}: {event.data}\")\n</code></pre>"},{"location":"getting-started/#interceptors","title":"Interceptors","text":"<p>Control execution with middleware:</p> <pre><code>from cogent.interceptors import BudgetGuard, RateLimiter, PIIShield\n\nagent = Agent(\n    name=\"Assistant\",\n    model=model,\n    interceptors=[\n        BudgetGuard(max_tokens=10000, max_cost=0.50),  # Token/cost limits\n        RateLimiter(max_requests_per_minute=60),        # Rate limiting\n        PIIShield(redact=True),                         # PII protection\n    ],\n)\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you know the basics, explore:</p> <ul> <li>Agent Documentation \u2014 Deep dive into agents, instructions, and configuration</li> <li>Multi-Agent \u2014 Build coordinated multi-agent systems</li> <li>Capabilities \u2014 Explore built-in tools and capabilities</li> <li>Graph Visualization \u2014 Visualize your agents</li> <li>RAG &amp; Retrieval \u2014 Build retrieval-augmented generation systems</li> <li>Examples \u2014 See working examples</li> </ul>"},{"location":"getting-started/#need-help","title":"Need Help?","text":"<ul> <li>Documentation: https://milad-o.github.io/cogent</li> <li>Examples: https://github.com/milad-o/cogent/tree/main/examples</li> <li>Issues: https://github.com/milad-o/cogent/issues</li> </ul>"},{"location":"graph/","title":"Graph Module","text":"<p>The <code>cogent.graph</code> module provides visualization for agents and their structure.</p>"},{"location":"graph/#overview","title":"Overview","text":"<p>Visualize any agent as a diagram:</p> <pre><code>from cogent import Agent\n\nagent = Agent(name=\"assistant\", model=model, tools=[search, write])\n\n# Get graph view\nview = agent.graph()\n\n# Render in any format\nprint(view.mermaid())   # Mermaid code\nprint(view.ascii())     # Terminal-friendly\nprint(view.dot())       # Graphviz DOT\nprint(view.url())       # mermaid.ink URL\nprint(view.html())      # Embeddable HTML\n\n# Save to file\nview.save(\"diagram.png\")\nview.save(\"diagram.svg\")\n</code></pre>"},{"location":"graph/#graphview-api","title":"GraphView API","text":"<p>All entities return a <code>GraphView</code> from their <code>.graph()</code> method:</p>"},{"location":"graph/#rendering-methods","title":"Rendering Methods","text":"<pre><code>view = agent.graph()\n\n# Mermaid diagram code\nmermaid_code = view.mermaid()\n\n# ASCII art for terminal\nascii_art = view.ascii()\n\n# Graphviz DOT format\ndot_code = view.dot()\n\n# mermaid.ink URL (shareable)\nurl = view.url()\n\n# Embeddable HTML\nhtml = view.html()\n</code></pre>"},{"location":"graph/#saving-diagrams","title":"Saving Diagrams","text":"<pre><code># Auto-detect format from extension\nview.save(\"diagram.png\")    # PNG image\nview.save(\"diagram.svg\")    # SVG vector\nview.save(\"diagram.mmd\")    # Mermaid source\nview.save(\"diagram.dot\")    # Graphviz DOT\nview.save(\"diagram.html\")   # HTML page\n</code></pre>"},{"location":"graph/#agent-graphs","title":"Agent Graphs","text":"<p>Visualize agent structure:</p> <pre><code>from cogent import Agent\nfrom cogent.capabilities import WebSearch, FileSystem\n\nagent = Agent(\n    name=\"researcher\",\n    model=model,\n    tools=[search, analyze, summarize],\n    capabilities=[WebSearch(), FileSystem()],\n)\n\nview = agent.graph()\nprint(view.mermaid())\n</code></pre>"},{"location":"graph/#configuration","title":"Configuration","text":""},{"location":"graph/#graphconfig","title":"GraphConfig","text":"<pre><code>from cogent.graph import GraphConfig, GraphTheme, GraphDirection\n\nconfig = GraphConfig(\n    direction=GraphDirection.TOP_DOWN,\n    theme=GraphTheme.DEFAULT,\n    show_tools=True,\n    show_capabilities=True,\n)\n\nview = agent.graph(config=config)\n</code></pre>"},{"location":"graph/#graphdirection","title":"GraphDirection","text":"Direction Description <code>TOP_DOWN</code> Vertical, top to bottom <code>LEFT_RIGHT</code> Horizontal, left to right <code>BOTTOM_UP</code> Vertical, bottom to top <code>RIGHT_LEFT</code> Horizontal, right to left"},{"location":"graph/#graphtheme","title":"GraphTheme","text":"Theme Description <code>DEFAULT</code> Standard colors <code>DARK</code> Dark background <code>FOREST</code> Green tones <code>NEUTRAL</code> Grayscale"},{"location":"graph/#execution-graphs","title":"Execution Graphs","text":"<p>Visualize execution traces:</p> <pre><code>from cogent.observability import ExecutionTracer\n\ntracer = ExecutionTracer()\nresult = await agent.run(\"Query\", tracer=tracer)\n\n# Get execution graph\nview = tracer.graph()\nprint(view.mermaid())\n</code></pre>"},{"location":"graph/#interactive-viewing","title":"Interactive Viewing","text":"<pre><code>view = agent.graph()\n\n# Open mermaid.ink in default browser\nview.open()\n\n# Or get URL to share\nurl = view.url()\n</code></pre>"},{"location":"graph/#api-reference","title":"API Reference","text":""},{"location":"graph/#graphview-methods","title":"GraphView Methods","text":"Method Returns Description <code>mermaid()</code> <code>str</code> Mermaid diagram code <code>ascii()</code> <code>str</code> ASCII art diagram <code>dot()</code> <code>str</code> Graphviz DOT code <code>url()</code> <code>str</code> mermaid.ink URL <code>html()</code> <code>str</code> Embeddable HTML <code>save(path)</code> <code>None</code> Save to file <code>open()</code> <code>None</code> Open in browser"},{"location":"interceptors/","title":"Interceptors Module","text":"<p>The <code>cogent.interceptors</code> module provides composable units that intercept agent execution for cross-cutting concerns like cost control, security, context management, and observability.</p>"},{"location":"interceptors/#overview","title":"Overview","text":"<p>Interceptors are middleware that wrap agent execution at specific phases: - Before LLM call - Modify context, filter tools, check budgets - After LLM call - Validate responses, mask PII, audit - Before tool call - Gate access, rate limit, retry logic - After tool call - Post-process results, aggregate data</p> <pre><code>from cogent import Agent\nfrom cogent.interceptors import BudgetGuard, PIIShield\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    intercept=[\n        BudgetGuard(max_model_calls=10, max_tool_calls=50),\n        PIIShield(patterns=[\"email\", \"ssn\"]),\n    ],\n)\n</code></pre>"},{"location":"interceptors/#core-concepts","title":"Core Concepts","text":""},{"location":"interceptors/#interceptor-lifecycle","title":"Interceptor Lifecycle","text":"<p>Interceptors run at specific phases:</p> <pre><code>User Query\n    \u2193\n[BEFORE_LLM] \u2190 Context modification, validation\n    \u2193\nLLM Call\n    \u2193\n[AFTER_LLM] \u2190 Response validation, PII masking\n    \u2193\n[BEFORE_TOOL] \u2190 Tool gating, rate limiting\n    \u2193\nTool Execution\n    \u2193\n[AFTER_TOOL] \u2190 Result post-processing\n    \u2193\nResponse to User\n</code></pre>"},{"location":"interceptors/#phase-enum","title":"Phase Enum","text":"<pre><code>from cogent.interceptors import Phase\n\nPhase.BEFORE_LLM    # Before sending to LLM\nPhase.AFTER_LLM     # After receiving LLM response\nPhase.BEFORE_TOOL   # Before tool execution\nPhase.AFTER_TOOL    # After tool execution\n</code></pre>"},{"location":"interceptors/#interceptresult","title":"InterceptResult","text":"<p>Interceptors return a result that can: - Continue - Proceed to next interceptor/phase - Modify - Change the data and continue - Stop - Halt execution with a response</p> <pre><code>from cogent.interceptors import InterceptResult\n\n# Continue unchanged\nreturn InterceptResult.continue_()\n\n# Modify and continue\nreturn InterceptResult.modify(new_messages=modified_messages)\n\n# Stop execution\nreturn InterceptResult.stop(response=\"Cannot proceed: budget exceeded\")\n</code></pre>"},{"location":"interceptors/#built-in-interceptors","title":"Built-in Interceptors","text":""},{"location":"interceptors/#budgetguard","title":"BudgetGuard","text":"<p>Control costs by limiting LLM and tool calls:</p> <pre><code>from cogent.interceptors import BudgetGuard\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    intercept=[\n        BudgetGuard(\n            max_model_calls=10,     # Max LLM invocations\n            max_tool_calls=50,      # Max tool executions\n            max_tokens=100000,      # Max token usage\n            on_exceeded=\"stop\",     # \"stop\" or \"warn\"\n        ),\n    ],\n)\n\n# Check budget status\nguard = agent.interceptors[0]\nprint(f\"Calls: {guard.model_calls}/{guard.max_model_calls}\")\nprint(f\"Tokens: {guard.tokens_used}/{guard.max_tokens}\")\n</code></pre>"},{"location":"interceptors/#piishield","title":"PIIShield","text":"<p>Detect and handle PII in inputs/outputs:</p> <pre><code>from cogent.interceptors import PIIShield, PIIAction\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    intercept=[\n        PIIShield(\n            patterns=[\"email\", \"phone\", \"ssn\", \"credit_card\"],\n            action=PIIAction.MASK,  # MASK, REDACT, or BLOCK\n        ),\n    ],\n)\n\n# Input: \"Contact john@email.com\"\n# Masked: \"Contact [EMAIL]\"\n</code></pre> <p>Actions:</p> Action Behavior <code>PIIAction.MASK</code> Replace with <code>[TYPE]</code> placeholder <code>PIIAction.REDACT</code> Remove entirely <code>PIIAction.BLOCK</code> Stop execution with error"},{"location":"interceptors/#contentfilter","title":"ContentFilter","text":"<p>Filter harmful or inappropriate content:</p> <pre><code>from cogent.interceptors import ContentFilter\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    intercept=[\n        ContentFilter(\n            block_patterns=[\"password\", \"secret key\"],\n            allow_patterns=[\"public api\"],  # Whitelist\n        ),\n    ],\n)\n</code></pre>"},{"location":"interceptors/#tokenlimiter","title":"TokenLimiter","text":"<p>Limit context size to fit model constraints:</p> <pre><code>from cogent.interceptors import TokenLimiter\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    intercept=[\n        TokenLimiter(\n            max_tokens=8000,        # Max context tokens\n            strategy=\"truncate\",   # \"truncate\" or \"summarize\"\n            keep_system=True,      # Always keep system message\n            keep_last_n=5,         # Keep last N messages\n        ),\n    ],\n)\n</code></pre>"},{"location":"interceptors/#contextcompressor","title":"ContextCompressor","text":"<p>Compress context to reduce token usage:</p> <pre><code>from cogent.interceptors import ContextCompressor\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    intercept=[\n        ContextCompressor(\n            model=model,           # LLM for summarization\n            trigger_tokens=6000,   # Compress when above this\n            target_tokens=3000,    # Target after compression\n        ),\n    ],\n)\n</code></pre>"},{"location":"interceptors/#tool-control","title":"Tool Control","text":""},{"location":"interceptors/#toolgate","title":"ToolGate","text":"<p>Control which tools are available:</p> <pre><code>from cogent.interceptors import ToolGate\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    tools=[search, write_file, delete_file],\n    intercept=[\n        ToolGate(\n            allow=[\"search\"],           # Only these tools\n            # Or: deny=[\"delete_file\"], # Block these tools\n        ),\n    ],\n)\n</code></pre> <p>Dynamic gating:</p> <pre><code>def gate_by_user(ctx: InterceptContext) -&gt; list[str]:\n    if ctx.run_context.get(\"is_admin\"):\n        return [\"*\"]  # All tools\n    return [\"search\", \"read_file\"]\n\nagent = Agent(\n    intercept=[ToolGate(allow=gate_by_user)],\n)\n</code></pre>"},{"location":"interceptors/#permissiongate","title":"PermissionGate","text":"<p>Role-based tool permissions:</p> <pre><code>from cogent.interceptors import PermissionGate\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    intercept=[\n        PermissionGate(\n            permissions={\n                \"admin\": [\"*\"],\n                \"user\": [\"search\", \"read\"],\n                \"guest\": [\"search\"],\n            },\n            get_role=lambda ctx: ctx.run_context.get(\"role\", \"guest\"),\n        ),\n    ],\n)\n</code></pre>"},{"location":"interceptors/#conversationgate","title":"ConversationGate","text":"<p>Enable tools based on conversation state:</p> <pre><code>from cogent.interceptors import ConversationGate\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    intercept=[\n        ConversationGate(\n            # Unlock tools after specific messages\n            unlock_on={\n                \"confirmed\": [\"execute_order\"],\n                \"authenticated\": [\"view_account\", \"transfer\"],\n            },\n        ),\n    ],\n)\n</code></pre>"},{"location":"interceptors/#resilience","title":"Resilience","text":""},{"location":"interceptors/#ratelimiter","title":"RateLimiter","text":"<p>Limit request rates:</p> <pre><code>from cogent.interceptors import RateLimiter\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    intercept=[\n        RateLimiter(\n            max_requests=10,       # Max requests\n            window_seconds=60,     # Per time window\n            on_exceeded=\"wait\",    # \"wait\" or \"error\"\n        ),\n    ],\n)\n</code></pre>"},{"location":"interceptors/#failover","title":"Failover","text":"<p>Automatic model failover:</p> <pre><code>from cogent.interceptors import Failover, FailoverTrigger\nfrom cogent.models import ChatModel\n\nagent = Agent(\n    name=\"assistant\",\n    model=primary_model,\n    intercept=[\n        Failover(\n            fallback_models=[\n                ChatModel(model=\"gpt-4o-mini\"),\n                ChatModel(model=\"gpt-3.5-turbo\"),\n            ],\n            triggers=[\n                FailoverTrigger.RATE_LIMIT,\n                FailoverTrigger.TIMEOUT,\n                FailoverTrigger.ERROR,\n            ],\n            max_retries=2,\n        ),\n    ],\n)\n</code></pre>"},{"location":"interceptors/#circuitbreaker","title":"CircuitBreaker","text":"<p>Prevent cascade failures:</p> <pre><code>from cogent.interceptors import CircuitBreaker\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    intercept=[\n        CircuitBreaker(\n            failure_threshold=5,   # Failures before opening\n            recovery_timeout=60,   # Seconds before retry\n            half_open_requests=2,  # Test requests when recovering\n        ),\n    ],\n)\n</code></pre>"},{"location":"interceptors/#toolguard","title":"ToolGuard","text":"<p>Per-tool retry and circuit breaker:</p> <pre><code>from cogent.interceptors import ToolGuard\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    intercept=[\n        ToolGuard(\n            tool_configs={\n                \"search\": {\n                    \"max_retries\": 3,\n                    \"backoff\": \"exponential\",\n                    \"circuit_breaker\": True,\n                },\n                \"database\": {\n                    \"max_retries\": 1,\n                    \"timeout\": 30,\n                },\n            },\n        ),\n    ],\n)\n</code></pre>"},{"location":"interceptors/#auditing","title":"Auditing","text":""},{"location":"interceptors/#auditor","title":"Auditor","text":"<p>Log all agent activity:</p> <pre><code>from cogent.interceptors import Auditor, AuditEventType\n\nasync def log_event(event):\n    print(f\"[{event.type}] {event.agent}: {event.data}\")\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    intercept=[\n        Auditor(\n            handler=log_event,\n            events=[\n                AuditEventType.LLM_REQUEST,\n                AuditEventType.LLM_RESPONSE,\n                AuditEventType.TOOL_CALL,\n                AuditEventType.TOOL_RESULT,\n            ],\n            include_content=True,  # Log message content\n        ),\n    ],\n)\n</code></pre>"},{"location":"interceptors/#prompt-adapters","title":"Prompt Adapters","text":""},{"location":"interceptors/#contextprompt","title":"ContextPrompt","text":"<p>Inject dynamic context into system prompt:</p> <pre><code>from cogent.interceptors import ContextPrompt\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    intercept=[\n        ContextPrompt(\n            template=\"\"\"Current time: {time}\nUser timezone: {timezone}\nUser preferences: {preferences}\"\"\",\n            get_context=lambda ctx: {\n                \"time\": datetime.now().isoformat(),\n                \"timezone\": ctx.run_context.get(\"timezone\", \"UTC\"),\n                \"preferences\": ctx.run_context.get(\"preferences\", {}),\n            },\n        ),\n    ],\n)\n</code></pre>"},{"location":"interceptors/#conversationprompt","title":"ConversationPrompt","text":"<p>Add conversation-aware context:</p> <pre><code>from cogent.interceptors import ConversationPrompt\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    intercept=[\n        ConversationPrompt(\n            summary_threshold=20,  # Summarize after N messages\n            include_summary=True,\n            model=model,\n        ),\n    ],\n)\n</code></pre>"},{"location":"interceptors/#lambdaprompt","title":"LambdaPrompt","text":"<p>Custom prompt modification:</p> <pre><code>from cogent.interceptors import LambdaPrompt\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    intercept=[\n        LambdaPrompt(\n            modifier=lambda messages, ctx: [\n                {**m, \"content\": m[\"content\"].upper()}\n                if m[\"role\"] == \"user\" else m\n                for m in messages\n            ],\n        ),\n    ],\n)\n</code></pre>"},{"location":"interceptors/#custom-interceptors","title":"Custom Interceptors","text":""},{"location":"interceptors/#basic-structure","title":"Basic Structure","text":"<pre><code>from cogent.interceptors import Interceptor, Phase, InterceptContext, InterceptResult\n\nclass MyInterceptor(Interceptor):\n    \"\"\"Custom interceptor example.\"\"\"\n\n    phases = [Phase.BEFORE_LLM, Phase.AFTER_LLM]\n\n    async def intercept(\n        self,\n        phase: Phase,\n        context: InterceptContext,\n    ) -&gt; InterceptResult:\n        if phase == Phase.BEFORE_LLM:\n            # Modify messages before LLM\n            messages = context.messages\n            messages.append({\"role\": \"system\", \"content\": \"Be concise.\"})\n            return InterceptResult.modify(new_messages=messages)\n\n        elif phase == Phase.AFTER_LLM:\n            # Log response\n            print(f\"Response: {context.response.content}\")\n            return InterceptResult.continue_()\n</code></pre>"},{"location":"interceptors/#interceptcontext","title":"InterceptContext","text":"<p>Available context in interceptors:</p> <pre><code>@dataclass\nclass InterceptContext:\n    agent: Agent                  # Current agent\n    phase: Phase                  # Current phase\n    messages: list[dict]          # Current messages\n    response: AIMessage | None    # LLM response (after phases)\n    tool_call: dict | None        # Tool call info (tool phases)\n    tool_result: Any | None       # Tool result (AFTER_TOOL)\n    run_context: RunContext       # User-provided context\n    metadata: dict                # Additional data\n</code></pre>"},{"location":"interceptors/#stateful-interceptors","title":"Stateful Interceptors","text":"<pre><code>class ConversationTracker(Interceptor):\n    \"\"\"Track conversation statistics.\"\"\"\n\n    phases = [Phase.AFTER_LLM]\n\n    def __init__(self):\n        self.message_count = 0\n        self.total_tokens = 0\n\n    async def intercept(\n        self,\n        phase: Phase,\n        context: InterceptContext,\n    ) -&gt; InterceptResult:\n        self.message_count += 1\n        if context.response and context.response.usage:\n            self.total_tokens += context.response.usage.get(\"total_tokens\", 0)\n        return InterceptResult.continue_()\n\n    def stats(self) -&gt; dict:\n        return {\n            \"messages\": self.message_count,\n            \"tokens\": self.total_tokens,\n        }\n</code></pre>"},{"location":"interceptors/#combining-interceptors","title":"Combining Interceptors","text":"<p>Interceptors execute in order. Use <code>StopExecution</code> to halt the chain:</p> <pre><code>from cogent.interceptors import StopExecution\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    intercept=[\n        # Order matters - these run sequentially\n        PIIShield(patterns=[\"ssn\"]),       # First: mask PII\n        BudgetGuard(max_model_calls=10),   # Second: check budget\n        ToolGate(allow=[\"search\"]),        # Third: filter tools\n        Auditor(handler=log),              # Last: audit all activity\n    ],\n)\n\n# If BudgetGuard exceeds limit, it raises StopExecution\n# and Auditor never runs for that call\n</code></pre>"},{"location":"interceptors/#api-reference","title":"API Reference","text":""},{"location":"interceptors/#core-classes","title":"Core Classes","text":"Class Description <code>Interceptor</code> Base class for all interceptors <code>InterceptContext</code> Context passed to interceptors <code>InterceptResult</code> Return type from intercept method <code>Phase</code> Enum of interception phases <code>StopExecution</code> Exception to halt execution"},{"location":"interceptors/#built-in-interceptors_1","title":"Built-in Interceptors","text":"Category Interceptors Budget <code>BudgetGuard</code> Security <code>PIIShield</code>, <code>ContentFilter</code> Context <code>TokenLimiter</code>, <code>ContextCompressor</code> Gates <code>ToolGate</code>, <code>PermissionGate</code>, <code>ConversationGate</code> Rate Limit <code>RateLimiter</code>, <code>ThrottleInterceptor</code> Resilience <code>Failover</code>, <code>CircuitBreaker</code>, <code>ToolGuard</code> Audit <code>Auditor</code> Prompts <code>ContextPrompt</code>, <code>ConversationPrompt</code>, <code>LambdaPrompt</code>"},{"location":"memory/","title":"Memory Module","text":"<p>The <code>cogent.memory</code> module provides a memory-first architecture where memory is a first-class citizen that can be wired to any agent.</p>"},{"location":"memory/#overview","title":"Overview","text":"<p>Memory enables agents to: - Persist knowledge across conversations - Share state between agents - Perform semantic search over memories - Scope memories by user, team, or conversation - ACC (Agentic Context Compression) \u2014 Bounded context for long conversations</p> <pre><code>from cogent import Agent\nfrom cogent.memory import Memory\n\n# Basic in-memory storage\nmemory = Memory()\nawait memory.remember(\"user_preference\", \"dark mode\")\nvalue = await memory.recall(\"user_preference\")\n\n# Wire to an agent\nagent = Agent(name=\"assistant\", model=model, memory=memory)\n\n# Memory with ACC enabled (prevents drift in long conversations)\nmemory = Memory(acc=True)\n</code></pre>"},{"location":"memory/#core-classes","title":"Core Classes","text":""},{"location":"memory/#memory","title":"Memory","text":"<p>The main memory interface with simple remember/recall API:</p> <pre><code>from cogent.memory import Memory\n\nmemory = Memory()\n\n# Remember a value\nawait memory.remember(\"key\", \"value\")\nawait memory.remember(\"user.name\", \"Alice\")\nawait memory.remember(\"conversation.topic\", \"AI research\")\n\n# Recall a value\nname = await memory.recall(\"user.name\")  # \"Alice\"\nmissing = await memory.recall(\"unknown\")  # None\nmissing = await memory.recall(\"unknown\", default=\"N/A\")  # \"N/A\"\n\n# Check existence\nexists = await memory.exists(\"user.name\")  # True\n\n# Delete a memory\nawait memory.forget(\"user.name\")\n\n# List all keys\nkeys = await memory.list_keys()  # [\"conversation.topic\"]\n\n# Clear all memories\nawait memory.clear()\n</code></pre>"},{"location":"memory/#scoped-memory","title":"Scoped Memory","text":"<p>Create isolated memory views for users, teams, or conversations:</p> <pre><code>from cogent.memory import Memory\n\nmemory = Memory()\n\n# Create scoped views\nuser_mem = memory.scoped(\"user:alice\")\nteam_mem = memory.scoped(\"team:research\")\nconv_mem = memory.scoped(\"conv:thread-123\")\n\n# Each scope is isolated\nawait user_mem.remember(\"preference\", \"compact\")\nawait team_mem.remember(\"preference\", \"detailed\")\n\nuser_pref = await user_mem.recall(\"preference\")  # \"compact\"\nteam_pref = await team_mem.recall(\"preference\")  # \"detailed\"\n\n# Scopes can be nested\nproject_mem = team_mem.scoped(\"project:alpha\")\nawait project_mem.remember(\"status\", \"active\")\n</code></pre>"},{"location":"memory/#shared-memory-between-agents","title":"Shared Memory Between Agents","text":"<p>Wire the same memory to multiple agents for shared knowledge:</p> <pre><code>from cogent import Agent\nfrom cogent.memory import Memory\n\n# Shared memory instance\nshared = Memory()\n\n# Both agents share the same memory\nresearcher = Agent(name=\"researcher\", model=model, memory=shared)\nwriter = Agent(name=\"writer\", model=model, memory=shared)\n\n# Researcher stores findings\nawait shared.remember(\"findings\", \"Key insight: AI adoption is growing\")\n\n# Writer can access them\nfindings = await shared.recall(\"findings\")\n</code></pre>"},{"location":"memory/#storage-backends","title":"Storage Backends","text":""},{"location":"memory/#inmemorystore-default","title":"InMemoryStore (Default)","text":"<p>Fast, no-persistence storage for development and testing:</p> <pre><code>from cogent.memory import Memory, InMemoryStore\n\n# Default - uses InMemoryStore\nmemory = Memory()\n\n# Explicit\nmemory = Memory(store=InMemoryStore())\n</code></pre>"},{"location":"memory/#sqlalchemystore","title":"SQLAlchemyStore","text":"<p>Persistent storage with SQLAlchemy 2.0 async support:</p> <pre><code>from cogent.memory import Memory, SQLAlchemyStore\n\n# SQLite (local file)\nstore = SQLAlchemyStore(\"sqlite+aiosqlite:///./memory.db\")\nmemory = Memory(store=store)\n\n# PostgreSQL\nstore = SQLAlchemyStore(\n    \"postgresql+asyncpg://user:pass@localhost/db\",\n    pool_size=10,\n)\nmemory = Memory(store=store)\n\n# Initialize tables (run once)\nawait store.initialize()\n\n# Cleanup\nawait store.close()\n</code></pre> <p>Context manager for cleanup:</p> <pre><code>async with SQLAlchemyStore(\"sqlite+aiosqlite:///./data.db\") as store:\n    memory = Memory(store=store)\n    await memory.remember(\"key\", \"value\")\n</code></pre>"},{"location":"memory/#redisstore","title":"RedisStore","text":"<p>Distributed cache with native TTL support:</p> <pre><code>from cogent.memory import Memory, RedisStore\n\nstore = RedisStore(\n    url=\"redis://localhost:6379\",\n    prefix=\"myapp:\",  # Key prefix\n    default_ttl=3600,  # 1 hour default TTL\n)\nmemory = Memory(store=store)\n\n# With TTL per key\nawait memory.remember(\"session\", {\"user\": \"alice\"}, ttl=1800)\n</code></pre>"},{"location":"memory/#memory-key-search","title":"Memory Key Search","text":"<p>Memory provides intelligent key search with three methods that automatically cascade:</p>"},{"location":"memory/#1-fuzzy-matching-default-fast-free","title":"1. Fuzzy Matching (Default - Fast &amp; Free)","text":"<p>The default search method uses fuzzy string matching for instant, offline key discovery:</p> <pre><code>from cogent import Agent\nfrom cogent.memory import Memory\n\n# No special setup needed - fuzzy matching works out of the box\nmemory = Memory()\n\nagent = Agent(name=\"assistant\", model=model, memory=memory)\n\n# Save memories\nawait agent.run(\"My name is Alice, I prefer dark mode, language is Python\")\n\n# Fuzzy matching finds similar keys instantly\nawait agent.run(\"What are my preferences?\")\n# \u2192 search_memories(\"preferences\") finds \"preferred_mode\" and \"preferred_language\"\n# Method: Fuzzy match (0.1ms, free, offline)\n</code></pre> <p>Benefits: - \u26a1 2,800\u00d7 faster than semantic search (0.1ms vs 280ms) - \ud83d\udcb0 Free - no API calls - \ud83d\udd0c Works offline - no network required - \ud83d\udcca 62.5% accuracy - good enough for most use cases - \ud83e\uddf9 Smart normalization - handles underscores, hyphens, word order</p> <p>How it works: <pre><code># String normalization helps matching:\n\"preferred_mode\" \u2192 \"preferred mode\"\n\"user_timezone\" \u2192 \"user timezone\"\n\"notification-settings\" \u2192 \"notification settings\"\n\n# Fuzzy matching finds similarity:\nQuery: \"preferences\" \u2192 Matches: \"preferred mode\", \"preferred language\"\nQuery: \"contact\" \u2192 Matches: \"email\", \"phone number\"\nQuery: \"settings\" \u2192 Matches: \"notification settings\"\n</code></pre></p>"},{"location":"memory/#2-semantic-search-optional-fallback","title":"2. Semantic Search (Optional Fallback)","text":"<p>Enable semantic search by adding a vectorstore (used when fuzzy matching unavailable):</p> <pre><code>from cogent import Agent\nfrom cogent.memory import Memory\nfrom cogent.vectorstore import VectorStore\n\n# Add vectorstore for semantic fallback\nmemory = Memory(vectorstore=VectorStore())\n\nagent = Agent(name=\"assistant\", model=model, memory=memory)\n</code></pre> <p>When semantic search is used: - Fuzzy matching library (rapidfuzz) not installed - Fuzzy matching finds no matches (&lt; 40% similarity)</p> <p>Trade-offs: - \u2705 75% accuracy - better than fuzzy (but only 12.5% improvement) - \u274c 280ms avg - 2,800\u00d7 slower than fuzzy - \u274c Costs money - OpenAI API calls - \u274c Requires network - API dependency</p>"},{"location":"memory/#3-keyword-search-final-fallback","title":"3. Keyword Search (Final Fallback)","text":"<p>Simple substring matching when all else fails:</p> <pre><code># Query: \"mode\" \u2192 Matches keys containing \"mode\": \"preferred_mode\", \"dark_mode\"\n</code></pre>"},{"location":"memory/#installation","title":"Installation","text":"<p>Recommended (fuzzy matching): <pre><code>uv add rapidfuzz  # For fast, free fuzzy matching\n</code></pre></p> <p>Optional (semantic fallback): <pre><code>from cogent.memory import Memory\nfrom cogent.vectorstore import VectorStore\n\nmemory = Memory(vectorstore=VectorStore())  # Enables semantic fallback\n</code></pre></p>"},{"location":"memory/#performance-comparison","title":"Performance Comparison","text":"Method Speed Accuracy Cost Offline Fuzzy 0.1ms 62.5% Free \u2705 Yes Semantic 280ms 75.0% $$ API \u274c No Keyword 0.1ms ~30% Free \u2705 Yes <p>Recommendation: Use fuzzy matching (default) for 99% of use cases.</p>"},{"location":"memory/#example","title":"Example","text":"<p>See examples/basics/memory_semantic_search.py for a complete demo.</p> <pre><code>from cogent import Agent\nfrom cogent.memory import Memory\n\nmemory = Memory()  # Fuzzy matching by default\n\nagent = Agent(name=\"assistant\", model=\"gpt-4o\", memory=memory)\n\n# Save with specific key names\nawait memory.remember(\"preferred_mode\", \"dark\")\nawait memory.remember(\"preferred_language\", \"Python\")\nawait memory.remember(\"email\", \"alice@example.com\")\n\n# Agent finds them with fuzzy matching (instant!)\nawait agent.run(\"What are my preferences?\")\n# \u2192 search_memories(\"preferences\") finds \"preferred_mode\" and \"preferred_language\"\n# \u26a1 0.1ms, free, offline\n\nawait agent.run(\"How can I contact the user?\")\n# \u2192 search_memories(\"contact\") finds \"email\"\n</code></pre>"},{"location":"memory/#memory-tools","title":"Memory Tools","text":"<p>Memory automatically exposes tools to agents for autonomous memory management:</p> <pre><code>from cogent import Agent\nfrom cogent.memory import Memory\n\n# Memory is always agentic - tools auto-added\nmemory = Memory()\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    memory=memory,\n)\n\n# Agent has 5 memory tools available:\n# 1. remember(key, value) - Save facts to long-term memory\n# 2. recall(key) - Retrieve specific facts\n# 3. forget(key) - Remove facts\n# 4. search_memories(query) - Search long-term facts (fuzzy matching by default)\n# 5. search_conversation(query) - Search conversation history\n\n# Agent can now use memory tools autonomously\nresult = await agent.run(\"Remember that my name is Alice\")\nresult = await agent.run(\"What's my name?\")\n</code></pre>"},{"location":"memory/#available-tools","title":"Available Tools","text":"<p>1. remember(key, value) - Save important facts <pre><code># Agent automatically calls when user shares information\nawait agent.run(\"My favorite language is Python\")\n# \u2192 Agent calls: remember(\"favorite_language\", \"Python\")\n</code></pre></p> <p>2. recall(key) - Retrieve specific saved facts <pre><code>await agent.run(\"What's my favorite language?\")\n# \u2192 Agent calls: recall(\"favorite_language\")\n</code></pre></p> <p>3. forget(key) - Remove facts (when user requests) <pre><code>await agent.run(\"Forget my favorite language\")\n# \u2192 Agent calls: forget(\"favorite_language\")\n</code></pre></p> <p>4. search_memories(query, k=5) - Search long-term facts with intelligent matching <pre><code># Default: Fast fuzzy matching (0.1ms, free, offline)\nmemory = Memory()\nawait agent.run(\"What are my preferences?\")\n# \u2192 Agent calls: search_memories(\"preferences\")\n# \u2192 Finds: \"preferred_mode\", \"preferred_language\" via fuzzy matching\n\n# Optional: Add vectorstore for semantic fallback\nmemory = Memory(vectorstore=VectorStore())\n# \u2192 Uses fuzzy matching first, falls back to semantic if needed\n</code></pre></p> <p>5. search_conversation(query, max_results=5) - Search conversation history <pre><code># Critical for long conversations exceeding context window\nawait agent.run(\"What were the three projects I mentioned earlier?\")\n# \u2192 Agent calls: search_conversation(\"three projects\")\n</code></pre></p>"},{"location":"memory/#when-tools-are-used","title":"When Tools Are Used","text":"<p>The agent's system prompt instructs it to:</p> <ol> <li>At conversation start \u2192 <code>search_memories(\"user\")</code> to recall context</li> <li>When user shares info \u2192 <code>remember(key, value)</code> immediately</li> <li>When asked about something \u2192 Search before saying \"I don't know\"</li> <li>For facts \u2192 <code>search_memories(query)</code> or <code>recall(key)</code></li> <li>For past conversation \u2192 <code>search_conversation(query)</code></li> <li>In long conversations \u2192 Use <code>search_conversation()</code> to find earlier context</li> </ol> <p>Shorthand - <code>memory=True</code> creates a Memory instance:</p> <pre><code># Shorthand for Memory()\nagent = Agent(name=\"assistant\", model=model, memory=True)\n</code></pre>"},{"location":"memory/#usage-patterns","title":"Usage Patterns","text":""},{"location":"memory/#conversation-history","title":"Conversation History","text":"<pre><code>from cogent.memory import Memory\n\nmemory = Memory()\n\nasync def chat(user_id: str, message: str) -&gt; str:\n    user_mem = memory.scoped(f\"user:{user_id}\")\n\n    # Load history\n    history = await user_mem.recall(\"history\", default=[])\n    history.append({\"role\": \"user\", \"content\": message})\n\n    # Get response (using agent)\n    response = await agent.run(message, history=history)\n\n    # Save updated history\n    history.append({\"role\": \"assistant\", \"content\": response})\n    await user_mem.remember(\"history\", history)\n\n    return response\n</code></pre>"},{"location":"memory/#team-knowledge-base","title":"Team Knowledge Base","text":"<pre><code>from cogent.memory import Memory, SQLAlchemyStore\nfrom cogent.vectorstore import VectorStore\n\n# Persistent team memory with search\nteam_memory = Memory(\n    store=SQLAlchemyStore(\"sqlite+aiosqlite:///./team.db\"),\n    vectorstore=VectorStore(),\n)\n\n# Store team knowledge\nawait team_memory.remember(\"policy:vacation\", \"Employees get 20 days PTO\")\nawait team_memory.remember(\"policy:remote\", \"Remote work allowed 3 days/week\")\nawait team_memory.remember(\"contact:hr\", \"hr@company.com\")\n\n# Search policies\nresults = await team_memory.search(\"time off work\", k=3)\n</code></pre>"},{"location":"memory/#agent-with-persistent-context","title":"Agent with Persistent Context","text":"<pre><code>from cogent import Agent\nfrom cogent.memory import Memory, SQLAlchemyStore\n\nstore = SQLAlchemyStore(\"sqlite+aiosqlite:///./agent.db\")\nmemory = Memory(store=store)\n\nagent = Agent(\n    name=\"assistant\",\n    model=model,\n    memory=memory,\n    instructions=\"\"\"You have access to persistent memory.\n    Use it to remember user preferences and context.\"\"\",\n)\n\n# First conversation\nawait agent.run(\"My favorite color is blue\")\n\n# Later conversation (same agent)\nawait agent.run(\"What's my favorite color?\")  # Recalls \"blue\"\n</code></pre>"},{"location":"memory/#store-protocol","title":"Store Protocol","text":"<p>Implement custom storage backends:</p> <pre><code>from typing import Protocol, Any\n\nclass Store(Protocol):\n    \"\"\"Protocol for memory storage backends.\"\"\"\n\n    async def get(self, key: str) -&gt; Any | None:\n        \"\"\"Get a value by key.\"\"\"\n        ...\n\n    async def set(self, key: str, value: Any, ttl: int | None = None) -&gt; None:\n        \"\"\"Set a value with optional TTL.\"\"\"\n        ...\n\n    async def delete(self, key: str) -&gt; bool:\n        \"\"\"Delete a key. Returns True if existed.\"\"\"\n        ...\n\n    async def exists(self, key: str) -&gt; bool:\n        \"\"\"Check if key exists.\"\"\"\n        ...\n\n    async def keys(self, pattern: str = \"*\") -&gt; list[str]:\n        \"\"\"List keys matching pattern.\"\"\"\n        ...\n\n    async def clear(self) -&gt; None:\n        \"\"\"Clear all keys.\"\"\"\n        ...\n</code></pre> <p>Custom implementation example:</p> <pre><code>class DynamoDBStore:\n    \"\"\"Custom DynamoDB backend.\"\"\"\n\n    def __init__(self, table_name: str):\n        self.table_name = table_name\n        self.client = boto3.resource(\"dynamodb\")\n        self.table = self.client.Table(table_name)\n\n    async def get(self, key: str) -&gt; Any | None:\n        response = self.table.get_item(Key={\"pk\": key})\n        item = response.get(\"Item\")\n        return item[\"value\"] if item else None\n\n    async def set(self, key: str, value: Any, ttl: int | None = None) -&gt; None:\n        item = {\"pk\": key, \"value\": value}\n        if ttl:\n            item[\"ttl\"] = int(time.time()) + ttl\n        self.table.put_item(Item=item)\n\n    # ... implement other methods\n\n# Use custom store\nmemory = Memory(store=DynamoDBStore(\"my-memories\"))\n</code></pre>"},{"location":"memory/#api-reference","title":"API Reference","text":""},{"location":"memory/#memory_1","title":"Memory","text":"Method Description <code>remember(key, value, ttl?)</code> Store a value <code>recall(key, default?)</code> Retrieve a value <code>forget(key)</code> Delete a value <code>exists(key)</code> Check if key exists <code>list_keys(pattern?)</code> List matching keys <code>clear()</code> Clear all memories <code>scoped(prefix)</code> Create scoped view <code>search(query, k?)</code> Semantic search (requires vectorstore)"},{"location":"memory/#stores","title":"Stores","text":"Store Use Case <code>InMemoryStore</code> Development, testing, ephemeral <code>SQLAlchemyStore</code> Persistent, ACID, SQL databases <code>RedisStore</code> Distributed, TTL, high-throughput"},{"location":"memory/#semantic-cache","title":"Semantic Cache","text":"<p>SemanticCache provides embedding-based caching with configurable similarity thresholds. When a query is \"close enough\" to a cached entry, return the cached result instead of making an expensive LLM or API call.</p> <p>Key Benefits: - 80%+ hit rates \u2014 Cache similar queries, not just exact matches - 7-10\u00d7 speedup \u2014 Cached responses return instantly - Cost reduction \u2014 Fewer API calls = lower costs - Automatic eviction \u2014 LRU policy and TTL expiration</p>"},{"location":"memory/#quick-start","title":"Quick Start","text":"<p>Enable caching with <code>cache=True</code>:</p> <pre><code>from cogent import Agent\n\nagent = Agent(\n    model=\"gpt-4o-mini\",\n    cache=True,  # Enable semantic cache with defaults\n)\n\n# First query\nawait agent.run(\"What are the best Python frameworks?\")\n\n# Similar query hits cache (instant!)\nawait agent.run(\"What are the top Python frameworks?\")\n</code></pre>"},{"location":"memory/#custom-configuration","title":"Custom Configuration","text":"<p>Pass a <code>SemanticCache</code> instance for custom settings:</p> <pre><code>from cogent import Agent\nfrom cogent.memory import SemanticCache\nfrom cogent.models import create_embedding\n\n# Create embedding model\nembed = create_embedding(\"openai\", \"text-embedding-3-small\")\n\nagent = Agent(\n    model=\"gpt-4o-mini\",\n    cache=SemanticCache(\n        embedding=embed,            # Embedding model (required for custom)\n        similarity_threshold=0.90,  # Stricter matching (default: 0.85)\n        max_entries=5000,           # Cache size (default: 10000)\n        default_ttl=3600,           # 1 hour TTL (default: 86400)\n    ),\n)\n</code></pre> <p>Similarity Threshold:</p> Threshold Behavior Use Case 0.95-1.0 Very strict, near-exact Deterministic outputs 0.85-0.95 Balanced, similar intent General purpose (default) 0.70-0.85 Loose, broad matching Exploratory queries"},{"location":"memory/#tool-level-caching","title":"Tool-Level Caching","text":"<p>Use <code>@tool(cache=True)</code> to cache expensive tool calls:</p> <pre><code>from cogent import Agent, tool\n\n@tool(cache=True)\nasync def search_products(query: str) -&gt; str:\n    \"\"\"Search products in the catalog.\"\"\"\n    return await product_api.search(query)\n\nagent = Agent(\n    model=\"gpt-4o-mini\",\n    tools=[search_products],\n    cache=True,  # Required \u2014 tools use agent's cache\n)\n\n# First call executes the tool\nawait agent.run(\"Find running shoes\")\n\n# Similar query hits cache\nawait agent.run(\"Show me running sneakers\")  # Cache hit!\n</code></pre> <p>See tool-building.md for more details.</p>"},{"location":"memory/#when-to-use","title":"When to Use","text":"Use Semantic Cache When Don't Use When User queries with variation Need exact-match guarantees"},{"location":"memory/#acc-agentic-context-compression","title":"ACC (Agentic Context Compression)","text":"<p>ACC provides bounded memory for long conversations, preventing context drift and memory poisoning.</p>"},{"location":"memory/#basic-usage","title":"Basic Usage","text":"<p>Enable ACC with <code>acc=True</code> on Memory or Agent:</p> <pre><code>from cogent import Agent\nfrom cogent.memory import Memory\n\n# Option 1: Enable on Agent\nagent = Agent(name=\"Assistant\", model=\"gpt-4o\", acc=True)\n\n# Option 2: Enable on Memory (then pass to Agent)\nmemory = Memory(acc=True)\nagent = Agent(name=\"Assistant\", model=\"gpt-4o\", memory=memory)\n</code></pre>"},{"location":"memory/#custom-acc-bounds","title":"Custom ACC Bounds","text":"<p>For fine-grained control, pass custom bounds directly:</p> <pre><code>from cogent import Agent\nfrom cogent.memory import Memory\nfrom cogent.memory.acc import AgentCognitiveCompressor\n\n# Create ACC with custom bounds\nacc = AgentCognitiveCompressor(\n    max_constraints=10,  # Rules, guidelines\n    max_entities=30,     # Facts, knowledge\n    max_actions=20,      # Past actions\n    max_context=15,      # Relevant context\n)\n\n# Pass to Memory\nmemory = Memory(acc=acc)\nagent = Agent(name=\"Assistant\", model=\"gpt-4o\", memory=memory)\n\n# Or pass directly to Agent\nagent = Agent(name=\"Assistant\", model=\"gpt-4o\", acc=acc)\n</code></pre>"},{"location":"memory/#thread-id-for-context-persistence","title":"Thread ID for Context Persistence","text":"<p>ACC requires <code>thread_id</code> to persist context across multiple <code>run()</code> calls:</p> <pre><code># Same thread_id = context persists\nawait agent.run(\"My name is Alice\", thread_id=\"session-1\")\nawait agent.run(\"What's my name?\", thread_id=\"session-1\")  # Remembers!\n\n# Different thread_id = fresh context\nawait agent.run(\"What's my name?\", thread_id=\"session-2\")  # Doesn't know\n</code></pre>"},{"location":"memory/#when-to-use-acc","title":"When to Use ACC","text":"Use ACC When Don't Use When Long conversations (&gt;10 turns) Short, one-off queries Need to prevent context drift Stateless operations Bounded memory is critical Need full conversation replay Multi-turn workflows Simple Q&amp;A <p>See acc.md for detailed ACC documentation. | Similar questions rephrased | Outputs must be deterministic | | Intent-based matching | Query structure matters | | High query volume | Low query volume |</p>"},{"location":"models/","title":"Models Module","text":"<p>The <code>cogent.models</code> module provides a 3-tier API for working with LLMs - from simple string-based models to full control with direct SDK access.</p>"},{"location":"models/#3-tier-model-api","title":"\ud83c\udfaf 3-Tier Model API","text":"<p>Cogent offers three levels of abstraction - choose based on your needs:</p>"},{"location":"models/#tier-1-high-level-string-models-recommended","title":"Tier 1: High-Level (String Models) \u2b50 Recommended","text":"<p>The simplest way to get started. Just use model name strings:</p> <pre><code>from cogent import Agent\n\n# Auto-resolves to gpt-4o\nagent = Agent(\"Helper\", model=\"gpt4\")\n\n# Auto-resolves to gemini-2.5-flash\nagent = Agent(\"Helper\", model=\"gemini\")\n\n# Auto-resolves to claude-sonnet-4\nagent = Agent(\"Helper\", model=\"claude\")\n\n# Provider prefix for explicit control\nagent = Agent(\"Helper\", model=\"anthropic:claude-opus-4\")\nagent = Agent(\"Helper\", model=\"openai:gpt-4o\")\n</code></pre> <p>30+ Model Aliases: - <code>gpt4</code>, <code>gpt4-mini</code>, <code>gpt4-turbo</code>, <code>gpt35</code> - <code>claude</code>, <code>claude-opus</code>, <code>claude-haiku</code> - <code>gemini</code>, <code>gemini-flash</code>, <code>gemini-pro</code> - <code>llama</code>, <code>llama-70b</code>, <code>llama-8b</code>, <code>mixtral</code> - <code>ollama</code></p> <p>API Key Loading (Priority Order): 1. Explicit <code>api_key=</code> parameter (highest) 2. Environment variables (includes <code>.env</code> when loaded) 3. Config file <code>cogent.toml</code> / <code>cogent.yaml</code> or <code>~/.cogent/config.*</code> (lowest)</p>"},{"location":"models/#tier-2-medium-level-factory-functions","title":"Tier 2: Medium-Level (Factory Functions)","text":"<p>For when you need a model instance without an agent. Supports 4 flexible usage patterns:</p> <pre><code>from cogent.models import create_chat\n\n# Pattern 1: Model name only (auto-detects provider)\nllm = create_chat(\"gpt-4o\")              # OpenAI\nllm = create_chat(\"gemini-2.5-pro\")      # Google Gemini\nllm = create_chat(\"claude-sonnet-4\")     # Anthropic\nllm = create_chat(\"llama-3.1-8b-instant\")  # Groq\nllm = create_chat(\"mistral-small-latest\")  # Mistral\n\n# Pattern 2: Provider:model syntax (explicit provider prefix)\nllm = create_chat(\"openai:gpt-4o\")\nllm = create_chat(\"gemini:gemini-2.5-flash\")\nllm = create_chat(\"anthropic:claude-sonnet-4-20250514\")\n\n# Pattern 3: Separate provider and model arguments\nllm = create_chat(\"openai\", \"gpt-4o\")\nllm = create_chat(\"gemini\", \"gemini-2.5-pro\")\nllm = create_chat(\"anthropic\", \"claude-sonnet-4\")\n\n# Pattern 4: With additional configuration\nllm = create_chat(\"gpt-4o\", temperature=0.7, max_tokens=1000)\nllm = create_chat(\"openai\", \"gpt-4o\", api_key=\"sk-custom...\")\n\n# Use the model\nresponse = await llm.ainvoke(\"What is 2+2?\")\nprint(response.content)\n</code></pre> <p>Auto-Detection: Patterns 1 and 2 automatically detect the provider from model name prefixes: - OpenAI: <code>gpt-</code>, <code>o1-</code>, <code>o3-</code>, <code>o4-</code>, <code>text-embedding-</code>, <code>gpt-audio</code>, <code>gpt-realtime</code>, <code>sora-</code> - Gemini: <code>gemini-</code>, <code>text-embedding-</code> - Anthropic: <code>claude-</code> - Mistral: <code>mistral-</code>, <code>ministral-</code>, <code>magistral-</code>, <code>devstral-</code>, <code>codestral-</code>, <code>voxtral-</code>, <code>ocr-</code> - Cohere: <code>command-</code>, <code>c4ai-aya-</code>, <code>embed-</code>, <code>rerank-</code> - Groq: <code>llama-</code>, <code>mixtral-</code>, <code>qwen-</code>, <code>deepseek-</code>, <code>gemma-</code> - Cloudflare: <code>@cf/</code></p>"},{"location":"models/#tier-3-low-level-direct-model-classes","title":"Tier 3: Low-Level (Direct Model Classes)","text":"<p>For maximum control over model configuration:</p> <pre><code>from cogent.models import OpenAIChat, AnthropicChat, GeminiChat\n\n# Full control over all parameters\nmodel = OpenAIChat(\n    model=\"gpt-4o\",\n    temperature=0.7,\n    max_tokens=2000,\n    api_key=\"sk-...\",\n    organization=\"org-...\",\n)\n\nmodel = GeminiChat(\n    model=\"gemini-2.5-flash\",\n    temperature=0.9,\n    api_key=\"...\",\n)\n\nmodel = AnthropicChat(\n    model=\"claude-sonnet-4-20250514\",\n    max_tokens=4096,\n    api_key=\"sk-ant-...\",\n)\n</code></pre> <p>When to Use Each Tier:</p> Tier Use Case Example Tier 1 (Strings) Quick prototyping, simple agents <code>Agent(model=\"gpt4\")</code> Tier 2 (Factory) Reusable model instances <code>create_chat(\"claude\")</code> Tier 3 (Direct) Custom config, advanced features <code>OpenAIChat(temperature=0.9)</code>"},{"location":"models/#configuration","title":"Configuration","text":""},{"location":"models/#env-file-recommended-for-development","title":".env File (Recommended for Development)","text":"<p>Create a <code>.env</code> file in your project root:</p> <pre><code># .env\nOPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\nGEMINI_API_KEY=AIza...\nGROQ_API_KEY=gsk_...\n</code></pre> <p>Cogent automatically loads <code>.env</code> files using <code>python-dotenv</code>.</p>"},{"location":"models/#model-overrides-environment-config","title":"Model Overrides (Environment + Config)","text":"<p>You can override default chat or embedding models via env vars or config files.</p> <p>Environment variables (highest): <pre><code>OPENAI_CHAT_MODEL=gpt-4.1\nOPENAI_EMBEDDING_MODEL=text-embedding-3-large\nGEMINI_CHAT_MODEL=gemini-2.5-flash\nGEMINI_EMBEDDING_MODEL=gemini-embedding-001\nMISTRAL_CHAT_MODEL=mistral-small-latest\nMISTRAL_EMBEDDING_MODEL=mistral-embed\nGROQ_CHAT_MODEL=llama-3.1-8b-instant\nCOHERE_CHAT_MODEL=command-a-03-2025\nCOHERE_EMBEDDING_MODEL=embed-english-v3.0\nCLOUDFLARE_CHAT_MODEL=@cf/meta/llama-3.1-8b-instruct\nCLOUDFLARE_EMBEDDING_MODEL=@cf/baai/bge-base-en-v1.5\nGITHUB_CHAT_MODEL=gpt-4.1\nGITHUB_EMBEDDING_MODEL=text-embedding-3-large\nOLLAMA_CHAT_MODEL=qwen2.5:7b\nOLLAMA_EMBEDDING_MODEL=nomic-embed-text\n</code></pre></p> <p>Config file (fallback): <pre><code>[models.openai]\nchat_model = \"gpt-4.1\"\nembedding_model = \"text-embedding-3-large\"\n</code></pre></p>"},{"location":"models/#config-file-recommended-for-production","title":"Config File (Recommended for Production)","text":"<p>Create a config file at one of these locations:</p> <p>TOML Format (<code>cogent.toml</code> or <code>~/.cogent/config.toml</code>):</p> <pre><code>[models]\ndefault = \"gpt4\"\n\n[models.openai]\napi_key = \"sk-...\"\norganization = \"org-...\"\n\n[models.anthropic]\napi_key = \"sk-ant-...\"\n\n[models.gemini]\napi_key = \"...\"\n\n[models.groq]\napi_key = \"gsk_...\"\n</code></pre> <p>YAML Format (<code>cogent.yaml</code> or <code>~/.cogent/config.yaml</code>):</p> <pre><code>models:\n  default: gpt4\n\n  openai:\n    api_key: sk-...\n    organization: org-...\n\n  anthropic:\n    api_key: sk-ant-...\n\n  gemini:\n    api_key: ...\n</code></pre>"},{"location":"models/#environment-variables","title":"Environment Variables","text":"<pre><code>export OPENAI_API_KEY=sk-...\nexport ANTHROPIC_API_KEY=sk-ant-...\nexport GEMINI_API_KEY=AIza...\nexport GROQ_API_KEY=gsk_...\n</code></pre>"},{"location":"models/#provider-support","title":"Provider Support","text":"<p>All chat models now accept multiple input formats for maximum convenience:</p>"},{"location":"models/#1-simple-string-most-convenient","title":"1. Simple String (Most Convenient)","text":"<pre><code>response = await model.ainvoke(\"What is the capital of France?\")\n</code></pre>"},{"location":"models/#2-list-of-dicts-standard-format","title":"2. List of Dicts (Standard Format)","text":"<pre><code>response = await model.ainvoke([\n    {\"role\": \"system\", \"content\": \"You are helpful\"},\n    {\"role\": \"user\", \"content\": \"Hello\"},\n])\n</code></pre>"},{"location":"models/#3-message-objects-type-safe","title":"3. Message Objects (Type-Safe)","text":"<pre><code>from cogent.core.messages import SystemMessage, HumanMessage\n\nresponse = await model.ainvoke([\n    SystemMessage(content=\"You are helpful\"),\n    HumanMessage(content=\"Hello\"),\n])\n</code></pre>"},{"location":"models/#openai","title":"OpenAI","text":"<pre><code>from cogent.models import OpenAIChat, OpenAIEmbedding\n\n# Tier 1: Simple string\nagent = Agent(\"Helper\", model=\"gpt4\")\n\n# Tier 2: Factory\nmodel = create_chat(\"gpt4\")\nmodel = create_chat(\"openai\", \"gpt-4o\")\n\n# Tier 3: Direct\nmodel = OpenAIChat(\n    model=\"gpt-4o\",\n    temperature=0.7,\n    max_tokens=2000,\n    api_key=\"sk-...\",  # Or OPENAI_API_KEY env var\n)\n\n# Embeddings\nembeddings = OpenAIEmbedding(model=\"text-embedding-3-small\")\n\n# Primary API with metadata\nresult = await embeddings.embed([\"Hello world\"])\nprint(result.embeddings)  # Vectors\nprint(result.metadata)    # Full metadata\n\n# Convenience for single text\nresult = await embeddings.embed(\"Query\")\nvector = result.embeddings[0]\n</code></pre> <p>With tools:</p> <pre><code>from cogent.tools import tool\n\n@tool\ndef search(query: str) -&gt; str:\n    \"\"\"Search the web.\"\"\"\n    return f\"Results for: {query}\"\n\nmodel = ChatModel(model=\"gpt-4o\")\nbound = model.bind_tools([search])\n\nresponse = await bound.ainvoke([\n    {\"role\": \"user\", \"content\": \"Search for AI news\"}\n])\n\nif response.tool_calls:\n    for call in response.tool_calls:\n        print(f\"Tool: {call['name']}, Args: {call['args']}\")\n</code></pre> <p>Responses API (Beta):</p> <p>OpenAI's Responses API is optimized for tool use and structured outputs. Use the <code>use_responses_api=True</code> parameter:</p> <pre><code>from cogent.models.openai import OpenAIChat\n\n# Standard Chat Completions API (default)\nmodel = OpenAIChat(model=\"gpt-4o\")\n\n# Responses API (optimized for tool use)\nmodel = OpenAIChat(model=\"gpt-4o\", use_responses_api=True)\n\n# Works seamlessly with tools\nbound = model.bind_tools([search_tool, calc_tool])\nresponse = await bound.ainvoke(messages)\n</code></pre> <p>The Responses API provides better performance for multi-turn tool conversations while maintaining the same interface.</p>"},{"location":"models/#azure-openai","title":"Azure OpenAI","text":"<p>Enterprise Azure deployments with Azure AD support:</p> <pre><code>from cogent.models.azure import AzureEntraAuth, AzureOpenAIChat, AzureOpenAIEmbedding\n\n# With API key\nmodel = AzureOpenAIChat(\n    azure_endpoint=\"https://your-resource.openai.azure.com\",\n    deployment=\"gpt-4o\",\n    api_key=\"your-api-key\",\n    api_version=\"2024-02-01\",\n)\n\n# With Entra ID (DefaultAzureCredential)\nmodel = AzureOpenAIChat(\n    azure_endpoint=\"https://your-resource.openai.azure.com\",\n    deployment=\"gpt-4o\",\n    entra=AzureEntraAuth(method=\"default\"),  # Uses DefaultAzureCredential\n)\n\n# With Entra ID (Managed Identity)\n# - System-assigned MI: omit client_id\n# - User-assigned MI: set client_id (recommended when multiple identities exist)\nmodel = AzureOpenAIChat(\n    azure_endpoint=\"https://your-resource.openai.azure.com\",\n    deployment=\"gpt-4o\",\n    entra=AzureEntraAuth(\n        method=\"managed_identity\",\n        client_id=\"&lt;USER_ASSIGNED_MANAGED_IDENTITY_CLIENT_ID&gt;\",\n    ),\n)\n\n# Embeddings\nembeddings = AzureOpenAIEmbedding(\n    azure_endpoint=\"https://your-resource.openai.azure.com\",\n    deployment=\"text-embedding-ada-002\",\n    entra=AzureEntraAuth(method=\"default\"),\n)\n\nresult = await embeddings.embed([\"Document text\"])\n</code></pre> <p>Environment variables:</p> <pre><code>AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com\nAZURE_OPENAI_API_VERSION=2024-02-01\nAZURE_OPENAI_DEPLOYMENT=gpt-4o\nAZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-ada-002\n\n# Auth selection\nAZURE_OPENAI_AUTH_TYPE=managed_identity  # api_key | default | managed_identity | client_secret\n\n# API key auth\n# AZURE_OPENAI_API_KEY=your-api-key\n\n# Managed identity auth (user-assigned MI)\n# AZURE_OPENAI_CLIENT_ID=...\n\n# Service principal auth (client secret)\n# AZURE_OPENAI_TENANT_ID=...\n# AZURE_OPENAI_CLIENT_ID=...\n# AZURE_OPENAI_CLIENT_SECRET=...\n</code></pre>"},{"location":"models/#anthropic","title":"Anthropic","text":"<p>Claude models with native SDK:</p> <pre><code>from cogent.models.anthropic import AnthropicChat\n\nmodel = AnthropicChat(\n    model=\"claude-sonnet-4-20250514\",\n    max_tokens=4096,\n    api_key=\"sk-ant-...\",  # Or ANTHROPIC_API_KEY env var\n)\n\nresponse = await model.ainvoke([\n    {\"role\": \"user\", \"content\": \"Explain quantum computing\"}\n])\n</code></pre> <p>Claude-specific features:</p> <pre><code># System message\nresponse = await model.ainvoke(\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    system=\"You are a helpful coding assistant.\",\n)\n\n# With tools\nmodel = AnthropicChat(model=\"claude-sonnet-4-20250514\")\nbound = model.bind_tools([search_tool])\n</code></pre>"},{"location":"models/#groq","title":"Groq","text":"<p>Ultra-fast inference for supported models:</p> <pre><code>from cogent.models.groq import GroqChat\n\nmodel = GroqChat(\n    model=\"llama-3.3-70b-versatile\",\n    api_key=\"gsk_...\",  # Or GROQ_API_KEY env var\n)\n\nresponse = await model.ainvoke([\n    {\"role\": \"user\", \"content\": \"Write a haiku about coding\"}\n])\n</code></pre> <p>Available models:</p> Model Description <code>llama-3.3-70b-versatile</code> Llama 3.3 70B <code>llama-3.1-8b-instant</code> Fast Llama 3.1 8B <code>mixtral-8x7b-32768</code> Mixtral 8x7B <code>gemma2-9b-it</code> Gemma 2 9B <p>Responses API (Beta):</p> <p>Groq also supports OpenAI's Responses API for optimized tool use:</p> <pre><code>from cogent.models.groq import GroqChat\n\n# Standard Chat Completions API (default)\nmodel = GroqChat(model=\"llama-3.3-70b-versatile\")\n\n# Responses API (optimized for tool use)\nmodel = GroqChat(model=\"llama-3.3-70b-versatile\", use_responses_api=True)\n\n# Works seamlessly with tools\nbound = model.bind_tools([search_tool])\nresponse = await bound.ainvoke(messages)\n</code></pre>"},{"location":"models/#google-gemini","title":"Google Gemini","text":"<p>Google's Gemini models:</p> <pre><code>from cogent.models.gemini import GeminiChat, GeminiEmbedding\n\nmodel = GeminiChat(\n    model=\"gemini-2.0-flash\",\n    api_key=\"...\",  # Or GOOGLE_API_KEY env var\n)\n\nresponse = await model.ainvoke([\n    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n])\n\n# Embeddings\nembeddings = GeminiEmbedding(model=\"text-embedding-004\")\n</code></pre>"},{"location":"models/#ollama","title":"Ollama","text":"<p>Local models via Ollama:</p> <pre><code>from cogent.models.ollama import OllamaChat, OllamaEmbedding\n\n# Chat (requires `ollama run llama3.2`)\nmodel = OllamaChat(\n    model=\"llama3.2\",\n    base_url=\"http://localhost:11434\",\n)\n\nresponse = await model.ainvoke([\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n])\n\n# Embeddings\nembeddings = OllamaEmbedding(model=\"nomic-embed-text\")\n</code></pre>"},{"location":"models/#xai-grok","title":"xAI (Grok)","text":"<p>Grok models with reasoning capabilities:</p> <pre><code>from cogent.models.xai import XAIChat\n\n# Flagship reasoning model (256K context)\nmodel = XAIChat(\n    model=\"grok-4\",\n    api_key=\"...\",  # Or XAI_API_KEY env var\n)\n\n# Fast agentic model (2M context, optimized for tools)\nmodel = XAIChat(model=\"grok-4-1-fast\")\n\n# Non-reasoning variant (faster, cheaper)\nmodel = XAIChat(model=\"grok-4-1-fast-non-reasoning\")\n\n# With reasoning effort control (grok-3-mini only)\nmodel = XAIChat(model=\"grok-3-mini\", reasoning_effort=\"high\")\n# or use with_reasoning()\nmodel = XAIChat(model=\"grok-3-mini\").with_reasoning(\"high\")\n\nresponse = await model.ainvoke([\n    {\"role\": \"user\", \"content\": \"What is 101 * 3?\"}\n])\n\n# Reasoning tokens tracked in metadata\nif response.metadata.tokens:\n    print(f\"Reasoning tokens: {response.metadata.tokens.reasoning_tokens}\")\n</code></pre> <p>Available models:</p> Model Context Description <code>grok-4</code> 256K Flagship reasoning model <code>grok-4-1-fast</code> 2M Fast agentic, optimized for tools <code>grok-4-1-fast-reasoning</code> 2M With explicit reasoning <code>grok-4-1-fast-non-reasoning</code> 2M Without reasoning (faster) <code>grok-3-mini</code> - Supports <code>reasoning_effort</code> (low/high) <code>grok-2-vision-1212</code> - Image understanding <code>grok-code-fast-1</code> - Code-optimized <p>Features: - Function/tool calling (all models) - Structured outputs (JSON mode) - Reasoning (grok-4, grok-4-1-fast-reasoning, grok-3-mini) - Vision (grok-2-vision-1212) - 2M context window (grok-4-1-fast models)</p>"},{"location":"models/#deepseek","title":"DeepSeek","text":"<p>DeepSeek models with Chain of Thought reasoning:</p> <pre><code>from cogent.models.deepseek import DeepSeekChat\n\n# Standard chat model\nmodel = DeepSeekChat(\n    model=\"deepseek-chat\",\n    api_key=\"...\",  # Or DEEPSEEK_API_KEY env var\n)\n\n# Reasoning model (exposes Chain of Thought)\nmodel = DeepSeekChat(model=\"deepseek-reasoner\")\n\nresponse = await model.ainvoke(\"9.11 and 9.8, which is greater?\")\n\n# Access reasoning content (Chain of Thought)\nif hasattr(response, 'reasoning'):\n    print(\"Reasoning:\", response.reasoning)\nprint(\"Answer:\", response.content)\n</code></pre> <p>Available models:</p> Model Tools Description <code>deepseek-chat</code> \u2705 General chat model with tool support <code>deepseek-reasoner</code> \u274c Reasoning model with CoT (no tools) <p>Note: <code>deepseek-reasoner</code> does NOT support: - Function calling/tools - <code>temperature</code>, <code>top_p</code>, <code>presence_penalty</code>, <code>frequency_penalty</code></p>"},{"location":"models/#custom-endpoints","title":"Custom Endpoints","text":"<p>Any OpenAI-compatible endpoint (vLLM, Together AI, etc.):</p> <pre><code>from cogent.models.custom import CustomChat, CustomEmbedding\n\n# vLLM\nmodel = CustomChat(\n    base_url=\"http://localhost:8000/v1\",\n    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n)\n\n# Together AI\nmodel = CustomChat(\n    base_url=\"https://api.together.xyz/v1\",\n    model=\"meta-llama/Llama-3-70b-chat-hf\",\n    api_key=\"...\",\n)\n\n# Custom embeddings\nembeddings = CustomEmbedding(\n    base_url=\"http://localhost:8000/v1\",\n    model=\"BAAI/bge-small-en-v1.5\",\n)\n</code></pre>"},{"location":"models/#factory-function","title":"Factory Function","text":"<p>Create models dynamically by provider:</p> <pre><code>from cogent.models import create_chat, create_embedding\n\n# OpenAI\nmodel = create_chat(\"openai\", model=\"gpt-4o\")\n\n# Azure\nmodel = create_chat(\n    \"azure\",\n    deployment=\"gpt-4o\",\n    azure_endpoint=\"https://your-resource.openai.azure.com\",\n    entra=AzureEntraAuth(method=\"default\"),\n)\n\n# Anthropic\nmodel = create_chat(\"anthropic\", model=\"claude-sonnet-4-20250514\")\n\n# Groq\nmodel = create_chat(\"groq\", model=\"llama-3.3-70b-versatile\")\n\n# Gemini\nmodel = create_chat(\"gemini\", model=\"gemini-2.0-flash\")\n\n# Ollama\nmodel = create_chat(\"ollama\", model=\"llama3.2\")\n\n# xAI (Grok)\nmodel = create_chat(\"xai\", model=\"grok-4-1-fast\")\n\n# DeepSeek\nmodel = create_chat(\"deepseek\", model=\"deepseek-chat\")\nmodel = create_chat(\"deepseek\", model=\"deepseek-reasoner\")  # Reasoning model\n\n# Custom\nmodel = create_chat(\n    \"custom\",\n    base_url=\"http://localhost:8000/v1\",\n    model=\"my-model\",\n)\n</code></pre>"},{"location":"models/#mock-models","title":"Mock Models","text":"<p>For testing without API calls:</p> <pre><code>from cogent.models import MockChatModel, MockEmbedding\n\n# Predictable responses\nmodel = MockChatModel(responses=[\"Hello!\", \"How can I help?\"])\n\nresponse = await model.ainvoke([{\"role\": \"user\", \"content\": \"Hi\"}])\nprint(response.content)  # \"Hello!\"\n\nresponse = await model.ainvoke([{\"role\": \"user\", \"content\": \"Help\"}])\nprint(response.content)  # \"How can I help?\"\n\n# Mock embeddings\nembeddings = MockEmbedding(dimension=384)\nvectors = await embeddings.embed_documents([\"test\"])\nprint(len(vectors[0]))  # 384\n</code></pre>"},{"location":"models/#streaming","title":"Streaming","text":"<p>All models support streaming with complete metadata:</p> <pre><code>from cogent.models import ChatModel\n\nmodel = ChatModel(model=\"gpt-4o\")\n\nasync for chunk in model.astream([\n    {\"role\": \"user\", \"content\": \"Write a story\"}\n]):\n    print(chunk.content, end=\"\", flush=True)\n\n    # Access metadata in all chunks\n    if chunk.metadata:\n        print(f\"\\nModel: {chunk.metadata.model}\")\n        print(f\"Response ID: {chunk.metadata.response_id}\")\n\n        # Token usage available in final chunk\n        if chunk.metadata.tokens:\n            print(f\"Tokens: {chunk.metadata.tokens.total_tokens}\")\n            print(f\"Finish: {chunk.metadata.finish_reason}\")\n</code></pre>"},{"location":"models/#streaming-metadata","title":"Streaming Metadata","text":"<p>All 10 chat providers return complete metadata during streaming:</p> Provider Model Finish Reason Token Usage Notes OpenAI \u2705 \u2705 \u2705 Uses <code>stream_options={\"include_usage\": True}</code> Gemini \u2705 \u2705 \u2705 Extracts from <code>usage_metadata</code> Groq \u2705 \u2705 \u2705 Compatible with OpenAI pattern Mistral \u2705 \u2705 \u2705 Metadata accumulation Cohere \u2705 \u2705 \u2705 Event-based streaming (<code>message-end</code>) Anthropic \u2705 \u2705 \u2705 Snapshot-based metadata Cloudflare \u2705 \u2705 \u2705 Stream options support Ollama \u2705 \u2705 \u2705 Local model metadata Azure OpenAI \u2705 \u2705 \u2705 Stream options support Azure AI Foundry / GitHub \u2705 \u2705 \u2705 Stream options via model_extras <p>Metadata Structure:</p> <pre><code>@dataclass\nclass MessageMetadata:\n    id: str | None              # Response ID\n    timestamp: str | None       # ISO 8601 timestamp\n    model: str | None           # Model name/version\n    tokens: TokenUsage | None   # Token counts\n    finish_reason: str | None   # stop, length, error\n    response_id: str | None     # Provider response ID\n    duration: float | None      # Request duration (ms)\n    correlation_id: str | None  # For tracing\n\n@dataclass\nclass TokenUsage:\n    prompt_tokens: int\n    completion_tokens: int\n    total_tokens: int\n</code></pre> <p>Streaming Pattern:</p> <ol> <li>Content chunks \u2014 Include partial metadata (model, response_id, timestamp)</li> <li>Final chunk \u2014 Empty content with complete metadata (finish_reason, tokens)</li> </ol> <pre><code># Example streaming flow\nasync for chunk in model.astream(messages):\n    # Chunks 1-N: Content with partial metadata\n    if chunk.content:\n        print(chunk.content, end=\"\")\n\n    # Final chunk: Complete metadata\n    if chunk.metadata and chunk.metadata.finish_reason:\n        print(f\"\\n\\nCompleted with {chunk.metadata.tokens.total_tokens} tokens\")\n</code></pre>"},{"location":"models/#embeddings","title":"Embeddings","text":"<p>All 9 embedding providers support a standardized API with rich metadata and flexible usage patterns:</p> <pre><code>from cogent.models import OpenAIEmbedding, GeminiEmbedding, OllamaEmbedding\n\nembedder = OpenAIEmbedding(model=\"text-embedding-3-small\")\n\n# Primary API: embed() / aembed() - Returns EmbeddingResult with full metadata\nresult = await embedder.aembed([\"Hello world\", \"Cogent\"])\nprint(result.embeddings)            # list[list[float]] - the actual vectors\nprint(result.metadata.model)        # \"text-embedding-3-small\"\nprint(result.metadata.tokens)       # TokenUsage(prompt=4, completion=0, total=4)\nprint(result.metadata.dimensions)   # 1536\nprint(result.metadata.duration)     # 0.181 seconds\nprint(result.metadata.num_texts)    # 2\n\n# Convenience: embed_one() / aembed_one() - Single text, returns vector only\nvector = await embedder.aembed_one(\"Single text\")\nprint(len(vector))  # 1536\n\n# Sync versions\nresult = embedder.embed([\"Text 1\", \"Text 2\"])\nvector = embedder.embed_one(\"Single text\")\n\n# VectorStore protocol: embed_texts() / embed_query() - Async, no metadata\nvectors = await embedder.embed_texts([\"Doc1\", \"Doc2\"])  # list[list[float]]\nquery_vec = await embedder.embed_query(\"Search query\")  # list[float]\n</code></pre> <p>Standardized API Summary:</p> Method Input Returns Async Metadata <code>embed(texts)</code> <code>list[str]</code> <code>EmbeddingResult</code> \u274c \u2705 <code>aembed(texts)</code> <code>list[str]</code> <code>EmbeddingResult</code> \u2705 \u2705 <code>embed_one(text)</code> <code>str</code> <code>list[float]</code> \u274c \u274c <code>aembed_one(text)</code> <code>str</code> <code>list[float]</code> \u2705 \u274c <code>embed_texts(texts)</code> <code>list[str]</code> <code>list[list[float]]</code> \u2705 \u274c <code>embed_query(text)</code> <code>str</code> <code>list[float]</code> \u2705 \u274c <code>dimension</code> property <code>int</code> - -"},{"location":"models/#embedding-metadata","title":"Embedding Metadata","text":"<p>All 9 embedding providers return complete metadata:</p> Provider Token Usage Notes OpenAI \u2705 Extracts from <code>response.usage.prompt_tokens</code> Cohere \u2705 Extracts from <code>response.meta.billed_units.input_tokens</code> Mistral \u2705 Uses OpenAI SDK, provides token counts Azure OpenAI \u2705 Extracts from <code>response.usage</code> like OpenAI Gemini \u274c API doesn't provide token counts for embeddings Ollama \u274c Local embeddings, no token tracking Cloudflare \u274c API doesn't track tokens Mock \u274c Test embedding, no real tokens Custom \u26a1 Conditional - depends on underlying API <p>Metadata Structure:</p> <pre><code>@dataclass\nclass EmbeddingMetadata:\n    id: str                     # Unique request ID\n    timestamp: str              # ISO 8601 timestamp\n    model: str | None           # Model name/version\n    tokens: TokenUsage | None   # Token usage (if available)\n    duration: float             # Request duration (seconds)\n    dimensions: int | None      # Vector dimensions\n    num_texts: int              # Number of texts embedded\n\n@dataclass\nclass EmbeddingResult:\n    embeddings: list[list[float]]  # The actual embedding vectors\n    metadata: EmbeddingMetadata    # Complete metadata\n</code></pre> <p>Usage Examples:</p> <pre><code># Use case 1: Need metadata for cost tracking\nresult = await embedder.aembed([\"Text 1\", \"Text 2\"])\nvectors = result.embeddings\ntokens = result.metadata.tokens  # Track token usage for billing\nduration = result.metadata.duration  # Monitor performance\n\n# Use case 2: Simple embedding without metadata\nvector = await embedder.aembed_one(\"Query text\")  # Just returns the vector\n\n# Use case 3: VectorStore integration (protocol compliance)\n# These methods are used internally by VectorStore\nvectors = await embedder.embed_texts([\"Document 1\", \"Document 2\"])\nquery_vec = await embedder.embed_query(\"Search query\")\n\n# Use case 4: Sync batch embedding\nresult = embedder.embed(large_batch)  # Sync version for compatibility\n</code></pre> <p>Observability Benefits:</p> <ul> <li>Cost tracking \u2014 Monitor token usage across providers</li> <li>Performance \u2014 Track request duration and batch sizes</li> <li>Debugging \u2014 Trace requests with unique IDs and timestamps</li> <li>Model versioning \u2014 Know which embedding model version was used</li> <li>Capacity planning \u2014 Understand dimensions and text counts</li> </ul>"},{"location":"models/#streaming_1","title":"Streaming","text":"<p>All models support streaming with complete metadata:</p> <pre><code>from cogent.models import ChatModel\n\nmodel = ChatModel(model=\"gpt-4o\")\n\nasync for chunk in model.astream([\n    {\"role\": \"user\", \"content\": \"Write a story\"}\n]):\n    print(chunk.content, end=\"\", flush=True)\n\n    # Access metadata in all chunks\n    if chunk.metadata:\n        print(f\"\\nModel: {chunk.metadata.model}\")\n        print(f\"Response ID: {chunk.metadata.response_id}\")\n\n        # Token usage available in final chunk\n        if chunk.metadata.tokens:\n            print(f\"Tokens: {chunk.metadata.tokens.total_tokens}\")\n            print(f\"Finish: {chunk.metadata.finish_reason}\")\n</code></pre>"},{"location":"models/#streaming-metadata_1","title":"Streaming Metadata","text":"<p>All 10 chat providers return complete metadata during streaming:</p> Provider Model Finish Reason Token Usage Notes OpenAI \u2705 \u2705 \u2705 Uses <code>stream_options={\"include_usage\": True}</code> Gemini \u2705 \u2705 \u2705 Extracts from <code>usage_metadata</code> Groq \u2705 \u2705 \u2705 Compatible with OpenAI pattern"},{"location":"models/#thinking-reasoning","title":"Thinking &amp; Reasoning","text":"<p>Several providers offer \"reasoning\" or \"thinking\" models that expose their chain-of-thought process. Cogent provides unified access to these capabilities.</p>"},{"location":"models/#feature-comparison","title":"Feature Comparison","text":"Provider Models Control Parameter Access Reasoning Structured Output Anthropic <code>claude-sonnet-4</code>, <code>claude-opus-4</code> <code>thinking_budget</code> <code>msg.thinking</code> \u2705 via thinking OpenAI <code>o1</code>, <code>o3</code>, <code>o4-mini</code> <code>reasoning_effort</code> Hidden \u2705 Gemini <code>gemini-2.5-*</code> <code>thinking_budget</code> <code>msg.thinking</code> \u2705 xAI <code>grok-3-mini</code> <code>reasoning_effort</code> Hidden \u2705 DeepSeek <code>deepseek-reasoner</code> Always on <code>msg.reasoning</code> \u274c"},{"location":"models/#anthropic-extended-thinking","title":"Anthropic Extended Thinking","text":"<p>Claude models support extended thinking with configurable token budgets:</p> <pre><code>from cogent.models.anthropic import AnthropicChat\n\n# Enable extended thinking with budget\nmodel = AnthropicChat(\n    model=\"claude-sonnet-4-20250514\",\n    thinking={\"type\": \"enabled\", \"budget_tokens\": 10000},\n)\n\nresponse = await model.ainvoke([\n    {\"role\": \"user\", \"content\": \"Solve this step by step: 15! / (12! * 3!)\"}\n])\n\n# Access thinking content\nif response.thinking:\n    print(\"Thinking:\", response.thinking)\nprint(\"Answer:\", response.content)\n</code></pre> <p>Using ReasoningConfig:</p> <pre><code>from cogent.models.anthropic import AnthropicChat\nfrom cogent.reasoning import ReasoningConfig\n\n# Create config\nconfig = ReasoningConfig(budget_tokens=10000)\n\n# Apply to model\nmodel = AnthropicChat(model=\"claude-sonnet-4-20250514\")\nthinking_model = model.with_reasoning(config)\n\nresponse = await thinking_model.ainvoke(messages)\n</code></pre> <p>Features: - Thinking exposed in <code>msg.thinking</code> attribute - Works with streaming (thinking streamed first) - Compatible with <code>with_structured_output()</code> via thinking</p>"},{"location":"models/#openai-reasoning-models","title":"OpenAI Reasoning Models","text":"<p>OpenAI's o-series models (o1, o3, o4-mini) have built-in reasoning:</p> <pre><code>from cogent.models.openai import OpenAIChat\n\n# Reasoning effort: \"low\", \"medium\", \"high\"\nmodel = OpenAIChat(\n    model=\"o4-mini\",\n    reasoning_effort=\"high\",  # More thorough reasoning\n)\n\nresponse = await model.ainvoke([\n    {\"role\": \"user\", \"content\": \"Prove that sqrt(2) is irrational\"}\n])\n</code></pre> <p>Using ReasoningConfig:</p> <pre><code>from cogent.models.openai import OpenAIChat\nfrom cogent.reasoning import ReasoningConfig\n\nmodel = OpenAIChat(model=\"o4-mini\")\nreasoning_model = model.with_reasoning(ReasoningConfig(effort=\"high\"))\n</code></pre> <p>Notes: - Reasoning is internal (not exposed in response) - No thinking budget - use <code>reasoning_effort</code> instead - Supports structured output with <code>json_schema</code> response format</p>"},{"location":"models/#gemini-thinking","title":"Gemini Thinking","text":"<p>Gemini 2.5 models support thinking with budget control:</p> <pre><code>from cogent.models.gemini import GeminiChat\n\nmodel = GeminiChat(\n    model=\"gemini-2.5-flash-preview-05-20\",\n    thinking_budget=8000,  # Token budget for thinking\n)\n\nresponse = await model.ainvoke([\n    {\"role\": \"user\", \"content\": \"What's the optimal strategy in this game?\"}\n])\n\n# Access thinking\nif response.thinking:\n    print(\"Thought process:\", response.thinking)\n</code></pre> <p>Using ReasoningConfig:</p> <pre><code>from cogent.models.gemini import GeminiChat\nfrom cogent.reasoning import ReasoningConfig\n\nmodel = GeminiChat(model=\"gemini-2.5-flash-preview-05-20\")\nthinking_model = model.with_reasoning(ReasoningConfig(budget_tokens=8000))\n</code></pre>"},{"location":"models/#xai-reasoning","title":"xAI Reasoning","text":"<p>Grok-3-mini supports reasoning effort control:</p> <pre><code>from cogent.models.xai import XAIChat\n\n# Enable reasoning with effort level\nmodel = XAIChat(\n    model=\"grok-3-mini\",\n    reasoning_effort=\"high\",  # \"low\" or \"high\"\n)\n\nresponse = await model.ainvoke([\n    {\"role\": \"user\", \"content\": \"Explain the halting problem\"}\n])\n</code></pre> <p>Using with_reasoning():</p> <pre><code>from cogent.models.xai import XAIChat\n\nmodel = XAIChat(model=\"grok-3-mini\")\nreasoning_model = model.with_reasoning(effort=\"high\")\n</code></pre> <p>Notes: - Only <code>grok-3-mini</code> and <code>grok-3-mini-beta</code> support reasoning_effort - Reasoning is internal (not exposed in response)</p>"},{"location":"models/#deepseek-reasoner","title":"DeepSeek Reasoner","text":"<p>DeepSeek's reasoner model exposes its chain-of-thought:</p> <pre><code>from cogent.models.deepseek import DeepSeekChat\n\nmodel = DeepSeekChat(model=\"deepseek-reasoner\")\n\nresponse = await model.ainvoke([\n    {\"role\": \"user\", \"content\": \"Prove the Pythagorean theorem\"}\n])\n\n# Access reasoning content\nif response.reasoning:\n    print(\"Chain of thought:\", response.reasoning)\nprint(\"Final answer:\", response.content)\n</code></pre> <p>Streaming reasoning:</p> <pre><code>async for chunk in model.astream(messages):\n    if chunk.reasoning:\n        print(f\"[Reasoning] {chunk.reasoning}\", end=\"\", flush=True)\n    if chunk.content:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <p>Notes: - Reasoning always enabled for <code>deepseek-reasoner</code> - Does NOT support tools or structured output - Use <code>deepseek-chat</code> for non-reasoning use cases</p>"},{"location":"models/#reasoningconfig","title":"ReasoningConfig","text":"<p>Unified configuration for reasoning across providers:</p> <pre><code>from cogent.reasoning import ReasoningConfig\n\n# Token budget (Anthropic, Gemini)\nconfig = ReasoningConfig(budget_tokens=10000)\n\n# Effort level (OpenAI, xAI)\nconfig = ReasoningConfig(effort=\"high\")\n\n# Both (uses appropriate one per provider)\nconfig = ReasoningConfig(budget_tokens=10000, effort=\"high\")\n</code></pre> <p>Provider mapping:</p> Provider <code>budget_tokens</code> <code>effort</code> Anthropic \u2705 <code>thinking.budget_tokens</code> \u274c OpenAI \u274c \u2705 <code>reasoning_effort</code> Gemini \u2705 <code>thinking_budget</code> \u274c xAI \u274c \u2705 <code>reasoning_effort</code> DeepSeek \u274c (always on) \u274c"},{"location":"models/#structured-output","title":"Structured Output","text":"<p>Chat models support structured output via <code>with_structured_output()</code> for type-safe JSON responses.</p>"},{"location":"models/#provider-support_1","title":"Provider Support","text":"Provider Method Strict Mode OpenAI <code>json_schema</code> \u2705 Anthropic Tool-based \u2705 Gemini <code>response_schema</code> \u2705 Groq <code>json_mode</code> \u274c xAI <code>json_schema</code> \u2705 DeepSeek <code>deepseek-chat</code> only \u274c Ollama <code>json_mode</code> \u274c"},{"location":"models/#basic-usage","title":"Basic Usage","text":"<pre><code>from pydantic import BaseModel, Field\nfrom cogent.models.openai import OpenAIChat\n\nclass Person(BaseModel):\n    name: str = Field(description=\"Full name\")\n    age: int = Field(description=\"Age in years\")\n\n# Configure model for structured output\nllm = OpenAIChat(model=\"gpt-4o\").with_structured_output(Person)\n\nresponse = await llm.ainvoke([\n    {\"role\": \"user\", \"content\": \"Extract: John Doe is 30 years old\"}\n])\n\n# Response content is JSON matching schema\nimport json\ndata = json.loads(response.content)\nprint(data)  # {\"name\": \"John Doe\", \"age\": 30}\n</code></pre>"},{"location":"models/#schema-types","title":"Schema Types","text":"<pre><code>from dataclasses import dataclass\nfrom typing import TypedDict\n\n# Pydantic (recommended)\nclass PersonPydantic(BaseModel):\n    name: str\n    age: int\n\n# Dataclass\n@dataclass\nclass PersonDataclass:\n    name: str\n    age: int\n\n# TypedDict\nclass PersonTypedDict(TypedDict):\n    name: str\n    age: int\n\n# JSON Schema dict\nperson_schema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"name\": {\"type\": \"string\"},\n        \"age\": {\"type\": \"integer\"}\n    },\n    \"required\": [\"name\", \"age\"]\n}\n\n# All work with with_structured_output()\nllm.with_structured_output(PersonPydantic)\nllm.with_structured_output(PersonDataclass)\nllm.with_structured_output(PersonTypedDict)\nllm.with_structured_output(person_schema)\n</code></pre>"},{"location":"models/#methods","title":"Methods","text":"<pre><code># json_schema (default, strict typing)\nllm.with_structured_output(Person, method=\"json_schema\")\n\n# json_mode (less strict, more compatible)\nllm.with_structured_output(Person, method=\"json_mode\")\n</code></pre>"},{"location":"models/#with-tools","title":"With Tools","text":"<p>Structured output and tools can be combined (the model decides when to use each):</p> <pre><code>@tool\ndef get_weather(location: str) -&gt; str:\n    \"\"\"Get weather for a location.\"\"\"\n    return f\"Sunny in {location}\"\n\nllm = OpenAIChat(model=\"gpt-4o\")\nllm = llm.bind_tools([get_weather])\nllm = llm.with_structured_output(Person)\n</code></pre>"},{"location":"models/#agent-level-structured-output","title":"Agent-Level Structured Output","text":"<p>For most use cases, use the Agent's <code>output</code> parameter instead:</p> <pre><code>from cogent import Agent\n\nagent = Agent(\n    name=\"Extractor\",\n    model=\"gpt4\",\n    output=Person,  # Automatic validation and retry\n)\n\nresult = await agent.run(\"Extract: John Doe, 30 years old\")\nprint(result.data)  # Person(name=\"John Doe\", age=30)\n</code></pre> <p>See Agent Documentation for more details.</p>"},{"location":"models/#base-classes","title":"Base Classes","text":""},{"location":"models/#basechatmodel","title":"BaseChatModel","text":"<p>Protocol for all chat models:</p> <pre><code>from cogent.models.base import BaseChatModel\n\nclass BaseChatModel(Protocol):\n    async def ainvoke(\n        self,\n        messages: list[dict],\n        **kwargs,\n    ) -&gt; AIMessage: ...\n\n    async def astream(\n        self,\n        messages: list[dict],\n        **kwargs,\n    ) -&gt; AsyncIterator[AIMessage]: ...\n\n    def bind_tools(\n        self,\n        tools: list[BaseTool],\n    ) -&gt; BaseChatModel: ...\n</code></pre>"},{"location":"models/#aimessage","title":"AIMessage","text":"<p>Response type from chat models:</p> <pre><code>from cogent.models.base import AIMessage\n\n@dataclass\nclass AIMessage:\n    content: str\n    tool_calls: list[dict] | None = None\n    usage: dict | None = None  # {\"input_tokens\": ..., \"output_tokens\": ...}\n    raw: Any = None  # Original provider response\n</code></pre>"},{"location":"models/#baseembedding","title":"BaseEmbedding","text":"<p>Standardized protocol for all embedding models:</p> <pre><code>from cogent.models.base import BaseEmbedding\nfrom cogent.core.messages import EmbeddingResult\n\nclass BaseEmbedding(ABC):\n    # Primary methods - return full metadata\n    @abstractmethod\n    def embed(self, texts: list[str]) -&gt; EmbeddingResult:\n        \"\"\"Embed texts synchronously with metadata.\"\"\"\n        ...\n\n    @abstractmethod\n    async def aembed(self, texts: list[str]) -&gt; EmbeddingResult:\n        \"\"\"Embed texts asynchronously with metadata.\"\"\"\n        ...\n\n    # Convenience methods - single text, no metadata\n    def embed_one(self, text: str) -&gt; list[float]:\n        \"\"\"Embed single text synchronously, returns vector only.\"\"\"\n        ...\n\n    async def aembed_one(self, text: str) -&gt; list[float]:\n        \"\"\"Embed single text asynchronously, returns vector only.\"\"\"\n        ...\n\n    # VectorStore protocol - async, no metadata\n    async def embed_texts(self, texts: list[str]) -&gt; list[list[float]]:\n        \"\"\"Embed texts for VectorStore (async, returns vectors only).\"\"\"\n        ...\n\n    async def embed_query(self, text: str) -&gt; list[float]:\n        \"\"\"Embed query for VectorStore (async, returns vector only).\"\"\"\n        ...\n\n    @property\n    def dimension(self) -&gt; int:\n        \"\"\"Return embedding dimension.\"\"\"\n        ...\n</code></pre> <p>All 9 providers implement this API: - OpenAIEmbedding - AzureOpenAIEmbedding - OllamaEmbedding - CohereEmbedding - GeminiEmbedding - CloudflareEmbedding - MistralEmbedding - CustomEmbedding - MockEmbedding</p>"},{"location":"models/#api-reference","title":"API Reference","text":""},{"location":"models/#chatmodel-aliases","title":"ChatModel Aliases","text":"Alias Actual Class <code>ChatModel</code> <code>OpenAIChat</code> <code>EmbeddingModel</code> <code>OpenAIEmbedding</code>"},{"location":"models/#provider-classes","title":"Provider Classes","text":"Provider Chat Class Embedding Class OpenAI <code>OpenAIChat</code> <code>OpenAIEmbedding</code> Azure <code>AzureOpenAIChat</code> <code>AzureOpenAIEmbedding</code> Anthropic <code>AnthropicChat</code> - Groq <code>GroqChat</code> - Gemini <code>GeminiChat</code> <code>GeminiEmbedding</code> xAI <code>XAIChat</code> - DeepSeek <code>DeepSeekChat</code> - Ollama <code>OllamaChat</code> <code>OllamaEmbedding</code> Custom <code>CustomChat</code> <code>CustomEmbedding</code>"},{"location":"models/#factory-functions","title":"Factory Functions","text":"Function Description <code>create_chat(provider, **kwargs)</code> Create chat model for any provider <code>create_embedding(provider, **kwargs)</code> Create embedding model for any provider"},{"location":"observability/","title":"Observability Module","text":"<p>The <code>cogent.observability</code> module provides real-time visibility into agent execution with a simple, composable API.</p>"},{"location":"observability/#quick-start","title":"Quick Start","text":"<pre><code>from cogent import Agent\nfrom cogent.observability import Observer\n\n# Create an observer\nobserver = Observer(level=\"progress\")\n\n# Attach to agent\nagent = Agent(\n    name=\"Assistant\",\n    model=\"gpt-4o-mini\",\n    tools=[my_tool],\n    observer=observer,\n)\n\n# Run and watch events flow\nresult = await agent.run(\"Do something useful\")\n\n# See summary\nprint(observer.summary())\n# Events: 8\n#   agent: 4\n#   tool: 4\n</code></pre>"},{"location":"observability/#output-levels","title":"Output Levels","text":"Level Shows <code>\"minimal\"</code> Only completion events <code>\"progress\"</code> Starting, thinking, tool calls, completion (default) <code>\"verbose\"</code> Progress + tool arguments <code>\"debug\"</code> Verbose + LLM requests/responses"},{"location":"observability/#example-output","title":"Example Output","text":"<pre><code>[Assistant] [starting]\n[Assistant] [thinking]\n[Assistant] [tool-call] 8552e158 calculate\n[Assistant] [tool-call] 9c9b41d8 get_weather\n[Assistant] [tool-result] 8552e158 calculate\n  '100'\n[Assistant] [tool-result] 9c9b41d8 get_weather\n  '68\u00b0F, Clear'\n[Assistant] [thinking] (iteration 2)\n[Assistant] [completed] (2.0s) \u2022 330 tokens\n</code></pre>"},{"location":"observability/#observer-api","title":"Observer API","text":""},{"location":"observability/#creating-an-observer","title":"Creating an Observer","text":"<pre><code>from cogent.observability import Observer\n\n# From preset level\nobserver = Observer(level=\"progress\")\nobserver = Observer(level=\"verbose\")\nobserver = Observer(level=\"debug\")\n\n# Disable/enable\nobserver.enabled = False\nobserver.level = \"debug\"  # Change level dynamically\n</code></pre>"},{"location":"observability/#subscribing-to-events","title":"Subscribing to Events","text":"<pre><code># Subscribe to specific event types\ndef on_tool_call(event):\n    print(f\"Tool called: {event.data['tool_name']}\")\n\nobserver.on(\"tool.called\", on_tool_call)\n\n# Subscribe to patterns (glob-style)\nobserver.on(\"tool.*\", lambda e: print(f\"Tool event: {e.type}\"))\nobserver.on(\"agent.*\", lambda e: print(f\"Agent event: {e.type}\"))\n\n# Subscribe to all events\nobserver.on_all(lambda e: print(f\"{e.type}: {e.data}\"))\n</code></pre>"},{"location":"observability/#event-types","title":"Event Types","text":"Event Level Description <code>agent.invoked</code> PROGRESS Agent started processing <code>agent.thinking</code> PROGRESS Agent is thinking (per iteration) <code>agent.responded</code> RESULT Agent completed with response <code>agent.error</code> RESULT Agent failed with error <code>tool.called</code> PROGRESS Tool invocation started <code>tool.result</code> PROGRESS Tool returned result <code>tool.error</code> PROGRESS Tool failed <code>llm.request</code> DEBUG LLM API request (verbose) <code>llm.response</code> DEBUG LLM API response (verbose) <code>stream.start</code> PROGRESS Streaming started <code>stream.end</code> PROGRESS Streaming completed <code>stream.token</code> TRACE Individual token (very verbose)"},{"location":"observability/#event-data","title":"Event Data","text":"<p>Events contain contextual data:</p> <pre><code>def on_tool_result(event):\n    print(f\"Agent: {event.data['agent_name']}\")\n    print(f\"Tool: {event.data['tool_name']}\")\n    print(f\"Call ID: {event.data['call_id']}\")  # UUID for correlation\n    print(f\"Result: {event.data['result']}\")\n\nobserver.on(\"tool.result\", on_tool_result)\n</code></pre>"},{"location":"observability/#summary","title":"Summary","text":"<pre><code># After running agent\nprint(observer.summary())\n# Events: 10\n#   agent: 6\n#   tool: 4\n</code></pre>"},{"location":"observability/#tool-call-tracking","title":"Tool Call Tracking","text":"<p>Tool calls include UUIDs for correlating calls with results:</p> <pre><code>[Assistant] [tool-call] 8552e158 calculate\n[Assistant] [tool-call] 9c9b41d8 get_weather\n[Assistant] [tool-result] 8552e158 calculate  # Same ID\n  '100'\n[Assistant] [tool-result] 9c9b41d8 get_weather  # Same ID\n  '68\u00b0F, Clear'\n</code></pre> <p>This enables tracking parallel tool executions and building execution graphs.</p>"},{"location":"observability/#multiple-agents","title":"Multiple Agents","text":"<p>A single observer can track multiple agents:</p> <pre><code>observer = Observer(level=\"progress\")\n\nresearcher = Agent(name=\"Researcher\", observer=observer, ...)\nwriter = Agent(name=\"Writer\", observer=observer, ...)\n\nawait researcher.run(\"Research topic\")\nawait writer.run(\"Write article\")\n\nprint(observer.summary())\n# Events: 16\n#   agent: 9\n#   tool: 7\n</code></pre> <p>Output shows agent names for clarity: <pre><code>[Researcher] [starting]\n[Researcher] [thinking]\n[Researcher] [tool-call] abc123 search\n[Researcher] [completed] (2.1s) \u2022 250 tokens\n[Writer] [starting]\n[Writer] [thinking]\n[Writer] [completed] (1.5s) \u2022 180 tokens\n</code></pre></p>"},{"location":"observability/#advanced-tracebus-v1-api","title":"Advanced: TraceBus (v1 API)","text":"<p>For advanced use cases, you can use the lower-level TraceBus API directly:</p> <pre><code>from cogent.observability import TraceBus, TraceType\n\nbus = TraceBus()\n\n# Subscribe to specific trace types\nbus.subscribe(TraceType.AGENT_THINKING, lambda e: print(f\"Thinking: {e}\"))\n\n# Publish events\nbus.publish(TraceType.AGENT_INVOKED, {\"agent\": \"assistant\"})\n</code></pre> <p>This is primarily used internally by the Observer. Most users should use the Observer API.</p>"},{"location":"observability/#event-history-capture","title":"Event History Capture","text":"<p>Capture and query events for later analysis:</p> <pre><code>from cogent.observability import Observer\n\n# Create observer with capture patterns\nobserver = Observer(\n    level=\"progress\",\n    capture=[\"tool.result\", \"agent.*\"],  # Capture matching events\n)\n\n# Run agent\nresult = await agent.run(\"Do something\", observer=observer)\n\n# Access captured event history\nfor event in observer.history():\n    print(f\"{event.type}: {event.data}\")\n\n# Filter history by pattern\ntool_events = observer.history(\"tool.*\")\nagent_events = observer.history(\"agent.completed\")\n\n# Clear history when done\nobserver.clear_history()\n</code></pre>"},{"location":"observability/#common-observability-patterns","title":"Common Observability Patterns","text":""},{"location":"observability/#1-tracking-tool-usage","title":"1. Tracking Tool Usage","text":"<pre><code>observer = Observer(capture=[\"tool.*\"])\nresult = await agent.run(\"Research topic\", observer=observer)\n\nfor event in observer.history(\"tool.result\"):\n    print(f\"Tool: {event.data.get('tool_name')}\")\n    print(f\"Result: {event.data.get('result')[:100]}...\")\n</code></pre>"},{"location":"observability/#2-performance-analysis","title":"2. Performance Analysis","text":"<pre><code>observer = Observer(level=\"debug\", capture=[\"agent.completed\"])\nresult = await agent.run(\"Complex task\", observer=observer)\n\nfor event in observer.history():\n    if \"duration\" in event.data:\n        print(f\"Duration: {event.data['duration']:.2f}s\")\n    if \"tokens\" in event.data:\n        print(f\"Tokens: {event.data['tokens']}\")\n</code></pre>"},{"location":"observability/#3-multi-agent-tracking","title":"3. Multi-Agent Tracking","text":"<pre><code>observer = Observer(capture=[\"agent.*\"])\n\n# Run multiple agents with same observer\nawait researcher.run(\"Research topic\", observer=observer)\nawait writer.run(\"Write article\", observer=observer)\n\n# See all agent activity\nfor event in observer.history():\n    print(f\"{event.data.get('agent_name')}: {event.type}\")\n</code></pre>"},{"location":"observability/#best-practices","title":"Best Practices","text":"<ol> <li>Start with PROGRESS: Use <code>Observer(level=\"progress\")</code> for production</li> <li>DEBUG for development: Use <code>Observer(level=\"debug\")</code> during development</li> <li>Use capture patterns: Only capture events you need for analysis</li> <li>Clear history: Call <code>observer.clear_history()</code> between runs if reusing</li> <li>Export for analysis: Save captured events to JSON for offline analysis</li> </ol>"},{"location":"observability/#tracebus","title":"TraceBus","text":"<p>Central pub/sub system for all framework events:</p> <pre><code>from cogent.observability import TraceBus, Event\nfrom cogent.core import TraceType\n\nbus = TraceBus()\n\n# Subscribe to specific event type\ndef on_task_complete(trace: Trace):\n    print(f\"Task completed: {event.data['task_id']}\")\n\nbus.subscribe(TraceType.TASK_COMPLETED, on_task_complete)\n\n# Subscribe to multiple types\nbus.subscribe_many(\n    [TraceType.TASK_STARTED, TraceType.TASK_COMPLETED],\n    log_task_events,\n)\n\n# Subscribe to ALL events\nbus.subscribe_all(lambda e: print(e))\n\n# Publish events\nawait bus.publish(Event(\n    type=TraceType.TASK_STARTED,\n    data={\"task_id\": \"123\", \"agent\": \"worker\"},\n))\n\n# Simple publish API\nawait bus.publish(\"task.completed\", {\"task_id\": \"123\"})\n</code></pre>"},{"location":"observability/#async-handlers","title":"Async Handlers","text":"<p>Both sync and async handlers are supported:</p> <pre><code># Sync handler\ndef sync_handler(trace: Trace):\n    print(event.data)\n\n# Async handler\nasync def async_handler(trace: Trace):\n    await send_notification(event.data)\n\nbus.subscribe(TraceType.TASK_COMPLETED, sync_handler)\nbus.subscribe(TraceType.TASK_COMPLETED, async_handler)\n</code></pre>"},{"location":"observability/#event-history","title":"Event History","text":"<p>Query past events:</p> <pre><code># Get event history\nevents = bus.get_history(\n    event_type=TraceType.TASK_COMPLETED,\n    limit=10,\n)\n\n# Filter by custom function\ntask_events = bus.get_history(\n    filter_fn=lambda e: e.data.get(\"task_id\") == \"123\"\n)\n</code></pre>"},{"location":"observability/#global-tracebus","title":"Global TraceBus","text":"<pre><code>from cogent.observability import get_trace_bus, set_trace_bus\n\n# Get the global bus\nbus = get_trace_bus()\n\n# Set a custom global bus\ncustom_bus = TraceBus(max_history=50000)\nset_trace_bus(custom_bus)\n</code></pre>"},{"location":"observability/#trace","title":"Trace","text":"<p>Immutable event records:</p> <pre><code>from cogent.observability import Trace\n\nevent = Event(\n    type=TraceType.TASK_COMPLETED,\n    data={\"task_id\": \"123\", \"result\": \"success\"},\n    source=\"agent:researcher\",\n    correlation_id=\"req-456\",\n)\n\nprint(event.id)          # Unique event ID\nprint(event.type)        # TraceType enum\nprint(event.data)        # Event payload\nprint(event.timestamp)   # When it occurred\nprint(event.source)      # What emitted it\nprint(event.correlation_id)  # For tracing\n</code></pre>"},{"location":"observability/#event-handlers","title":"Event Handlers","text":"<p>Pre-built handlers for common use cases:</p>"},{"location":"observability/#consoleeventhandler","title":"ConsoleEventHandler","text":"<pre><code>from cogent.observability import ConsoleEventHandler\n\nhandler = ConsoleEventHandler(\n    format=\"[{timestamp}] {type}: {data}\",\n    colored=True,\n)\n\nbus.subscribe_all(handler)\n</code></pre>"},{"location":"observability/#fileeventhandler","title":"FileEventHandler","text":"<pre><code>from cogent.observability import FileEventHandler\n\nhandler = FileEventHandler(\n    path=\"events.jsonl\",\n    format=\"json\",  # or \"text\"\n    rotate_size_mb=100,\n)\n\nbus.subscribe_all(handler)\n</code></pre>"},{"location":"observability/#filteringeventhandler","title":"FilteringEventHandler","text":"<pre><code>from cogent.observability import FilteringEventHandler\n\nhandler = FilteringEventHandler(\n    wrapped=ConsoleEventHandler(),\n    include_types=[TraceType.TASK_COMPLETED, TraceType.TASK_FAILED],\n    exclude_data_keys=[\"sensitive_field\"],\n)\n\nbus.subscribe_all(handler)\n</code></pre>"},{"location":"observability/#metricseventhandler","title":"MetricsEventHandler","text":"<pre><code>from cogent.observability import MetricsEventHandler\n\nhandler = MetricsEventHandler()\nbus.subscribe_all(handler)\n\n# Get metrics\nprint(handler.metrics)\n# {\n#     \"task_completed\": 42,\n#     \"task_failed\": 3,\n#     \"tool_called\": 156,\n# }\n</code></pre>"},{"location":"observability/#tracer","title":"Tracer","text":"<p>Distributed tracing with spans:</p> <pre><code>from cogent.observability import Tracer, SpanKind\n\ntracer = Tracer(service_name=\"my-agent\")\n\nasync with tracer.start_span(\"process_request\", kind=SpanKind.SERVER) as span:\n    span.set_attribute(\"user_id\", \"123\")\n\n    # Nested spans\n    async with tracer.start_span(\"query_database\") as db_span:\n        db_span.set_attribute(\"query\", \"SELECT ...\")\n        result = await db.query(...)\n\n    async with tracer.start_span(\"call_llm\") as llm_span:\n        llm_span.set_attribute(\"model\", \"gpt-4o\")\n        response = await llm.invoke(...)\n</code></pre>"},{"location":"observability/#span-context","title":"Span Context","text":"<pre><code>from cogent.observability import SpanContext\n\n# Get current span context\nctx = tracer.get_current_context()\n\n# Propagate to other services\nheaders = {\"traceparent\": ctx.to_header()}\n\n# Create span from incoming context\nincoming_ctx = SpanContext.from_header(request.headers[\"traceparent\"])\nasync with tracer.start_span(\"handle\", context=incoming_ctx) as span:\n    ...\n</code></pre>"},{"location":"observability/#execution-tracer","title":"Execution Tracer","text":"<p>Detailed execution tracing for debugging:</p> <pre><code>from cogent.observability import ExecutionTracer, TraceLevel\n\ntracer = ExecutionTracer(level=TraceLevel.DETAILED)\n\nresult = await agent.run(\"Query\", tracer=tracer)\n\n# Access trace\ntrace = tracer.get_trace()\nprint(f\"Nodes: {len(trace.nodes)}\")\nprint(f\"Duration: {trace.duration_ms}ms\")\n\n# Export trace\ntrace.to_json(\"trace.json\")\ntrace.to_html(\"trace.html\")\n</code></pre>"},{"location":"observability/#tracingobserver","title":"TracingObserver","text":"<p>Combine observability with tracing:</p> <pre><code>from cogent.observability import TracingObserver\n\nobserver = TracingObserver(\n    level=ObservabilityLevel.DEBUG,\n    export_on_complete=True,\n    export_path=\"traces/\",\n)\n\nresult = await agent.run(\"Task\", observer=observer)\n</code></pre>"},{"location":"observability/#metrics","title":"Metrics","text":"<p>Collect and export metrics:</p> <pre><code>from cogent.observability import MetricsCollector, Counter, Gauge, Histogram\n\ncollector = MetricsCollector()\n\n# Counter - monotonically increasing\nrequests = collector.counter(\"requests_total\", \"Total requests\")\nrequests.inc()\nrequests.inc(5)\n\n# Gauge - can go up and down\nactive = collector.gauge(\"active_agents\", \"Currently active agents\")\nactive.set(3)\nactive.inc()\nactive.dec()\n\n# Histogram - distribution of values\nlatency = collector.histogram(\n    \"request_latency_ms\",\n    \"Request latency\",\n    buckets=[10, 50, 100, 500, 1000],\n)\nlatency.observe(42.5)\n\n# Timer context manager\ntimer = collector.timer(\"operation_duration\")\nwith timer:\n    await do_work()\n</code></pre>"},{"location":"observability/#export-metrics","title":"Export Metrics","text":"<pre><code># Prometheus format\nprint(collector.export_prometheus())\n\n# JSON format\nprint(collector.export_json())\n\n# Start Prometheus endpoint\nawait collector.start_server(port=9090)\n</code></pre>"},{"location":"observability/#progress-output","title":"Progress Output","text":"<p>Rich terminal output for agent execution:</p>"},{"location":"observability/#quick-start_1","title":"Quick Start","text":"<pre><code>from cogent import Agent\n\n# Simple verbose flag\nagent = Agent(name=\"assistant\", model=model, verbose=True)\n\n# Or configure output\nfrom cogent.observability import configure_output, Verbosity\n\nconfigure_output(\n    verbosity=Verbosity.DETAILED,\n    show_timing=True,\n    show_tokens=True,\n)\n</code></pre>"},{"location":"observability/#outputconfig","title":"OutputConfig","text":"<pre><code>from cogent.observability import OutputConfig, Verbosity, OutputFormat, Theme\n\nconfig = OutputConfig(\n    verbosity=Verbosity.DETAILED,\n    format=OutputFormat.RICH,     # or TEXT, JSON, MINIMAL\n    theme=Theme.DARK,             # or LIGHT, MONOCHROME\n    show_thinking=True,\n    show_tool_calls=True,\n    show_timing=True,\n    show_tokens=True,\n)\n\nobserver = Observer(output_config=config)\n</code></pre>"},{"location":"observability/#verbosity-levels","title":"Verbosity Levels","text":"Level Shows <code>SILENT</code> Nothing <code>MINIMAL</code> Start/complete only <code>NORMAL</code> Progress + results <code>DETAILED</code> + Tool calls <code>DEBUG</code> + Thinking/reasoning <code>TRACE</code> Everything"},{"location":"observability/#progresstracker","title":"ProgressTracker","text":"<pre><code>from cogent.observability import ProgressTracker, ProgressEvent\n\ntracker = ProgressTracker()\n\n# Manual progress updates\ntracker.update(ProgressEvent(\n    type=\"task_started\",\n    agent=\"researcher\",\n    message=\"Starting research...\",\n))\n\ntracker.update(ProgressEvent(\n    type=\"tool_called\",\n    agent=\"researcher\",\n    tool=\"search\",\n    args={\"query\": \"AI news\"},\n))\n\ntracker.complete(result=\"Research complete\")\n</code></pre>"},{"location":"observability/#dashboard","title":"Dashboard","text":"<p>Real-time monitoring UI:</p> <pre><code>from cogent.observability import Dashboard, DashboardConfig\n\nconfig = DashboardConfig(\n    port=8080,\n    refresh_rate_ms=1000,\n    show_events=True,\n    show_metrics=True,\n    show_traces=True,\n)\n\ndashboard = Dashboard(config)\nawait dashboard.start()\n\n# Dashboard available at http://localhost:8080\n</code></pre>"},{"location":"observability/#websocket-streaming","title":"WebSocket Streaming","text":"<p>Real-time event streaming:</p> <pre><code>from cogent.observability import WebSocketServer, start_websocket_server\n\n# Start server\nserver = await start_websocket_server(port=8765)\n\n# Or use the handler directly with your own server\nfrom cogent.observability import websocket_handler\n\n# In your WebSocket endpoint:\nasync def handle_client(websocket):\n    await websocket_handler(websocket)\n</code></pre>"},{"location":"observability/#client-connection","title":"Client Connection","text":"<pre><code>// JavaScript client\nconst ws = new WebSocket('ws://localhost:8765');\n\nws.onmessage = (event) =&gt; {\n    const data = JSON.parse(event.data);\n    console.log('Event:', data.type, data.data);\n};\n</code></pre>"},{"location":"observability/#inspectors","title":"Inspectors","text":"<p>Inspect system state at runtime:</p> <pre><code>from cogent.observability import SystemInspector, AgentInspector\n\n# System-wide inspection\ninspector = SystemInspector()\nprint(inspector.summary())\nprint(inspector.active_agents())\nprint(inspector.recent_events(limit=10))\n\n# Agent-specific inspection\nagent_inspector = AgentInspector(agent)\nprint(agent_inspector.state())\nprint(agent_inspector.history())\nprint(agent_inspector.tools())\n</code></pre>"},{"location":"observability/#api-reference","title":"API Reference","text":""},{"location":"observability/#core-classes","title":"Core Classes","text":"Class Description <code>Observer</code> Unified observability interface <code>TraceBus</code> Central pub/sub system <code>Event</code> Immutable event record <code>Tracer</code> Distributed tracing <code>MetricsCollector</code> Metrics collection <code>ProgressTracker</code> Progress output <code>Dashboard</code> Real-time monitoring UI"},{"location":"observability/#event-handlers_1","title":"Event Handlers","text":"Class Description <code>ConsoleEventHandler</code> Print to terminal <code>FileEventHandler</code> Log to file <code>FilteringEventHandler</code> Filter events <code>MetricsEventHandler</code> Collect metrics"},{"location":"observability/#tracing","title":"Tracing","text":"Class Description <code>Span</code> A single traced operation <code>SpanContext</code> Context for distributed tracing <code>SpanKind</code> Type of span (SERVER, CLIENT, etc.) <code>ExecutionTracer</code> Detailed execution tracing <code>TracingObserver</code> Combined observer + tracer"},{"location":"observability/#metrics_1","title":"Metrics","text":"Class Description <code>Counter</code> Monotonically increasing counter <code>Gauge</code> Value that can go up/down <code>Histogram</code> Distribution of values <code>Timer</code> Duration measurement"},{"location":"observability/#progress","title":"Progress","text":"Class Description <code>OutputConfig</code> Output configuration <code>Verbosity</code> Verbosity levels <code>OutputFormat</code> Output formats (RICH, TEXT, JSON) <code>Theme</code> Color themes <code>ProgressEvent</code> Progress update event"},{"location":"resilience/","title":"Tool Resilience &amp; Recovery","text":"<p>Cogent provides production-grade resilience features that automatically handle transient failures, prevent cascading failures, and enable graceful degradation.</p>"},{"location":"resilience/#quick-start","title":"Quick Start","text":"<pre><code>from cogent import Agent\nfrom cogent.agent.resilience import ResilienceConfig\n\n# Default: 3 retries with exponential backoff (ENABLED by default)\nagent = Agent(name=\"MyAgent\", model=model, tools=[...])\n\n# Aggressive: 5 retries for flaky services\nagent = Agent(\n    name=\"ReliableAgent\",\n    model=model,\n    tools=[...],\n    resilience=ResilienceConfig.aggressive()\n)\n\n# Fast-fail: No retries for time-sensitive operations\nagent = Agent(\n    name=\"FastAgent\",\n    model=model,\n    tools=[...],\n    resilience=ResilienceConfig.fast_fail()\n)\n</code></pre>"},{"location":"resilience/#default-configuration","title":"Default Configuration","text":"<p>Agents have resilience ENABLED by default with sensible production settings:</p> <ul> <li>Max Retries: 3 attempts per tool call</li> <li>Strategy: Exponential backoff with jitter</li> <li>Timeout: 60 seconds per tool call  </li> <li>Circuit Breaker: Enabled (protects against cascading failures)</li> <li>Failure Learning: Enabled (learns from patterns)</li> </ul>"},{"location":"resilience/#when-resilience-applies","title":"When Resilience Applies","text":""},{"location":"resilience/#direct-tool-calls-agentact","title":"\u2705 Direct Tool Calls (<code>agent.act()</code>)","text":"<p>The resilience layer automatically retries failed tool calls:</p> <pre><code># Resilience applies here - automatic retry with exponential backoff\nresult = await agent.act(\n    tool_name=\"web_search\",\n    args={\"query\": \"Python tutorials\"},\n    use_resilience=True  # Default\n)\n</code></pre> <p>Behavior: Tool failures are retried transparently. The LLM never sees transient errors.</p>"},{"location":"resilience/#llm-driven-execution-agentrun","title":"\ud83e\udd16 LLM-Driven Execution (<code>agent.run()</code>)","text":"<p>When the LLM decides which tools to call, resilience does NOT apply by default:</p> <pre><code># LLM sees tool errors and decides retry strategy\nresult = await agent.run(\"Search for Python tutorials\")\n</code></pre> <p>Behavior: Tool errors are returned to the LLM, which can decide whether to retry, try a different approach, or give up.</p> <p>Why? This gives the LLM flexibility to adapt its strategy based on errors rather than blindly retrying.</p>"},{"location":"resilience/#workflows-programmatic-usage","title":"\ud83d\udd27 Workflows &amp; Programmatic Usage","text":"<p>Use <code>.act()</code> in workflows and scripts to get automatic resilience:</p> <pre><code># In a workflow - resilience applies to each tool call\nasync def data_pipeline(agent: Agent):\n    raw_data = await agent.act(\"fetch_data\", {\"source\": \"api\"})\n    cleaned = await agent.act(\"clean_data\", {\"data\": raw_data})\n    result = await agent.act(\"analyze_data\", {\"data\": cleaned})\n    return result\n</code></pre>"},{"location":"resilience/#resilience-features","title":"Resilience Features","text":""},{"location":"resilience/#1-automatic-retry-with-exponential-backoff","title":"1. Automatic Retry with Exponential Backoff","text":"<pre><code>from cogent.agent.resilience import RetryPolicy, RetryStrategy\n\npolicy = RetryPolicy(\n    max_retries=5,\n    strategy=RetryStrategy.EXPONENTIAL_JITTER,\n    base_delay=0.5,  # Start with 0.5s delay\n    max_delay=30.0,  # Cap at 30s\n    jitter_factor=0.3,  # Add 30% randomness\n)\n\nconfig = ResilienceConfig(retry_policy=policy)\nagent = Agent(name=\"Agent\", model=model, resilience=config)\n</code></pre> <p>Available Strategies: - <code>EXPONENTIAL_JITTER</code>: Exponential backoff with random jitter (recommended) - <code>EXPONENTIAL</code>: Pure exponential backoff (2^attempt * base_delay) - <code>LINEAR</code>: Linear increase (attempt * base_delay) - <code>FIXED</code>: Fixed delay between retries - <code>NONE</code>: No backoff (fail fast)</p>"},{"location":"resilience/#2-circuit-breaker","title":"2. Circuit Breaker","text":"<p>Prevents cascading failures by temporarily disabling failing tools:</p> <pre><code>config = ResilienceConfig(\n    circuit_breaker_enabled=True,  # Default\n    circuit_breaker_config=CircuitBreaker(\n        failure_threshold=3,  # Open after 3 failures\n        reset_timeout=30.0,   # Wait 30s before testing recovery\n    )\n)\n</code></pre> <p>States: - CLOSED: Normal operation, requests pass through - OPEN: Blocking requests after repeated failures - HALF_OPEN: Testing if service recovered</p>"},{"location":"resilience/#3-fallback-tool-chains","title":"3. Fallback Tool Chains","text":"<p>Gracefully degrade to backup tools when primary fails:</p> <pre><code>agent = Agent(\n    name=\"Agent\",\n    model=model,\n    tools=[primary_search, backup_search],\n    resilience=ResilienceConfig(fallback_enabled=True),\n)\n\n# Register fallback chain\nagent.config = agent.config.with_fallbacks({\n    \"primary_search\": [\"backup_search\"]\n})\nagent._setup_resilience()  # Re-init resilience with new config\n\n# If primary_search fails after retries, backup_search is tried\nresult = await agent.act(\"primary_search\", {\"query\": \"test\"})\n</code></pre>"},{"location":"resilience/#4-failure-learning","title":"4. Failure Learning","text":"<p>Learn from failures to adapt future behavior:</p> <pre><code>config = ResilienceConfig(\n    learning_enabled=True,  # Default\n)\n</code></pre> <p>The resilience layer tracks: - Common failure patterns - Error message patterns (rate limits, server errors, etc.) - Recovery methods that worked - Tool reliability over time</p>"},{"location":"resilience/#pre-built-configurations","title":"Pre-Built Configurations","text":""},{"location":"resilience/#default-balanced","title":"Default / Balanced","text":"<pre><code>ResilienceConfig()  # or ResilienceConfig.balanced()\n</code></pre> <ul> <li>3 retries</li> <li>Exponential backoff with jitter</li> <li>60s timeout</li> <li>Circuit breaker enabled</li> <li>Failure learning enabled</li> </ul> <p>Use for: General purpose workflows, balanced reliability/latency</p>"},{"location":"resilience/#aggressive","title":"Aggressive","text":"<pre><code>ResilienceConfig.aggressive()\n</code></pre> <ul> <li>5 retries</li> <li>Exponential backoff with jitter</li> <li>120s timeout</li> <li>Circuit breaker enabled</li> <li>Failure learning enabled</li> <li>Fallback enabled</li> </ul> <p>Use for: Flaky external APIs, critical data retrieval, background jobs</p>"},{"location":"resilience/#fast-fail","title":"Fast-Fail","text":"<pre><code>ResilienceConfig.fast_fail()\n</code></pre> <ul> <li>0 retries</li> <li>10s timeout</li> <li>No circuit breaker</li> <li>No learning</li> <li>No fallbacks</li> </ul> <p>Use for: Real-time paths, user-facing operations, testing</p>"},{"location":"resilience/#custom-configuration","title":"Custom Configuration","text":"<p>Full control over all resilience parameters:</p> <pre><code>from cogent.agent.resilience import (\n    ResilienceConfig,\n    RetryPolicy,\n    RetryStrategy,\n    CircuitBreaker,\n    RecoveryAction,\n)\n\ncustom_config = ResilienceConfig(\n    retry_policy=RetryPolicy(\n        max_retries=4,\n        strategy=RetryStrategy.EXPONENTIAL_JITTER,\n        base_delay=1.0,\n        max_delay=60.0,\n        jitter_factor=0.25,\n    ),\n    circuit_breaker_enabled=True,\n    circuit_breaker_config=CircuitBreaker(\n        failure_threshold=5,\n        success_threshold=2,\n        reset_timeout=60.0,\n    ),\n    fallback_enabled=True,\n    learning_enabled=True,\n    timeout_seconds=90.0,\n    on_failure=RecoveryAction.SKIP,  # or RETRY, FALLBACK, ABORT, ADAPT\n)\n\nagent = Agent(name=\"Agent\", model=model, resilience=custom_config)\n</code></pre>"},{"location":"resilience/#per-tool-configuration","title":"Per-Tool Configuration","text":"<p>Override resilience settings for specific tools:</p> <pre><code>config = ResilienceConfig()\n\n# Add per-tool overrides\nconfig.tool_configs[\"expensive_tool\"] = {\n    \"timeout_seconds\": 300.0,  # 5 min timeout\n    \"retry_policy\": RetryPolicy.no_retry(),  # Don't retry\n}\n\nconfig.tool_configs[\"flaky_api\"] = {\n    \"retry_policy\": RetryPolicy.aggressive(),  # More retries\n}\n</code></pre>"},{"location":"resilience/#monitoring-resilience","title":"Monitoring Resilience","text":"<p>Track retries and recovery with observability:</p> <pre><code>from cogent.observability import Observer, ObservabilityLevel\n\nobserver = Observer(\n    level=ObservabilityLevel.DEBUG,  # See retry events\n    show_timestamps=True,\n    show_duration=True,\n)\nagent.add_observer(observer)\n\n# Output shows retry attempts:\n# [12:00:01] \ud83d\udd04 Retry 1/3 for web_search in 1.2s\n# [12:00:03] \ud83d\udd04 Retry 2/3 for web_search in 2.5s\n# [12:00:06] \u2705 Recovered via retry\n</code></pre>"},{"location":"resilience/#example-production-api-integration","title":"Example: Production API Integration","text":"<pre><code>from cogent import Agent\nfrom cogent.agent.resilience import ResilienceConfig, RetryPolicy\nfrom cogent.tools import tool\n\n@tool\nasync def external_api_call(query: str) -&gt; dict:\n    \"\"\"Call external API (may have transient failures).\"\"\"\n    # API implementation\n    pass\n\n# Production agent with resilience\nagent = Agent(\n    name=\"APIAgent\",\n    model=model,\n    tools=[external_api_call],\n    resilience=ResilienceConfig(\n        retry_policy=RetryPolicy(\n            max_retries=5,\n            base_delay=1.0,\n            max_delay=30.0,\n        ),\n        circuit_breaker_enabled=True,\n        learning_enabled=True,\n        timeout_seconds=120.0,\n    ),\n)\n\n# Direct call - automatic retry on transient failures\nresult = await agent.act(\n    tool_name=\"external_api_call\",\n    args={\"query\": \"production data\"},\n)\n</code></pre>"},{"location":"resilience/#best-practices","title":"Best Practices","text":"<ol> <li>Use <code>.act()</code> for reliability: Direct tool calls get automatic resilience</li> <li>Choose the right config: Default for most cases, Aggressive for flaky services, Fast-fail for testing</li> <li>Monitor with Observer: Track retry patterns and adjust configuration</li> <li>Set appropriate timeouts: Balance between giving tools time and failing fast</li> <li>Use fallbacks for critical paths: Define backup tools for important operations</li> <li>Test resilience: Verify retry behavior with simulated failures</li> <li>Circuit breakers prevent cascades: Let failing services recover instead of hammering them</li> </ol>"},{"location":"resilience/#troubleshooting","title":"Troubleshooting","text":""},{"location":"resilience/#tool-keeps-failing-despite-retries","title":"Tool keeps failing despite retries","text":"<p>Check: - Tool signature matches between primary and fallback - Timeout is sufficient for tool execution - Error is retryable (not ValueError, TypeError, etc.) - Circuit breaker hasn't opened</p>"},{"location":"resilience/#resilience-not-applying","title":"Resilience not applying","text":"<p>Ensure: - Using <code>agent.act()</code> not <code>agent.run()</code> for direct resilience - <code>use_resilience=True</code> (default) - Agent has resilience config set - Tool is registered with agent</p>"},{"location":"resilience/#too-many-retries-slowing-down","title":"Too many retries slowing down","text":"<p>Consider: - Reduce <code>max_retries</code> - Use <code>Fast-fail</code> config for testing - Check if error is actually retryable - Increase <code>base_delay</code> to space out retries</p>"},{"location":"resilience/#see-also","title":"See Also","text":"<ul> <li>tool_resilience.py - Complete working examples</li> <li>docs/observability.md - Monitoring and tracing</li> <li>docs/tool-building.md - Creating tools</li> </ul>"},{"location":"retrievers/","title":"Retriever Guide","text":"<p>Cogent provides a comprehensive retrieval system with multiple strategies for different use cases. This guide covers all available retrievers and when to use each.</p>"},{"location":"retrievers/#unified-api","title":"Unified API","text":"<p>All retrievers share a unified <code>retrieve()</code> API with optional scoring:</p> <pre><code># Get documents only (default)\ndocs = await retriever.retrieve(\"query\", k=5)\n\n# Get documents with relevance scores\nresults = await retriever.retrieve(\"query\", k=5, include_scores=True)\nfor r in results:\n    print(f\"{r.score:.3f}: {r.document.text[:50]}\")\n\n# With metadata filter\nresults = await retriever.retrieve(\n    \"query\",\n    k=10,\n    filter={\"category\": \"docs\"},\n    include_scores=True,\n)\n\n# Retriever-specific args (e.g., TimeBasedIndex)\nresults = await retriever.retrieve(\n    \"recent news\",\n    k=5,\n    time_range=TimeRange.last_days(30),\n    include_scores=True,\n)\n</code></pre>"},{"location":"retrievers/#overview","title":"Overview","text":"Category Retriever Best For Core <code>DenseRetriever</code> Semantic similarity search <code>BM25Retriever</code> Keyword/lexical matching <code>EnsembleRetriever</code> Combining multiple retrievers (dense + sparse) <code>HybridRetriever</code> Metadata filtering + content search Contextual <code>ParentDocumentRetriever</code> Precise chunks \u2192 full context <code>SentenceWindowRetriever</code> Sentence-level \u2192 paragraph context LLM-Powered <code>SummaryIndex</code> Document summaries <code>TreeIndex</code> Hierarchical summary tree <code>KeywordTableIndex</code> Keyword extraction + lookup <code>KnowledgeGraphIndex</code> Entity-based retrieval <code>SelfQueryRetriever</code> Natural language \u2192 filters Specialized <code>HierarchicalIndex</code> Structured docs (markdown/html) <code>TimeBasedIndex</code> Recency-aware retrieval <code>MultiRepresentationIndex</code> Multiple embeddings per doc"},{"location":"retrievers/#core-retrievers","title":"Core Retrievers","text":""},{"location":"retrievers/#denseretriever","title":"DenseRetriever","text":"<p>Semantic search using vector embeddings. The most common retriever for general RAG applications.</p> <pre><code>from cogent.retriever import DenseRetriever\nfrom cogent.vectorstore import VectorStore\n\n# Create vectorstore and retriever\nvectorstore = VectorStore(embeddings=embeddings)\nawait vectorstore.add_texts([\n    \"Python is a programming language\",\n    \"Machine learning uses algorithms\",\n    \"Neural networks learn from data\",\n])\n\nretriever = DenseRetriever(vectorstore)\nresults = await retriever.retrieve(\"AI and deep learning\", k=2)\n</code></pre> <p>When to use: - General semantic search - Finding conceptually similar content - When exact keyword matching isn't required</p>"},{"location":"retrievers/#bm25retriever","title":"BM25Retriever","text":"<p>Lexical retrieval using the BM25 algorithm. Fast, interpretable, and excellent for keyword queries.</p> <pre><code>from cogent.retriever import BM25Retriever\nfrom cogent.vectorstore import Document\n\n# Create documents\ndocuments = [\n    Document(text=\"Python programming tutorial\", metadata={\"type\": \"tutorial\"}),\n    Document(text=\"JavaScript web development\", metadata={\"type\": \"tutorial\"}),\n    Document(text=\"Machine learning with Python\", metadata={\"type\": \"guide\"}),\n]\n\n# Create BM25 retriever with documents\nretriever = BM25Retriever(documents, k1=1.5, b=0.75)\n\n# Or add documents later\nretriever = BM25Retriever()\nretriever.add_documents(documents)\n\n# Keyword-based search\nresults = await retriever.retrieve(\"Python tutorial\", k=2)\n</code></pre> <p>When to use: - Exact keyword matching is important - Domain-specific terminology - Fast, interpretable results needed - No embedding model available</p>"},{"location":"retrievers/#hybridretriever","title":"HybridRetriever","text":"<p>Combines metadata search with content search. Wraps any retriever and boosts/filters by metadata fields.</p> <pre><code>from cogent.retriever import HybridRetriever, DenseRetriever, MetadataMatchMode\n\n# Wrap any content retriever\ncontent_retriever = DenseRetriever(vectorstore)\n\nhybrid = HybridRetriever(\n    retriever=content_retriever,\n    metadata_fields=[\"category\", \"author\", \"department\"],\n    metadata_weight=0.3,   # 30% from metadata match\n    content_weight=0.7,    # 70% from content match\n    mode=MetadataMatchMode.BOOST,  # or ALL, ANY\n)\n\n# Query searches both metadata and content\nresults = await hybrid.retrieve(\"machine learning best practices\", k=5)\n\n# Each result has enriched metadata\nfor r in results:\n    print(f\"Content score: {r.metadata['content_score']}\")\n    print(f\"Metadata score: {r.metadata['metadata_score']}\")\n</code></pre> <p>Matching modes: - <code>BOOST</code>: Metadata matches increase score (no filtering) - <code>ALL</code>: Only return docs matching ALL metadata terms - <code>ANY</code>: Return docs matching ANY metadata term</p> <p>When to use: - Documents have rich metadata (author, category, date) - Users search by both content and attributes - Want to boost relevant metadata matches</p>"},{"location":"retrievers/#ensembleretriever","title":"EnsembleRetriever","text":"<p>Combine any number of retrievers with configurable fusion strategies.</p> <pre><code>from cogent.retriever import (\n    EnsembleRetriever,\n    DenseRetriever,\n    BM25Retriever,\n)\n\n# Combine multiple retrievers\nensemble = EnsembleRetriever(\n    retrievers=[\n        DenseRetriever(vectorstore_openai),    # OpenAI embeddings\n        DenseRetriever(vectorstore_cohere),    # Cohere embeddings\n        BM25Retriever(documents),              # Lexical\n    ],\n    weights=[0.4, 0.4, 0.2],\n    fusion=\"rrf\",  # or \"linear\", \"max\", \"voting\"\n)\n\nresults = await ensemble.retrieve(\"query\", k=10)\n</code></pre> <p>Fusion strategies: - <code>rrf</code> (Reciprocal Rank Fusion): Best for diverse retrievers (default) - <code>linear</code>: Weighted score combination - <code>max</code>: Take highest score per document - <code>voting</code>: Count how many retrievers found each doc</p> <p>Tip: The RAG capability accepts <code>retrievers=</code> directly and creates an EnsembleRetriever internally.</p>"},{"location":"retrievers/#contextual-retrievers","title":"Contextual Retrievers","text":""},{"location":"retrievers/#parentdocumentretriever","title":"ParentDocumentRetriever","text":"<p>Index small chunks for precise matching, but return full parent documents for context.</p> <pre><code>from cogent.retriever import ParentDocumentRetriever\n\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    chunk_size=500,     # Small chunks for precise matching\n    chunk_overlap=50,\n)\n\n# Add full documents (automatically chunked)\nawait retriever.add_documents(large_documents)\n\n# Search finds chunks, returns parents\nresults = await retriever.retrieve(\"specific concept\", k=3)\n# Each result is a full document, not a chunk\n</code></pre> <p>When to use: - LLM needs more context than a single chunk - Documents have interconnected information - You want precise matching with comprehensive results</p>"},{"location":"retrievers/#sentencewindowretriever","title":"SentenceWindowRetriever","text":"<p>Index individual sentences, but return with surrounding context.</p> <pre><code>from cogent.retriever import SentenceWindowRetriever\n\nretriever = SentenceWindowRetriever(\n    vectorstore=vectorstore,\n    window_size=2,  # 2 sentences before and after\n)\n\nawait retriever.add_documents(documents)\n\n# Precise sentence match with context\nresults = await retriever.retrieve(\"specific fact\", k=3, include_scores=True)\nfor r in results:\n    print(f\"Matched: {r.metadata['matched_sentence']}\")\n    print(f\"Context: {r.document.text}\")  # Full window\n</code></pre> <p>When to use: - Need precise sentence-level matching - Want to return paragraph-level context - Fact-checking or citation tasks</p>"},{"location":"retrievers/#llm-powered-indexes","title":"LLM-Powered Indexes","text":""},{"location":"retrievers/#summaryindex","title":"SummaryIndex","text":"<p>Generate LLM summaries of documents for efficient high-level retrieval.</p> <pre><code>from cogent.retriever import SummaryIndex\n\nindex = SummaryIndex(\n    llm=model,\n    vectorstore=vectorstore,\n    extract_entities=True,   # For knowledge graph\n    extract_keywords=True,\n)\n\nawait index.add_documents(long_documents)\n\n# Search by summary\nresults = await index.retrieve(\"machine learning concepts\", k=3)\n\n# Access extracted entities for KG integration\nfor doc_id, summary in index.summaries.items():\n    print(f\"Keywords: {summary.keywords}\")\n    print(f\"Entities: {summary.entities}\")\n</code></pre> <p>When to use: - Long documents that don't fit in embeddings well - Need document-level topics quickly - Building knowledge graphs from documents</p>"},{"location":"retrievers/#treeindex","title":"TreeIndex","text":"<p>Hierarchical tree of summaries for very large documents or corpora.</p> <pre><code>from cogent.retriever import TreeIndex\n\nindex = TreeIndex(\n    llm=model,\n    vectorstore=vectorstore,\n    max_children=5,      # Children per node\n    max_depth=3,         # Tree depth\n)\n\nawait index.add_documents(very_large_documents)\n\n# Efficient tree traversal\nresults = await index.retrieve(\"specific topic\", k=5)\n</code></pre> <p>When to use: - Very large documents (books, manuals) - Corpus-level search across many documents - When full indexing is too slow/expensive</p>"},{"location":"retrievers/#keywordtableindex","title":"KeywordTableIndex","text":"<p>Extract keywords with LLM and build inverted index for fast lookup.</p> <pre><code>from cogent.retriever import KeywordTableIndex\n\nindex = KeywordTableIndex(\n    llm=model,\n    max_keywords_per_doc=10,\n)\n\nawait index.add_documents(documents)\n\n# Fast keyword-based lookup\nresults = await index.retrieve(\"Python machine learning\", k=5)\n\n# Access keyword table\nprint(index.keyword_table)  # {\"python\": [doc_ids...], \"ml\": [...]}\n</code></pre> <p>When to use: - Domain with specific terminology - Fast keyword lookup needed - Interpretable retrieval wanted</p>"},{"location":"retrievers/#selfqueryretriever","title":"SelfQueryRetriever","text":"<p>LLM parses natural language queries into semantic search + metadata filters.</p> <pre><code>from cogent.retriever import SelfQueryRetriever, AttributeInfo\n\nretriever = SelfQueryRetriever(\n    vectorstore=vectorstore,\n    llm=model,\n    attribute_info=[\n        AttributeInfo(\"category\", \"Document category\", \"string\"),\n        AttributeInfo(\"year\", \"Publication year\", \"integer\"),\n        AttributeInfo(\"author\", \"Author name\", \"string\"),\n    ],\n)\n\n# Natural language with implicit filters\nresults = await retriever.retrieve(\n    \"research papers about AI from 2024 by OpenAI\"\n)\n# LLM extracts: semantic=\"AI research papers\"\n#              filter={\"year\": 2024, \"author\": \"OpenAI\"}\n</code></pre> <p>When to use: - Users query in natural language - Documents have filterable metadata - Want to combine semantic + structured search</p>"},{"location":"retrievers/#specialized-indexes","title":"Specialized Indexes","text":""},{"location":"retrievers/#hierarchicalindex","title":"HierarchicalIndex","text":"<p>Respect and leverage document structure (headers, sections).</p> <pre><code>from cogent.retriever import HierarchicalIndex\n\nindex = HierarchicalIndex(\n    vectorstore=vectorstore,\n    llm=model,\n    structure_type=\"markdown\",  # or \"html\"\n    top_k_sections=3,\n    chunks_per_section=3,\n)\n\nawait index.add_documents(structured_docs)\n\n# Find section first, then relevant chunks\nresults = await index.retrieve(\"installation\", k=5)\nfor r in results:\n    print(f\"Section: {r.metadata['section_title']}\")\n    print(f\"Path: {r.metadata['hierarchy_path']}\")\n</code></pre> <p>When to use: - Well-structured documents (docs, manuals, specs) - Want to respect document organization - Need section-level context</p>"},{"location":"retrievers/#timebasedindex","title":"TimeBasedIndex","text":"<p>Prioritize recent information with time-decay scoring.</p> <pre><code>from cogent.retriever import TimeBasedIndex, TimeRange, DecayFunction\n\nindex = TimeBasedIndex(\n    vectorstore=vectorstore,\n    decay_function=DecayFunction.EXPONENTIAL,\n    decay_rate=0.01,  # Halve score every ~70 days\n    auto_extract_timestamps=True,\n)\n\nawait index.add_documents(news_articles)\n\n# Recent docs score higher\nresults = await index.retrieve(\"market trends\", k=5)\n\n# Filter by time range\nresults = await index.retrieve(\n    \"company policy\",\n    time_range=TimeRange.last_days(30),\n)\n\n# Point-in-time query\nresults = await index.retrieve(\n    \"regulations\",\n    time_range=TimeRange.year(2023),\n)\n</code></pre> <p>Decay functions: - <code>EXPONENTIAL</code>: Smooth decay over time - <code>LINEAR</code>: Linear decrease - <code>STEP</code>: Full score within window, zero outside - <code>LOGARITHMIC</code>: Slow initial decay - <code>NONE</code>: No decay, just filtering</p> <p>When to use: - News, articles, changelogs - Evolving knowledge bases - Time-sensitive information</p>"},{"location":"retrievers/#multirepresentationindex","title":"MultiRepresentationIndex","text":"<p>Store multiple embeddings per document for diverse query handling.</p> <pre><code>from cogent.retriever import MultiRepresentationIndex, QueryType\n\nindex = MultiRepresentationIndex(\n    vectorstore=vectorstore,\n    llm=model,\n    representations=[\"original\", \"summary\", \"detailed\", \"questions\"],\n)\n\nawait index.add_documents(documents)\n\n# Auto-detect query type\nresults = await index.retrieve(\"What is machine learning?\")\n\n# Force specific representation\nresults = await index.retrieve(\n    \"backpropagation gradient calculation\",\n    query_type=QueryType.SPECIFIC,  # Uses detailed representation\n)\n\n# Search all and fuse\nresults = await index.retrieve(\n    \"AI applications\",\n    search_all=True,\n)\n</code></pre> <p>Representations: - <code>original</code>: Raw document embedding - <code>summary</code>: Conceptual summary - <code>detailed</code>: Technical details - <code>keywords</code>: Key terms - <code>questions</code>: Hypothetical Q&amp;A - <code>entities</code>: Named entities</p> <p>When to use: - Diverse query styles expected - Technical/specialized domains - Want maximum recall</p>"},{"location":"retrievers/#rerankers","title":"Rerankers","text":"<p>Rerankers improve retrieval quality by re-scoring initial results.</p> <pre><code>from cogent.retriever import (\n    DenseRetriever,\n    CrossEncoderReranker,\n    CohereReranker,\n    LLMReranker,\n)\n\n# Initial retrieval\nretriever = DenseRetriever(vectorstore)\ninitial_docs = await retriever.retrieve(query, k=20)  # Get documents\n\n# Rerank with cross-encoder (local)\nreranker = CrossEncoderReranker(model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\nreranked = await reranker.rerank(query, initial_docs, top_n=5)\n\n# Or Cohere Rerank API\nreranker = CohereReranker(api_key=\"...\")\nreranked = await reranker.rerank(query, initial_docs, top_n=5)\n\n# Or any LLM\nreranker = LLMReranker(llm=model)\nreranked = await reranker.rerank(query, initial_docs, top_n=5)\n</code></pre> <p>Available rerankers: - <code>CrossEncoderReranker</code>: Local cross-encoder models - <code>FlashRankReranker</code>: Lightweight, fast local reranker - <code>CohereReranker</code>: Cohere Rerank API - <code>LLMReranker</code>: Any LLM for pointwise scoring - <code>ListwiseLLMReranker</code>: LLM ranks all docs at once</p>"},{"location":"retrievers/#utilities","title":"Utilities","text":""},{"location":"retrievers/#fusion-functions","title":"Fusion Functions","text":"<pre><code>from cogent.retriever import fuse_results, FusionStrategy\n\n# Fuse results from multiple retrievers\nfused = fuse_results(\n    [results_1, results_2, results_3],\n    strategy=FusionStrategy.RRF,\n    weights=[0.5, 0.3, 0.2],\n    k=10,\n)\n</code></pre>"},{"location":"retrievers/#score-normalization","title":"Score Normalization","text":"<pre><code>from cogent.retriever import normalize_scores\n\n# Normalize scores to 0-1 range\nnormalized = normalize_scores(results)\n</code></pre>"},{"location":"retrievers/#deduplication","title":"Deduplication","text":"<pre><code>from cogent.retriever import deduplicate_results\n\n# Remove duplicate documents\nunique = deduplicate_results(results, by=\"content\")  # or \"id\"\n</code></pre>"},{"location":"retrievers/#citations-and-formatting","title":"Citations and Formatting","text":"<p>For RAG applications, use these utilities to prepare results for LLM prompts:</p> <pre><code>from cogent.retriever import (\n    add_citations,\n    format_context,\n    format_citations_reference,\n    filter_by_score,\n    top_k,\n)\n\n# Retrieve results\nresults = await retriever.retrieve(query, k=10, include_scores=True)\n\n# Filter low-quality results\nresults = filter_by_score(results, min_score=0.5)\nresults = top_k(results, k=5)\n\n# Add citation markers \u00ab1\u00bb, \u00ab2\u00bb, etc.\nresults = add_citations(results)\n# results[0].metadata[\"citation\"] == \"\u00ab1\u00bb\"\n\n# Format as context string for LLM prompt\ncontext = format_context(results)\n# Output:\n# \u00ab1\u00bb [Source: doc.pdf]\n# This is the first chunk of text...\n#\n# ---\n#\n# \u00ab2\u00bb [Source: other.pdf]\n# This is the second chunk...\n\n# Generate citations reference section\nreference = format_citations_reference(results)\n# Output:\n# Sources:\n# \u00ab1\u00bb doc.pdf: This is a preview of the first document...\n# \u00ab2\u00bb other.pdf: This is a preview of the second...\n</code></pre> <p>Example RAG prompt construction:</p> <pre><code>query = \"What are the key findings?\"\nresults = await retriever.retrieve(query, k=5, include_scores=True)\nresults = filter_by_score(results, min_score=0.5)\nresults = add_citations(results)\ncontext = format_context(results)\n\nprompt = f\"\"\"Based on the following context, answer the question.\nUse citation markers like \u00ab1\u00bb to reference sources.\n\nContext:\n{context}\n\nQuestion: {query}\n\nAnswer:\"\"\"\n</code></pre>"},{"location":"retrievers/#choosing-a-retriever","title":"Choosing a Retriever","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    What's your use case?                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u25bc                 \u25bc                 \u25bc\n     General RAG        Specialized        Advanced\n            \u2502                 \u2502                 \u2502\n            \u25bc                 \u2502                 \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502                 \u2502\n    \u2502 HybridRetriever\u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2524                 \u2502\n    \u2502   (default)    \u2502        \u2502                 \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502                 \u2502\n                              \u25bc                 \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n                    \u2502  Time-sensitive?    \u2502    \u2502\n                    \u2502  \u2192 TimeBasedIndex   \u2502    \u2502\n                    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524    \u2502\n                    \u2502  Structured docs?   \u2502    \u2502\n                    \u2502  \u2192 HierarchicalIndex\u2502    \u2502\n                    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524    \u2502\n                    \u2502  Need full context? \u2502    \u2502\n                    \u2502  \u2192 ParentDocument   \u2502    \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n                                               \u25bc\n                              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                              \u2502 Multiple embedding models?  \u2502\n                              \u2502 \u2192 EnsembleRetriever         \u2502\n                              \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n                              \u2502 Natural language filters?   \u2502\n                              \u2502 \u2192 SelfQueryRetriever        \u2502\n                              \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n                              \u2502 Very long documents?        \u2502\n                              \u2502 \u2192 SummaryIndex / TreeIndex  \u2502\n                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"retrievers/#performance-tips","title":"Performance Tips","text":"<ol> <li>Start with EnsembleRetriever(dense + sparse) - Best default for most cases</li> <li>Use rerankers - Cheap way to improve quality</li> <li>Retrieve more, rerank less - Get top 20-50, rerank to top 5</li> <li>Cache embeddings - Reuse for similar queries</li> <li>Batch operations - Add documents in batches</li> <li>Add HybridRetriever for metadata - When you have structured metadata to filter/boost</li> </ol>"},{"location":"streaming/","title":"Streaming","text":"<p>Real-time token-by-token streaming from agent executions.</p>"},{"location":"streaming/#overview","title":"Overview","text":"<p>Streaming enables agents to yield output progressively as tokens are generated, rather than waiting for complete responses. This provides:</p> <ul> <li>Real-time feedback during long-running agent operations</li> <li>Better UX with progressive output display</li> <li>Lower perceived latency by showing immediate progress</li> <li>Cancellation support for in-flight operations</li> </ul>"},{"location":"streaming/#quick-start","title":"Quick Start","text":"<pre><code>from cogent import Agent\n\nagent = Agent(\n    name=\"writer\",\n    model=\"gpt-4o\",\n    stream=True,  # Enable streaming\n)\n\n# Stream tokens as they arrive\nasync for chunk in agent.run_stream(\"Write a poem about AI\"):\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"streaming/#agent-streaming","title":"Agent Streaming","text":""},{"location":"streaming/#enabling-streaming","title":"Enabling Streaming","text":"<p>Two ways to enable streaming:</p> <pre><code># Option 1: Set on agent creation\nagent = Agent(\n    name=\"writer\",\n    model=\"gpt-4o\",\n    stream=True,\n)\n\nresult = await agent.run(\"Write about AI\")  # Still works normally\nasync for chunk in agent.run_stream(\"Write a poem\"):  # Stream tokens\n    print(chunk.content, end=\"\", flush=True)\n\n# Option 2: Stream on demand (any agent)\nagent = Agent(name=\"writer\", model=\"gpt-4o\")\n\nasync for chunk in agent.run_stream(\"Write a poem\"):\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"streaming/#streamchunk","title":"StreamChunk","text":"<p>Each streaming chunk contains:</p> <pre><code>@dataclass\nclass StreamChunk:\n    content: str              # Token content\n    delta: str                # Incremental text (same as content)\n    is_final: bool           # Last chunk?\n    finish_reason: str | None # Why stopped (stop, length, tool_calls)\n    metadata: dict | None     # Token usage, model info\n</code></pre> <p>Key Properties: - <code>content</code> \u2014 The token text - <code>delta</code> \u2014 Incremental text (alias for content) - <code>is_final</code> \u2014 True when streaming completes - <code>finish_reason</code> \u2014 \"stop\" (complete), \"length\" (max tokens), \"tool_calls\" (tool invocation) - <code>metadata</code> \u2014 Includes token usage when available</p>"},{"location":"streaming/#streaming-with-metadata","title":"Streaming with Metadata","text":"<p>Access model metadata during streaming:</p> <pre><code>async for chunk in agent.run_stream(\"Analyze data\"):\n    print(chunk.content, end=\"\")\n\n    if chunk.is_final and chunk.metadata:\n        print(f\"\\nModel: {chunk.metadata.get('model')}\")\n        print(f\"Tokens: {chunk.metadata.get('tokens')}\")\n</code></pre>"},{"location":"streaming/#run_stream-vs-run","title":"run_stream() vs run()","text":"Feature <code>run()</code> <code>run_stream()</code> Returns <code>Response</code> <code>AsyncIterator[StreamChunk]</code> Output Complete final output Progressive tokens Latency Wait for completion Immediate feedback Use Case Batch processing Interactive UX <p>When to use streaming: - Interactive applications (CLIs, web UIs, chatbots) - Long-running agent operations - Progress tracking and status updates - User experience is priority</p> <p>When to use regular run(): - Batch processing or automation - Final result is all that matters - Simpler code (no async iteration)</p>"},{"location":"streaming/#usage-patterns","title":"Usage Patterns","text":""},{"location":"streaming/#basic-streaming","title":"Basic Streaming","text":"<pre><code>from cogent import Agent\n\nagent = Agent(name=\"assistant\", model=\"gpt-4o\")\n\n# Stream tokens as they arrive\nasync for chunk in agent.run_stream(\"Explain quantum computing\"):\n    print(chunk.content, end=\"\", flush=True)\n\nprint()  # Newline at end\n</code></pre>"},{"location":"streaming/#collecting-streamed-output","title":"Collecting Streamed Output","text":"<pre><code>full_response = []\n\nasync for chunk in agent.run_stream(\"Write a story\"):\n    full_response.append(chunk.content)\n    print(chunk.content, end=\"\", flush=True)\n\nfinal_text = \"\".join(full_response)\n</code></pre>"},{"location":"streaming/#progress-tracking","title":"Progress Tracking","text":"<pre><code>async for chunk in agent.run_stream(\"Long analysis task\"):\n    print(chunk.content, end=\"\", flush=True)\n\n    if chunk.is_final:\n        print(\"\\n\u2705 Complete!\")\n</code></pre>"},{"location":"streaming/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    async for chunk in agent.run_stream(\"Query\"):\n        print(chunk.content, end=\"\", flush=True)\n\n        if chunk.finish_reason == \"length\":\n            print(\"\\n\u26a0\ufe0f Response truncated (max tokens reached)\")\nexcept Exception as e:\n    print(f\"\\n\u274c Streaming error: {e}\")\n</code></pre>"},{"location":"streaming/#streaming-with-tools","title":"Streaming with Tools","text":"<p>When an agent calls tools during streaming, the stream may pause while tools execute:</p> <pre><code>from cogent import Agent, tool\n\n@tool\ndef search(query: str) -&gt; str:\n    \"\"\"Search for information.\"\"\"\n    return f\"Results for: {query}\"\n\nagent = Agent(name=\"researcher\", model=\"gpt-4o\", tools=[search])\n\nasync for chunk in agent.run_stream(\"Search for AI news and summarize\"):\n    if chunk.content:\n        print(chunk.content, end=\"\", flush=True)\n\n    if chunk.finish_reason == \"tool_calls\":\n        print(\"\\n[Tool calling...]\")\n</code></pre>"},{"location":"streaming/#web-ui-integration","title":"Web UI Integration","text":""},{"location":"streaming/#fastapi-streaming-endpoint","title":"FastAPI Streaming Endpoint","text":"<pre><code>from fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\n\napp = FastAPI()\n\n@app.post(\"/chat/stream\")\nasync def chat_stream(message: str):\n    async def generate():\n        async for chunk in agent.run_stream(message):\n            yield chunk.content\n\n    return StreamingResponse(generate(), media_type=\"text/plain\")\n</code></pre>"},{"location":"streaming/#server-sent-events-sse","title":"Server-Sent Events (SSE)","text":"<pre><code>from fastapi import FastAPI\nfrom sse_starlette.sse import EventSourceResponse\n\n@app.post(\"/chat/sse\")\nasync def chat_sse(message: str):\n    async def event_stream():\n        async for chunk in agent.run_stream(message):\n            yield {\"event\": \"token\", \"data\": chunk.content}\n        yield {\"event\": \"done\", \"data\": \"\"}\n\n    return EventSourceResponse(event_stream())\n</code></pre>"},{"location":"streaming/#model-streaming-support","title":"Model Streaming Support","text":"<p>All major model providers support streaming:</p> Provider Streaming Support OpenAI \u2705 Full support Anthropic \u2705 Full support Gemini \u2705 Full support Groq \u2705 Full support Ollama \u2705 Full support Azure OpenAI \u2705 Full support"},{"location":"streaming/#api-reference","title":"API Reference","text":""},{"location":"streaming/#agentrun_stream","title":"Agent.run_stream()","text":"<pre><code>async def run_stream(\n    self,\n    message: str,\n    *,\n    context: dict | None = None,\n) -&gt; AsyncIterator[StreamChunk]:\n    \"\"\"\n    Stream agent response token-by-token.\n\n    Args:\n        message: The user message to process.\n        context: Optional context dictionary.\n\n    Yields:\n        StreamChunk objects with progressive tokens.\n    \"\"\"\n</code></pre>"},{"location":"streaming/#streamchunk_1","title":"StreamChunk","text":"<pre><code>@dataclass\nclass StreamChunk:\n    content: str              # Token content\n    delta: str                # Alias for content\n    is_final: bool           # True on last chunk\n    finish_reason: str | None # \"stop\", \"length\", \"tool_calls\"\n    metadata: dict | None     # Model metadata\n</code></pre>"},{"location":"streaming/#best-practices","title":"Best Practices","text":"<ol> <li>Use <code>end=\"\"</code> and <code>flush=True</code> \u2014 Ensure tokens display immediately</li> <li>Handle <code>is_final</code> \u2014 Add newline or summary after completion</li> <li>Check <code>finish_reason</code> \u2014 Detect truncation or tool calls</li> <li>Collect output \u2014 Append chunks for final text if needed</li> <li>Error handling \u2014 Wrap iteration in try/except for robustness</li> </ol>"},{"location":"tool-building/","title":"Building Custom Tools","text":"<p>Create production-ready tools for cogent agents.</p>"},{"location":"tool-building/#overview","title":"Overview","text":"<p>Tools are functions that agents can call to interact with the world \u2014 APIs, databases, files, web searches, and more. Cogent provides two ways to add tools:</p> <ol> <li>@tool decorator \u2014 Create custom tools from any function (this guide)</li> <li>Capabilities \u2014 Use pre-built tool classes like WebSearch, FileSystem, CodeSandbox</li> </ol> <p>Capabilities are classes that provide tools. They handle setup, state management, and expose multiple related tools:</p> <pre><code>from cogent import Agent\nfrom cogent.capabilities import WebSearch, FileSystem, CodeSandbox\n\nagent = Agent(\n    model=\"gpt-4o-mini\",\n    capabilities=[\n        WebSearch(),                          # Provides: search, fetch_url\n        FileSystem(allowed_paths=[\"./data\"]), # Provides: read, write, list\n        CodeSandbox(),                        # Provides: execute_python\n    ],\n)\n</code></pre> <p>See Capabilities for 12+ production-ready capabilities \u2014 KnowledgeGraph, Browser, PDF, Shell, MCP, and more.</p> <p>For custom logic, use the <code>@tool</code> decorator:</p>"},{"location":"tool-building/#quick-start","title":"Quick Start","text":"<pre><code>from cogent import tool\n\n@tool\ndef get_weather(city: str) -&gt; str:\n    \"\"\"Get current weather for a city.\n\n    Args:\n        city: The city name to get weather for.\n\n    Returns:\n        Weather information as a string.\n    \"\"\"\n    # Your implementation\n    return f\"Weather in {city}: 72\u00b0F, sunny\"\n</code></pre> <p>That's it! The <code>@tool</code> decorator automatically: - Extracts function signature \u2192 JSON schema - Parses docstring \u2192 tool description - Handles sync/async execution - Provides error handling</p>"},{"location":"tool-building/#the-tool-decorator","title":"The @tool Decorator","text":"<pre><code>from cogent import tool\n\n@tool\ndef my_tool(param1: str, param2: int = 10) -&gt; str:\n    \"\"\"Tool description goes here.\n\n    Args:\n        param1: Description of param1.\n        param2: Description of param2 (optional).\n\n    Returns:\n        Description of return value.\n    \"\"\"\n    return f\"Result: {param1} x {param2}\"\n</code></pre>"},{"location":"tool-building/#what-gets-generated","title":"What Gets Generated","text":"<pre><code>{\n  \"type\": \"function\",\n  \"function\": {\n    \"name\": \"my_tool\",\n    \"description\": \"Tool description goes here.\",\n    \"parameters\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"param1\": {\n          \"type\": \"string\",\n          \"description\": \"Description of param1.\"\n        },\n        \"param2\": {\n          \"type\": \"integer\",\n          \"description\": \"Description of param2 (optional).\",\n          \"default\": 10\n        }\n      },\n      \"required\": [\"param1\"]\n    }\n  }\n}\n</code></pre>"},{"location":"tool-building/#return-type-information","title":"Return Type Information","text":"<p>The <code>@tool</code> decorator automatically extracts return type information and includes it in the tool description. This helps the LLM understand what output to expect:</p> <pre><code>@tool\ndef get_weather(city: str) -&gt; dict[str, int]:\n    \"\"\"Get weather data for a city.\n\n    Args:\n        city: City name to query.\n\n    Returns:\n        A dictionary with temp, humidity, and wind_speed.\n    \"\"\"\n    return {\"temp\": 75, \"humidity\": 45, \"wind_speed\": 10}\n\n# LLM sees this description:\n# \"Get weather data for a city. Returns: dict[str, int] - A dictionary with temp, humidity, and wind_speed.\"\n\n# Access the return info directly:\nprint(get_weather.return_info)\n# Output: \"dict[str, int] - A dictionary with temp, humidity, and wind_speed.\"\n</code></pre> <p>What gets extracted:</p> Source Example Result Return type annotation <code>-&gt; str</code> <code>\"str\"</code> Generic types <code>-&gt; dict[str, int]</code> <code>\"dict[str, int]\"</code> Optional types <code>-&gt; str \\| None</code> <code>\"str \\| None\"</code> Docstring Returns section <code>Returns: The result.</code> <code>\"The result.\"</code> Both combined Type + docstring <code>\"dict[str, int] - A dictionary with...\"</code> <p>[!TIP] Always include a <code>Returns:</code> section in your docstrings to give the LLM context about the output format.</p>"},{"location":"tool-building/#decorator-options","title":"Decorator Options","text":"<pre><code>@tool(\n    name=\"web_search\",           # Override function name\n    description=\"Search the web\",  # Override docstring\n    return_direct=True,          # Return result directly to user\n    cache=True,                  # Enable semantic caching (requires agent.cache)\n)\ndef search(query: str, max_results: int = 10) -&gt; str:\n    \"\"\"Search implementation.\"\"\"\n    return f\"Found {max_results} results for: {query}\"\n</code></pre> Option Type Default Description <code>name</code> <code>str</code> Function name Override the tool name <code>description</code> <code>str</code> Docstring Override the tool description <code>return_direct</code> <code>bool</code> <code>False</code> Return result directly to user without LLM processing <code>cache</code> <code>bool</code> <code>False</code> Enable automatic semantic caching (see Semantic Caching)"},{"location":"tool-building/#async-tools","title":"Async Tools","text":"<p>Most production tools are async (API calls, database queries, file I/O):</p> <pre><code>from cogent import tool\nimport httpx\n\n@tool\nasync def fetch_url(url: str) -&gt; str:\n    \"\"\"Fetch content from a URL.\n\n    Args:\n        url: The URL to fetch.\n\n    Returns:\n        The response text.\n    \"\"\"\n    async with httpx.AsyncClient() as client:\n        response = await client.get(url)\n        return response.text\n</code></pre> <p>The executor handles both sync and async tools automatically.</p>"},{"location":"tool-building/#type-hints","title":"Type Hints","text":"<p>Use type hints for automatic schema generation:</p> <pre><code>from cogent import tool\nfrom typing import Literal\n\n@tool\ndef search(\n    query: str,\n    engine: Literal[\"google\", \"bing\", \"duckduckgo\"] = \"duckduckgo\",\n    max_results: int = 10,\n) -&gt; list[dict[str, str]]:\n    \"\"\"Search the web.\n\n    Args:\n        query: Search query string.\n        engine: Search engine to use.\n        max_results: Maximum number of results.\n\n    Returns:\n        List of search results with title and URL.\n    \"\"\"\n    # Implementation\n    return [{\"title\": \"Result 1\", \"url\": \"https://...\"}]\n</code></pre> <p>Supported types: - <code>str</code>, <code>int</code>, <code>float</code>, <code>bool</code> - <code>list[T]</code>, <code>dict[K, V]</code> - <code>Literal[\"option1\", \"option2\"]</code> - <code>Optional[T]</code> or <code>T | None</code> - Pydantic models</p>"},{"location":"tool-building/#error-handling","title":"Error Handling","text":"<p>Always handle errors gracefully:</p> <pre><code>from cogent import tool\nimport httpx\n\n@tool\nasync def safe_fetch(url: str) -&gt; str:\n    \"\"\"Safely fetch URL with error handling.\"\"\"\n    try:\n        async with httpx.AsyncClient(timeout=10.0) as client:\n            response = await client.get(url)\n            response.raise_for_status()\n            return response.text\n    except httpx.TimeoutException:\n        return f\"Error: Request to {url} timed out after 10 seconds\"\n    except httpx.HTTPStatusError as e:\n        return f\"Error: HTTP {e.response.status_code} from {url}\"\n    except Exception as e:\n        return f\"Error fetching {url}: {str(e)}\"\n</code></pre> <p>Best practices: - Catch specific exceptions first - Provide helpful error messages - Return error as string (don't raise in tools) - Log errors for debugging</p>"},{"location":"tool-building/#tool-composition","title":"Tool Composition","text":"<p>Combine related tools into a capability class:</p> <pre><code>from cogent import tool\n\nclass Calculator:\n    \"\"\"Calculator with memory.\"\"\"\n\n    def __init__(self):\n        self.memory: float = 0.0\n\n    @tool\n    def add(self, a: float, b: float) -&gt; float:\n        \"\"\"Add two numbers.\"\"\"\n        result = a + b\n        self.memory = result\n        return result\n\n    @tool\n    def recall(self) -&gt; float:\n        \"\"\"Recall last result from memory.\"\"\"\n        return self.memory\n</code></pre> <p>Pattern: Related tools in a class share state and configuration. This is exactly how capabilities work \u2014 see Capabilities.</p>"},{"location":"tool-building/#context-injection","title":"Context Injection","text":"<p>Access agent context in tools:</p> <pre><code>from cogent import tool\nfrom cogent.core.context import RunContext\n\n@tool\nasync def get_user_data(\n    user_id: int,\n    ctx: RunContext,  # Auto-injected\n) -&gt; dict:\n    \"\"\"Get user data with context.\"\"\"\n    # Access agent context\n    session_id = ctx.session_id\n    user = ctx.user_id\n\n    # Use context for logging, auth, etc\n    logger.info(f\"User {user} requesting data for {user_id} in session {session_id}\")\n\n    return await fetch_user(user_id)\n</code></pre> <p>Context fields: - <code>session_id</code> \u2014 Current session identifier - <code>user_id</code> \u2014 User making the request - <code>metadata</code> \u2014 Custom metadata dict - <code>agent</code> \u2014 Reference to the agent instance</p>"},{"location":"tool-building/#registering-tools-with-agents","title":"Registering Tools with Agents","text":"<pre><code>from cogent import Agent\nfrom cogent.capabilities import WebSearch, FileSystem\n\n# Custom tools + capabilities together\nagent = Agent(\n    model=\"gpt-4o-mini\",\n    tools=[get_weather, my_custom_tool],  # Your @tool functions\n    capabilities=[WebSearch(), FileSystem()],  # Pre-built tool providers\n)\n</code></pre> <p>All tools (custom + from capabilities) are available to the agent.</p>"},{"location":"tool-building/#testing-tools","title":"Testing Tools","text":"<p>Use pytest for tool testing:</p> <pre><code>import pytest\nfrom cogent import tool\n\n@tool\nasync def divide(a: float, b: float) -&gt; float:\n    \"\"\"Divide two numbers.\"\"\"\n    if b == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\n@pytest.mark.asyncio\nasync def test_divide_success():\n    result = await divide(10, 2)\n    assert result == 5.0\n\n@pytest.mark.asyncio\nasync def test_divide_by_zero():\n    with pytest.raises(ValueError, match=\"Cannot divide by zero\"):\n        await divide(10, 0)\n</code></pre> <p>See comprehensive testing guide: docs/testing.md</p>"},{"location":"tool-building/#tool-patterns","title":"Tool Patterns","text":""},{"location":"tool-building/#pattern-1-retry-with-exponential-backoff","title":"Pattern 1: Retry with Exponential Backoff","text":"<pre><code>import asyncio\nfrom cogent import tool\n\n@tool\nasync def resilient_api_call(url: str, max_retries: int = 3) -&gt; str:\n    \"\"\"API call with retries.\"\"\"\n    for attempt in range(max_retries):\n        try:\n            return await fetch(url)\n        except Exception as e:\n            if attempt == max_retries - 1:\n                return f\"Failed after {max_retries} attempts: {e}\"\n            await asyncio.sleep(2 ** attempt)\n</code></pre>"},{"location":"tool-building/#pattern-2-rate-limiting","title":"Pattern 2: Rate Limiting","text":"<pre><code>import asyncio\nfrom cogent import tool\n\nclass RateLimitedAPI:\n    def __init__(self, calls_per_second: int = 10):\n        self.delay = 1.0 / calls_per_second\n        self.last_call = 0.0\n\n    @tool\n    async def call_api(self, endpoint: str) -&gt; str:\n        \"\"\"Call API with rate limiting.\"\"\"\n        now = asyncio.get_event_loop().time()\n        wait_time = max(0, self.delay - (now - self.last_call))\n\n        if wait_time &gt; 0:\n            await asyncio.sleep(wait_time)\n\n        self.last_call = asyncio.get_event_loop().time()\n        return await self._make_request(endpoint)\n</code></pre>"},{"location":"tool-building/#pattern-3-caching","title":"Pattern 3: Caching","text":""},{"location":"tool-building/#semantic-caching-recommended","title":"Semantic Caching (Recommended)","text":"<p>Use <code>@tool(cache=True)</code> for automatic semantic caching. Similar queries return cached results:</p> <pre><code>from cogent import Agent, tool\n\n@tool(cache=True)\nasync def search_products(query: str) -&gt; str:\n    \"\"\"Search products in the catalog.\n\n    Args:\n        query: Search query for products.\n\n    Returns:\n        Product search results.\n    \"\"\"\n    # Expensive API call - cached semantically\n    return await product_api.search(query)\n\n# Agent must have cache enabled\nagent = Agent(\n    model=\"gpt-4o-mini\",\n    tools=[search_products],\n    cache=True,  # Required for @tool(cache=True)\n)\n\n# First call executes the tool\nawait agent.run(\"Find running shoes\")\n\n# Similar query hits cache (semantic match)\nawait agent.run(\"Show me running sneakers\")  # Cache hit!\n</code></pre> <p>How it works:</p> <ol> <li>Tool input is embedded using the agent's embedding model</li> <li>Cache checks for semantically similar previous calls</li> <li>If similarity exceeds threshold, cached result is returned</li> <li>Otherwise, tool executes and result is stored</li> </ol> <p>Requirements:</p> <ul> <li>Agent must have <code>cache=True</code> enabled</li> <li>An embedding model must be configured (or uses default)</li> <li>Tool must have <code>cache=True</code> in decorator</li> </ul>"},{"location":"tool-building/#simple-lru-caching","title":"Simple LRU Caching","text":"<p>For exact-match caching (same input = same output):</p> <pre><code>from cogent import tool\nfrom functools import lru_cache\n\n@tool\n@lru_cache(maxsize=100)\ndef cached_computation(input: str) -&gt; str:\n    \"\"\"Expensive computation with exact-match caching.\"\"\"\n    # Cached result for identical input only\n    return expensive_operation(input)\n</code></pre> <p>[!TIP] Use <code>@tool(cache=True)</code> for semantic similarity matching, <code>@lru_cache</code> for exact string matching.</p>"},{"location":"tool-building/#pattern-4-validation","title":"Pattern 4: Validation","text":"<pre><code>from cogent import tool\nfrom pydantic import BaseModel, Field\n\nclass SearchParams(BaseModel):\n    query: str = Field(min_length=1, max_length=500)\n    max_results: int = Field(ge=1, le=100)\n\n@tool\nasync def validated_search(params: SearchParams) -&gt; list:\n    \"\"\"Search with validated parameters.\"\"\"\n    # Pydantic ensures constraints\n    return await search(params.query, params.max_results)\n</code></pre>"},{"location":"tool-building/#tool-execution","title":"Tool Execution","text":"<p>When an agent has multiple tools, they execute in parallel by default via NativeExecutor:</p> <pre><code>from cogent import Agent\n\nagent = Agent(\n    model=\"gpt-4o-mini\",\n    tools=[fetch_weather, fetch_news, fetch_stock]\n)\n\n# If LLM requests multiple tools in one turn, they run concurrently\nresult = await agent.run(\"Get weather, news, and stock data\")\n# 3 tools \u00d7 0.5s each = ~0.5s total (parallel via asyncio.gather)\n</code></pre> <p>Execution behavior: - Parallel: When LLM requests multiple tools in one turn - Sequential: When LLM naturally calls tools one at a time across turns - LLM decides: Based on task requirements and your prompt</p> <p>Configuration:</p> <pre><code>from cogent.executors import NativeExecutor\n\nexecutor = NativeExecutor(\n    agent,\n    max_tool_calls_per_turn=50,  # Max tools per LLM response\n    max_concurrent_tools=20,      # Tune for external API rate limits\n    resilience=True               # Auto-retry on LLM rate limits\n)\n</code></pre>"},{"location":"tool-building/#standalone-execution","title":"Standalone Execution","text":"<p>For quick tasks without creating an Agent:</p> <pre><code>from cogent.executors import run\n\nresult = await run(\n    \"Search for Python tutorials\",\n    tools=[search],\n    model=\"gpt-4o-mini\",\n)\n</code></pre>"},{"location":"tool-building/#best-practices","title":"Best Practices","text":"<ol> <li>Use type hints \u2014 Enable automatic schema generation</li> <li>Write clear docstrings \u2014 Becomes tool description for LLM</li> <li>Handle errors gracefully \u2014 Return error strings, don't raise</li> <li>Make tools atomic \u2014 One clear purpose per tool</li> <li>Use async for I/O \u2014 All network/DB/file operations</li> <li>Validate inputs \u2014 Pydantic models for complex inputs</li> <li>Add retries for resilience \u2014 External calls can fail</li> <li>Log for observability \u2014 Track tool usage and errors</li> <li>Test thoroughly \u2014 Unit tests for all tools</li> <li>Document return types \u2014 Help LLM use results correctly</li> </ol>"},{"location":"tool-building/#common-pitfalls","title":"Common Pitfalls","text":"Issue Problem Solution Missing type hints No schema generated Add type hints to all params Unclear docstring LLM misuses tool Write clear, specific descriptions Raising exceptions Agent execution halts Return error strings instead Blocking I/O Poor performance Use async for all I/O operations No error handling Crashes on failures Wrap in try/except Too complex LLM struggles to use Split into multiple simpler tools"},{"location":"tool-building/#tool-registry","title":"Tool Registry","text":"<p>Manage collections of tools:</p> <pre><code>from cogent.tools import ToolRegistry\n\nregistry = ToolRegistry()\n\n# Register tools\nregistry.register(search_tool)\nregistry.register(calculate_tool)\nregistry.register(fetch_tool)\n\n# Get all tools\nall_tools = registry.get_all()\n\n# Get by name\nsearch = registry.get(\"search\")\n\n# Check existence\nhas_search = registry.has(\"search\")\n\n# List names\nnames = registry.list_names()  # [\"search\", \"calculate\", \"fetch\"]\n</code></pre>"},{"location":"tool-building/#from-functions","title":"From Functions","text":"<pre><code>from cogent.tools import create_tool_from_function\n\ndef my_function(x: int, y: int) -&gt; int:\n    \"\"\"Add two numbers.\"\"\"\n    return x + y\n\ntool = create_tool_from_function(my_function)\nregistry.register(tool)\n</code></pre>"},{"location":"tool-building/#categories","title":"Categories","text":"<p>Organize tools by category:</p> <pre><code># Register with category\nregistry.register(search_tool, category=\"web\")\nregistry.register(fetch_tool, category=\"web\")\nregistry.register(calculate_tool, category=\"math\")\n\n# Get by category\nweb_tools = registry.get_by_category(\"web\")\n</code></pre>"},{"location":"tool-building/#deferred-tools","title":"Deferred Tools","text":"<p>For operations requiring human approval or async completion:</p>"},{"location":"tool-building/#deferredresult","title":"DeferredResult","text":"<pre><code>from cogent.tools import DeferredResult, DeferredStatus\n\n@tool\ndef send_email(to: str, subject: str, body: str) -&gt; DeferredResult:\n    \"\"\"Send an email (requires approval).\"\"\"\n    return DeferredResult(\n        status=DeferredStatus.PENDING,\n        message=\"Email pending approval\",\n        data={\"to\": to, \"subject\": subject},\n    )\n</code></pre>"},{"location":"tool-building/#deferredmanager","title":"DeferredManager","text":"<p>Manage deferred operations:</p> <pre><code>from cogent.tools import DeferredManager\n\nmanager = DeferredManager()\n\n# Register deferred result\nresult_id = await manager.register(deferred_result)\n\n# Check status\nstatus = await manager.status(result_id)\n\n# Approve/reject\nawait manager.approve(result_id, approver=\"admin\")\nawait manager.reject(result_id, reason=\"Not allowed\")\n\n# Get result after approval\nfinal_result = await manager.get_result(result_id)\n</code></pre>"},{"location":"tool-building/#deferredstatus","title":"DeferredStatus","text":"<pre><code>from cogent.tools import DeferredStatus\n\nDeferredStatus.PENDING    # Waiting for action\nDeferredStatus.APPROVED   # Approved, ready to execute\nDeferredStatus.REJECTED   # Rejected\nDeferredStatus.COMPLETED  # Execution completed\nDeferredStatus.FAILED     # Execution failed\n</code></pre>"},{"location":"tool-building/#deferredwaiter","title":"DeferredWaiter","text":"<p>Wait for deferred completion:</p> <pre><code>from cogent.tools import DeferredWaiter\n\nwaiter = DeferredWaiter(manager)\n\n# Wait for result (with timeout)\nresult = await waiter.wait(result_id, timeout=300)\n\n# Wait for multiple\nresults = await waiter.wait_all([id1, id2, id3])\n</code></pre>"},{"location":"tool-building/#deferredretry","title":"DeferredRetry","text":"<p>Auto-retry failed deferred operations:</p> <pre><code>from cogent.tools import DeferredRetry\n\nretry = DeferredRetry(\n    manager=manager,\n    max_attempts=3,\n    backoff=\"exponential\",\n)\n\nresult = await retry.execute(result_id)\n</code></pre>"},{"location":"tool-building/#is_deferred-helper","title":"is_deferred Helper","text":"<pre><code>from cogent.tools import is_deferred\n\nresult = tool_call()\n\nif is_deferred(result):\n    # Handle deferred result\n    result_id = await manager.register(result)\nelse:\n    # Handle immediate result\n    print(result)\n</code></pre>"},{"location":"tool-building/#complex-types","title":"Complex Types","text":""},{"location":"tool-building/#pydantic-models","title":"Pydantic Models","text":"<pre><code>from pydantic import BaseModel\n\nclass EmailRequest(BaseModel):\n    to: str\n    subject: str\n    body: str\n    cc: list[str] = []\n\n@tool\ndef send_email(request: EmailRequest) -&gt; str:\n    \"\"\"Send an email.\"\"\"\n    return f\"Sent to {request.to}\"\n</code></pre>"},{"location":"tool-building/#enum-parameters","title":"Enum Parameters","text":"<pre><code>from enum import Enum\n\nclass Priority(Enum):\n    LOW = \"low\"\n    MEDIUM = \"medium\"\n    HIGH = \"high\"\n\n@tool\ndef create_task(title: str, priority: Priority = Priority.MEDIUM) -&gt; str:\n    \"\"\"Create a task with priority.\"\"\"\n    return f\"Created: {title} ({priority.value})\"\n</code></pre>"},{"location":"tool-building/#api-reference","title":"API Reference","text":""},{"location":"tool-building/#decorators","title":"Decorators","text":"Decorator Description <code>@tool</code> Create a tool from a function"},{"location":"tool-building/#core-classes","title":"Core Classes","text":"Class Description <code>ToolRegistry</code> Manage tool collections <code>BaseTool</code> Base class for tools"},{"location":"tool-building/#deferred-execution","title":"Deferred Execution","text":"Class Description <code>DeferredResult</code> Result requiring async completion <code>DeferredStatus</code> Status of deferred operation <code>DeferredManager</code> Manage deferred results <code>DeferredWaiter</code> Wait for deferred completion <code>DeferredRetry</code> Retry failed operations"},{"location":"tool-building/#utility-functions","title":"Utility Functions","text":"Function Description <code>create_tool_from_function(fn)</code> Create tool from function <code>is_deferred(result)</code> Check if result is deferred"},{"location":"tool-building/#further-reading","title":"Further Reading","text":"<ul> <li>Capabilities \u2014 12+ production-ready tool classes</li> <li>Agent Configuration \u2014 Using tools with agents</li> <li>Resilience \u2014 Error handling and retry policies</li> </ul>"},{"location":"vectorstore/","title":"VectorStore Module","text":"<p>The <code>cogent.vectorstore</code> module provides semantic search and vector storage for RAG applications.</p>"},{"location":"vectorstore/#overview","title":"Overview","text":"<p>VectorStore provides: - Document storage with embeddings - Similarity search - Multiple backend support (InMemory, FAISS, Chroma, Qdrant, pgvector) - Multiple embedding providers (OpenAI, Ollama, etc.)</p> <pre><code>from cogent.vectorstore import VectorStore\n\n# Simple: in-memory with OpenAI embeddings\nstore = VectorStore()\nawait store.add_texts([\"Python is great\", \"JavaScript is popular\"])\nresults = await store.search(\"programming language\", k=2)\n</code></pre>"},{"location":"vectorstore/#quick-start","title":"Quick Start","text":""},{"location":"vectorstore/#basic-usage","title":"Basic Usage","text":"<pre><code>from cogent.vectorstore import VectorStore\n\n# Create store (uses OpenAI embeddings by default)\nstore = VectorStore()\n\n# Add texts\nawait store.add_texts([\n    \"Python is a programming language\",\n    \"Machine learning uses algorithms\",\n    \"Neural networks learn from data\",\n])\n\n# Search\nresults = await store.search(\"AI and deep learning\", k=2)\nfor r in results:\n    print(f\"{r.score:.3f}: {r.text[:50]}\")\n</code></pre>"},{"location":"vectorstore/#with-documents","title":"With Documents","text":"<pre><code>from cogent.vectorstore import VectorStore, Document, DocumentMetadata\n\n# Create documents with structured metadata\ndocs = [\n    Document(\n        text=\"Python guide\", \n        metadata=DocumentMetadata(\n            source=\"tutorial.md\",\n            source_type=\"markdown\",\n            custom={\"type\": \"tutorial\", \"lang\": \"python\"}\n        )\n    ),\n    Document(\n        text=\"JavaScript intro\", \n        metadata=DocumentMetadata(\n            source=\"intro.md\",\n            custom={\"type\": \"tutorial\", \"lang\": \"js\"}\n        )\n    ),\n    Document(\n        text=\"ML basics\", \n        metadata=DocumentMetadata(\n            source=\"guide.pdf\",\n            source_type=\"pdf\",\n            page=1,\n            custom={\"type\": \"guide\", \"topic\": \"ml\"}\n        )\n    ),\n]\n\nstore = VectorStore()\nawait store.add_documents(docs)\n\n# Access structured metadata in results\nresults = await store.search(\"programming tutorial\", k=5)\nfor r in results:\n    print(f\"Source: {r.document.source}\")  # Convenience property\n    print(f\"Type: {r.document.metadata.source_type}\")\n    print(f\"Custom: {r.document.metadata.custom.get('type')}\")\n\n# Search with metadata filter (use custom dict for app-specific filters)\nresults = await store.search(\n    \"programming tutorial\",\n    k=5,\n    filter={\"custom\": {\"type\": \"tutorial\"}},\n)\n</code></pre> <p>Note on Metadata Filtering:</p> <p>VectorStore backends have different filtering capabilities:</p> <ul> <li>InMemory/FAISS: Full dict matching, nested filtering supported</li> <li>Chroma: Only supports primitive types (str, int, float, bool) - nested dicts like <code>custom</code> are stringified</li> <li>Qdrant/PGVector: Full JSON filtering supported</li> </ul> <p>For maximum compatibility, use standard DocumentMetadata fields (source, page, etc.) or keep custom dict values simple.</p>"},{"location":"vectorstore/#embedding-providers","title":"Embedding Providers","text":"<p>All embedding providers support a standardized API:</p> <p>Primary Methods (with metadata): - <code>embed(texts)</code> / <code>aembed(texts)</code> \u2192 Returns <code>EmbeddingResult</code> with metadata - <code>embed_one(text)</code> / <code>aembed_one(text)</code> \u2192 Returns vector only</p> <p>VectorStore Protocol Methods (async, no metadata): - <code>embed_texts(texts)</code> \u2192 Returns <code>list[list[float]]</code>  - <code>embed_query(text)</code> \u2192 Returns <code>list[float]</code></p>"},{"location":"vectorstore/#openai-default","title":"OpenAI (Default)","text":"<pre><code>from cogent.vectorstore import VectorStore\nfrom cogent.models import OpenAIEmbedding\n\n# Uses text-embedding-3-small by default\nstore = VectorStore()\n\n# Or specify model\nembeddings = OpenAIEmbedding(model=\"text-embedding-3-large\")\nstore = VectorStore(embeddings=embeddings)\n\n# Use embeddings directly\nresult = await embeddings.embed([\"Hello\", \"World\"])\nprint(result.metadata.tokens)  # Track token usage\n</code></pre>"},{"location":"vectorstore/#ollama-local","title":"Ollama (Local)","text":"<pre><code>from cogent.models import OllamaEmbedding\n\nembeddings = OllamaEmbedding(\n    model=\"nomic-embed-text\",\n    host=\"http://localhost:11434\",\n)\nstore = VectorStore(embeddings=embeddings)\n</code></pre>"},{"location":"vectorstore/#other-providers","title":"Other Providers","text":"<pre><code>from cogent.models import (\n    GeminiEmbedding,\n    CohereEmbedding,\n    MistralEmbedding,\n    AzureOpenAIEmbedding,\n    CloudflareEmbedding,\n    CustomEmbedding,\n)\n\n# Gemini\nembeddings = GeminiEmbedding(model=\"text-embedding-004\")\n\n# Cohere\nembeddings = CohereEmbedding(model=\"embed-english-v3.0\")\n\n# Mistral\nembeddings = MistralEmbedding(model=\"mistral-embed\")\n\n# Azure OpenAI\nfrom cogent.models.azure import AzureEntraAuth\nembeddings = AzureOpenAIEmbedding(\n    azure_endpoint=\"https://your-resource.openai.azure.com\",\n    deployment=\"text-embedding-ada-002\",\n)\n\n# Cloudflare Workers AI\nembeddings = CloudflareEmbedding(\n    model=\"@cf/baai/bge-base-en-v1.5\",\n    account_id=\"your-account-id\",\n)\n\n# Custom OpenAI-compatible\nembeddings = CustomEmbedding(\n    base_url=\"http://localhost:8000/v1\",\n    model=\"custom-model\",\n)\n</code></pre>"},{"location":"vectorstore/#mock-testing","title":"Mock (Testing)","text":"<pre><code>from cogent.models import MockEmbedding\n\nembeddings = MockEmbedding(dimensions=384)\nstore = VectorStore(embeddings=embeddings)\n\n# Fast, deterministic embeddings for tests\n</code></pre>"},{"location":"vectorstore/#storage-backends","title":"Storage Backends","text":""},{"location":"vectorstore/#inmemory-default","title":"InMemory (Default)","text":"<p>NumPy-based, no external dependencies:</p> <pre><code>from cogent.vectorstore import VectorStore\nfrom cogent.vectorstore.backends.inmemory import SimilarityMetric\n\nstore = VectorStore(\n    metric=SimilarityMetric.COSINE,  # or DOT_PRODUCT, EUCLIDEAN\n)\n</code></pre>"},{"location":"vectorstore/#faiss","title":"FAISS","text":"<p>Large-scale similarity search:</p> <pre><code># pip install faiss-cpu (or faiss-gpu)\nfrom cogent.vectorstore.backends import FAISSBackend\n\nbackend = FAISSBackend(\n    index_type=\"IVF\",      # or \"Flat\", \"HNSW\"\n    nlist=100,             # Number of clusters\n    nprobe=10,             # Search clusters\n)\nstore = VectorStore(backend=backend)\n</code></pre>"},{"location":"vectorstore/#chroma","title":"Chroma","text":"<p>Persistent vector database:</p> <pre><code># pip install chromadb\nfrom cogent.vectorstore.backends import ChromaBackend\n\nbackend = ChromaBackend(\n    collection_name=\"my_docs\",\n    persist_directory=\"./chroma_db\",\n)\nstore = VectorStore(backend=backend)\n</code></pre>"},{"location":"vectorstore/#qdrant","title":"Qdrant","text":"<p>Production-ready vector database:</p> <pre><code># pip install qdrant-client\nfrom cogent.vectorstore.backends import QdrantBackend\n\nbackend = QdrantBackend(\n    collection_name=\"my_docs\",\n    url=\"http://localhost:6333\",\n    # Or cloud:\n    # url=\"https://xxx.qdrant.io\",\n    # api_key=\"...\",\n)\nstore = VectorStore(backend=backend)\n</code></pre>"},{"location":"vectorstore/#pgvector","title":"pgvector","text":"<p>PostgreSQL with vector extension:</p> <pre><code># pip install psycopg[pool]\nfrom cogent.vectorstore.backends import PgVectorBackend\n\nbackend = PgVectorBackend(\n    connection_string=\"postgresql://user:pass@localhost/db\",\n    table_name=\"embeddings\",\n    dimension=1536,\n)\nstore = VectorStore(backend=backend)\n</code></pre>"},{"location":"vectorstore/#document-management","title":"Document Management","text":""},{"location":"vectorstore/#adding-documents","title":"Adding Documents","text":"<pre><code>from cogent.vectorstore import VectorStore, Document\n\nstore = VectorStore()\n\n# Add texts (simple)\nids = await store.add_texts([\n    \"First document\",\n    \"Second document\",\n])\n\n# Add texts with metadata\nids = await store.add_texts(\n    texts=[\"Doc 1\", \"Doc 2\"],\n    metadatas=[{\"type\": \"a\"}, {\"type\": \"b\"}],\n)\n\n# Add Document objects\ndocs = [\n    Document(text=\"Content\", metadata={\"source\": \"file.txt\"}),\n]\nids = await store.add_documents(docs)\n</code></pre>"},{"location":"vectorstore/#document-utilities","title":"Document Utilities","text":"<pre><code>from cogent.vectorstore import (\n    create_documents,\n    split_text,\n    split_documents,\n)\n\n# Create documents from texts\ndocs = create_documents(\n    texts=[\"Text 1\", \"Text 2\"],\n    metadatas=[{\"id\": 1}, {\"id\": 2}],\n)\n\n# Split text into chunks\nchunks = split_text(\n    text=long_text,\n    chunk_size=1000,\n    chunk_overlap=200,\n)\n\n# Split documents into chunks\nchunks = split_documents(\n    documents=docs,\n    chunk_size=1000,\n    chunk_overlap=200,\n)\n</code></pre>"},{"location":"vectorstore/#searching","title":"Searching","text":""},{"location":"vectorstore/#basic-search","title":"Basic Search","text":"<pre><code>results = await store.search(\"query\", k=5)\n\nfor r in results:\n    print(f\"Score: {r.score}\")\n    print(f\"Text: {r.text}\")\n    print(f\"Metadata: {r.metadata}\")\n</code></pre>"},{"location":"vectorstore/#filtered-search","title":"Filtered Search","text":"<pre><code># Filter by metadata\nresults = await store.search(\n    \"query\",\n    k=10,\n    filter={\"type\": \"tutorial\"},\n)\n\n# Multiple filter conditions\nresults = await store.search(\n    \"query\",\n    k=10,\n    filter={\n        \"type\": \"tutorial\",\n        \"language\": \"python\",\n    },\n)\n</code></pre>"},{"location":"vectorstore/#search-with-threshold","title":"Search with Threshold","text":"<pre><code># Only return results above score threshold\nresults = await store.search(\n    \"query\",\n    k=10,\n    score_threshold=0.7,\n)\n</code></pre>"},{"location":"vectorstore/#searchresult","title":"SearchResult","text":"<p>Search results contain:</p> <pre><code>from cogent.vectorstore import SearchResult\n\n@dataclass\nclass SearchResult:\n    text: str              # Document text\n    score: float           # Similarity score\n    metadata: dict         # Document metadata\n    id: str | None         # Document ID\n    embedding: list | None # Vector (if requested)\n</code></pre>"},{"location":"vectorstore/#factory-function","title":"Factory Function","text":"<p>Create vectorstore with specific configuration:</p> <pre><code>from cogent.vectorstore import create_vectorstore\n\n# Simple\nstore = create_vectorstore()\n\n# With backend\nstore = create_vectorstore(\n    backend=\"chroma\",\n    collection_name=\"docs\",\n    persist_directory=\"./data\",\n)\n\n# With embeddings\nstore = create_vectorstore(\n    embeddings=\"ollama\",\n    model=\"nomic-embed-text\",\n)\n</code></pre>"},{"location":"vectorstore/#similarity-metrics","title":"Similarity Metrics","text":"<pre><code>from cogent.vectorstore.backends.inmemory import SimilarityMetric\n\nSimilarityMetric.COSINE       # Normalized dot product (default)\nSimilarityMetric.DOT_PRODUCT  # Raw dot product\nSimilarityMetric.EUCLIDEAN    # L2 distance\n</code></pre>"},{"location":"vectorstore/#integration-with-retrievers","title":"Integration with Retrievers","text":"<pre><code>from cogent.retriever import DenseRetriever\nfrom cogent.vectorstore import VectorStore\n\n# Create vectorstore and add documents\nstore = VectorStore()\nawait store.add_texts(docs)\n\n# Create retriever\nretriever = DenseRetriever(store)\n\n# Search directly\nresults = await retriever.retrieve(\"What is X?\", k=5)\nfor r in results:\n    print(r.document.text)\n</code></pre>"},{"location":"vectorstore/#persistence","title":"Persistence","text":""},{"location":"vectorstore/#saveload-inmemory","title":"Save/Load (InMemory)","text":"<pre><code># Save to disk\nawait store.save(\"vectorstore.pkl\")\n\n# Load from disk\nstore = await VectorStore.load(\"vectorstore.pkl\")\n</code></pre>"},{"location":"vectorstore/#persistent-backends","title":"Persistent Backends","text":"<p>Chroma, Qdrant, and pgvector automatically persist:</p> <pre><code># Chroma persists to disk\nbackend = ChromaBackend(persist_directory=\"./data\")\nstore = VectorStore(backend=backend)\n\n# Data persists across sessions\n</code></pre>"},{"location":"vectorstore/#batch-operations","title":"Batch Operations","text":"<pre><code># Add in batches for large datasets\nbatch_size = 100\nfor i in range(0, len(texts), batch_size):\n    batch = texts[i:i + batch_size]\n    await store.add_texts(batch)\n\n# Parallel embedding\nawait store.add_texts(\n    texts=large_list,\n    batch_size=50,\n    parallel=True,\n)\n</code></pre>"},{"location":"vectorstore/#api-reference","title":"API Reference","text":""},{"location":"vectorstore/#vectorstore","title":"VectorStore","text":"Method Description <code>add_texts(texts, metadatas?)</code> Add texts to store <code>add_documents(docs)</code> Add Document objects <code>search(query, k?, filter?)</code> Similarity search <code>delete(ids)</code> Delete by IDs <code>clear()</code> Remove all documents <code>save(path)</code> Save to disk <code>load(path)</code> Load from disk"},{"location":"vectorstore/#embedding-providers_1","title":"Embedding Providers","text":"<p>All embedding providers implement the <code>EmbeddingProvider</code> protocol:</p> <pre><code>from cogent.vectorstore.base import EmbeddingProvider\n\nclass EmbeddingProvider(Protocol):\n    \"\"\"Protocol for embedding providers used by VectorStore.\"\"\"\n\n    @property\n    def dimension(self) -&gt; int:\n        \"\"\"Return embedding dimension.\"\"\"\n\n    async def embed_texts(self, texts: list[str]) -&gt; list[list[float]]:\n        \"\"\"Generate embeddings for multiple texts.\"\"\"\n\n    async def embed_query(self, text: str) -&gt; list[float]:\n        \"\"\"Generate embedding for a query text.\"\"\"\n</code></pre> <p>Available providers: - <code>OpenAIEmbedding</code> - OpenAI API (default) - <code>AzureOpenAIEmbedding</code> - Azure OpenAI - <code>OllamaEmbedding</code> - Local Ollama - <code>GeminiEmbedding</code> - Google Gemini - <code>CohereEmbedding</code> - Cohere - <code>MistralEmbedding</code> - Mistral AI - <code>CloudflareEmbedding</code> - Cloudflare Workers AI - <code>CustomEmbedding</code> - Custom OpenAI-compatible APIs - <code>MockEmbedding</code> - Testing</p> <p>All providers are imported from <code>cogent.models</code>.</p>"},{"location":"vectorstore/#backends","title":"Backends","text":"Backend Use Case <code>InMemoryBackend</code> Development, small datasets <code>FAISSBackend</code> Large-scale, local <code>ChromaBackend</code> Persistent, easy setup <code>QdrantBackend</code> Production, distributed <code>PgVectorBackend</code> PostgreSQL integration"},{"location":"vectorstore/#utilities","title":"Utilities","text":"Function Description <code>create_documents(texts, metadatas)</code> Create Document list <code>split_text(text, chunk_size)</code> Split text into chunks <code>split_documents(docs, chunk_size)</code> Split documents into chunks"}]}